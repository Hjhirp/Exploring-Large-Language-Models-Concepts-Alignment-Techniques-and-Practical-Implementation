{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-06T17:37:14.982101Z","iopub.execute_input":"2024-10-06T17:37:14.982397Z","iopub.status.idle":"2024-10-06T17:37:16.318553Z","shell.execute_reply.started":"2024-10-06T17:37:14.982364Z","shell.execute_reply":"2024-10-06T17:37:16.317708Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nos.mkdir('/kaggle/working/output_dir')","metadata":{"execution":{"iopub.status.busy":"2024-10-06T17:37:16.320632Z","iopub.execute_input":"2024-10-06T17:37:16.321162Z","iopub.status.idle":"2024-10-06T17:37:16.326167Z","shell.execute_reply.started":"2024-10-06T17:37:16.321116Z","shell.execute_reply":"2024-10-06T17:37:16.325110Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install -U -q \"huggingface_hub[cli]\"","metadata":{"execution":{"iopub.status.busy":"2024-10-06T17:37:16.327486Z","iopub.execute_input":"2024-10-06T17:37:16.327796Z","iopub.status.idle":"2024-10-06T17:37:29.835049Z","shell.execute_reply.started":"2024-10-06T17:37:16.327764Z","shell.execute_reply":"2024-10-06T17:37:29.833809Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token [HF_Token]","metadata":{"execution":{"iopub.status.busy":"2024-10-06T17:37:29.837320Z","iopub.execute_input":"2024-10-06T17:37:29.837643Z","iopub.status.idle":"2024-10-06T17:37:31.534145Z","shell.execute_reply.started":"2024-10-06T17:37:29.837608Z","shell.execute_reply":"2024-10-06T17:37:31.533153Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"scitldr\")\n\ndataset['train'] = dataset['train'].remove_columns(['source_labels', 'rouge_scores', 'paper_id'])\n\ndataset['validation'] = dataset['validation'].remove_columns(['source_labels', 'rouge_scores', 'paper_id'])\n\ndataset['test'] = dataset['test'].remove_columns(['source_labels', 'rouge_scores', 'paper_id'])\n\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-10-06T17:24:27.044491Z","iopub.execute_input":"2024-10-06T17:24:27.044886Z","iopub.status.idle":"2024-10-06T17:24:43.141432Z","shell.execute_reply.started":"2024-10-06T17:24:27.044848Z","shell.execute_reply":"2024-10-06T17:24:43.140413Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"scitldr.py:   0%|          | 0.00/7.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f18048d9e0f4c958a290cd81f2a8c99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dataset_infos.json:   0%|          | 0.00/7.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da977ecf4a024d409beebd3c3534ed97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20f2e72638fe4c37ac7743fdc2486a07"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for scitldr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/scitldr.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ee6b6b05ea040679b406a83dcfd1366"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.12M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93029839a64343059b4613f627cc81ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.20M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e7405f703464ab891ee29ea4147354b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1992 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd4c6878e878419f9f2ca4d73c280a79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/618 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41a3f030ebb04faebbfe1e0b70e3d945"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/619 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e001b51aa363471aaf47214d0d95c8e4"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['source', 'target'],\n        num_rows: 1992\n    })\n    test: Dataset({\n        features: ['source', 'target'],\n        num_rows: 618\n    })\n    validation: Dataset({\n        features: ['source', 'target'],\n        num_rows: 619\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom transformers import DataCollatorForLanguageModeling\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\ndef preprocess_function(examples):\n    return tokenizer([\" \".join(x) for x in examples[\"source\"]])\n\n# Map this function to dataset\ntokenized_dataset = dataset.map(\n                      preprocess_function,\n                      batched= True,#Batch the data to increase speed\n                      num_proc = 4, #Number of Processes | Parallelization step\n                      remove_columns= dataset['train'].column_names\n                      )\n\ntokenizer.pad_token = tokenizer.eos_token\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T17:24:43.142608Z","iopub.execute_input":"2024-10-06T17:24:43.142923Z","iopub.status.idle":"2024-10-06T17:25:04.296194Z","shell.execute_reply.started":"2024-10-06T17:24:43.142890Z","shell.execute_reply":"2024-10-06T17:25:04.295143Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcb37376f85344a9a6aa97b96f4fc95a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98197c96f29a4279b03ca0ab3588207d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e955638fd76c4e439320a86e522930b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50f3676111064909b323053af7f0cb4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2e23f752fc04eefb08219840a6c3ab0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"933ffd8cab9a4d6eb32e0bfcb5907a24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1992 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef3a35ff015c442da933f062012c6a2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/618 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41299273977748b385ce4fd43fb450ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/619 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6568ef8bf6c4acf8050efbc57946b04"}},"metadata":{}}]},{"cell_type":"markdown","source":"# SFT on Complete Model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoConfig, AutoModelForCausalLM, TrainingArguments, Trainer\nimport math\n\n# Import the model:\n# Model_checkpoint = string containing model Id or Path to directory containing model weights save using save_pretrained()\n# Kwargs generally mention change to configuration to the model while loading like loading a tensorflow model in pytorch using from_tf=True\nmodel = AutoModelForCausalLM.from_pretrained('gpt2')\n\n# Training the model:\n# TrainingArguments can be found at https://huggingface.co/transformers/v4.7.0/main_classes/trainer.html?highlight=trainingarguments#transformers.TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/RawSFT\",\n    eval_strategy=\"epoch\",\n    run_name=\"gpt2-summerizer\",\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    # push_to_hub=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    data_collator=data_collator, # For padding with EOS as mentioned earlier\n    tokenizer=tokenizer,\n)\n\ntrainer.train()\n\n# Evaluation of results:\neval_results = trainer.evaluate()\nprint(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-06T17:25:53.598076Z","iopub.execute_input":"2024-10-06T17:25:53.598503Z","iopub.status.idle":"2024-10-06T17:32:56.888981Z","shell.execute_reply.started":"2024-10-06T17:25:53.598460Z","shell.execute_reply":"2024-10-06T17:32:56.888008Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [375/375 06:46, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>3.419472</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>3.375183</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>3.364841</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [39/39 00:14]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Perplexity: 28.93\n","output_type":"stream"}]},{"cell_type":"code","source":"# To push the model to HuggingFace we can use:\ntrainer.create_model_card()\ntrainer.push_to_hub(\"HF_username/RawSFT\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SFT LoRA","metadata":{}},{"cell_type":"code","source":"!pip install -q trl peft","metadata":{"execution":{"iopub.status.busy":"2024-10-06T17:33:17.488427Z","iopub.execute_input":"2024-10-06T17:33:17.488791Z","iopub.status.idle":"2024-10-06T17:33:31.406622Z","shell.execute_reply.started":"2024-10-06T17:33:17.488754Z","shell.execute_reply":"2024-10-06T17:33:31.405535Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom trl import SFTTrainer \nfrom peft import LoraConfig\nfrom random import randrange\nfrom transformers import AutoConfig, AutoModelForCausalLM, TrainingArguments, Trainer \nimport math \n\n\n# Create a data modification function like below if required to perform SFT.\n# Here we are considering case of text-summarization as shown in blog: LoRA for Fine-Tuning LLMs explained with codes and example\n# Change or remove this function based on your dataset.\ndef prompt_instruction_format(sample):\n    return f\"\"\"### Instruction:\n        Use the Task below and the Input given to write the Response:\n\n        ### Task:\n        Summarize the Input\n\n        ### Input:\n        {sample['source']}\n\n        ### Response:\n        {sample['target']}\n        \"\"\" \n\n# Load your data that you want to fine-tune the existing model with\ndataset = load_dataset(\"scitldr\")\n\ndataset['train'] = dataset['train'].remove_columns(['source_labels', 'rouge_scores', 'paper_id'])\n\ndataset['validation'] = dataset['validation'].remove_columns(['source_labels', 'rouge_scores', 'paper_id'])\n\ndataset['test'] = dataset['test'].remove_columns(['source_labels', 'rouge_scores', 'paper_id'])\n\n# Load your pretrained model, either from huggingface or from your local\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\n# Set new training arguments for fine-tuning \ntrainingArgs = TrainingArguments(\n    output_dir=\"/kaggle/working/SFT_with_LoRA\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    save_strategy=\"epoch\",\n    learning_rate=2e-4\n)\n\n# Create configuration for LoRA \npeft_config = LoraConfig(\n      lora_alpha=16, # The alpha parameter for Lora scaling, adjusts the influence of LoRA adaptations, generally set 16, 32, or 64.\n      lora_dropout=0.1, # Dropout Rate.\n      r=64, #  Lora attention dimension (the “rank”).\n      bias=\"none\", # Bias type for LoRA. Can be ‘none’, ‘all’ or ‘lora_only’.\n      task_type=\"CAUSAL_LM\", # Type of task\n      # target_modules = [\"q_proj\", \"v_proj\"] # Specify which layers of the model should be adapted by LoRA.\n      # layers_to_transform: [0,1,2,4] # Control which layers are adapted by specifying their indices.\n)\n\n# Create a SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset['train'],\n    eval_dataset = dataset['test'],\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    packing=True,\n    formatting_func=prompt_instruction_format,\n    args=trainingArgs,\n)\n\n# Evaluation of results:\neval_results = trainer.evaluate()\nprint(f\"Perplexity Before Training: {math.exp(eval_results['eval_loss']):.2f}\")\n\n# Fine-tune the final model\ntrainer.train()\n\n# Evaluation of results:\neval_results = trainer.evaluate()\nprint(f\"Perplexity After Training: {math.exp(eval_results['eval_loss']):.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-06T17:33:31.409137Z","iopub.execute_input":"2024-10-06T17:33:31.409511Z","iopub.status.idle":"2024-10-06T17:36:23.876383Z","shell.execute_reply.started":"2024-10-06T17:33:31.409462Z","shell.execute_reply":"2024-10-06T17:36:23.875443Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d75f047b2f748cd88ad75545bf827c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0217057d9d9c4386a6c3cd1d4675de5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='32' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [16/16 02:45]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"Perplexity Before Training: 27.88\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='85' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [85/85 02:11, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Perplexity After Training: 11.57\n","output_type":"stream"}]},{"cell_type":"code","source":"# To push the new model to HuggingFace we can use:\ntrainer.create_model_card()\ntrainer.push_to_hub(\"HF_username/SFT_with_LoRA\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RLHF","metadata":{}},{"cell_type":"markdown","source":"### IMP NOTE: Restart the notebook, run the code before Data Preprocessing and then directly the cells below. This is for not getting any dependency or CUDA error","metadata":{}},{"cell_type":"code","source":"!pip install -q trl peft","metadata":{"execution":{"iopub.status.busy":"2024-10-06T17:39:34.074929Z","iopub.execute_input":"2024-10-06T17:39:34.075705Z","iopub.status.idle":"2024-10-06T17:39:47.254667Z","shell.execute_reply.started":"2024-10-06T17:39:34.075661Z","shell.execute_reply":"2024-10-06T17:39:47.253603Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSequenceClassification, \n    AutoTokenizer, \n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, TaskType, get_peft_model\nfrom trl import RewardTrainer, RewardConfig\n\n# Load the dataset and tokenizer\ndataset = load_dataset(\"davanstrien/dataset-tldr-preference-dpo\")\ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n\n# Remove unnecessary columns from the dataset\ndataset = dataset.remove_columns([\n    'datasetId', 'card', 'generation_models', \n    'generations', 'model_name', 'ratings', \n    'rationales', 'prompt'\n])\n\n# Preprocessing function for tokenizing the \"chosen\" and \"rejected\" texts\ndef preprocess_function(examples):\n    chosen = tokenizer([x for x in examples[\"chosen\"]], padding=\"max_length\", truncation=True)\n    rejected = tokenizer([x for x in examples[\"rejected\"]], padding=\"max_length\", truncation=True)\n    \n    return {\n        \"input_ids_chosen\": chosen[\"input_ids\"],\n        \"attention_mask_chosen\": chosen[\"attention_mask\"],\n        \"input_ids_rejected\": rejected[\"input_ids\"],\n        \"attention_mask_rejected\": rejected[\"attention_mask\"]\n    }\n\n# Apply preprocessing to the dataset\ntokenized_dataset = dataset.map(\n    preprocess_function,\n    batched=True,  # Process in batches to increase speed\n    num_proc=4,    # Use parallel processing\n    remove_columns=dataset['train'].column_names  # Remove unused columns after preprocessing\n)\n\n# Split the tokenized dataset into training and testing sets\ntokenized_dataset = tokenized_dataset['train'].train_test_split(test_size=0.2)\n\n# Ensure the padding token is set correctly\ntokenizer.pad_token = tokenizer.eos_token\n\n# Data collator to handle padding\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n# Set model ID\nmodel_id = \"t5-small\"\n\n# Load the model and move it to the correct device (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModelForSequenceClassification.from_pretrained(model_id)\nmodel.to(device)\n\n# Align padding tokens between tokenizer and model\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n# Define PEFT (LoRA) configuration for efficient fine-tuning\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n)\n\n# Apply PEFT to the model\nmodel = get_peft_model(model, peft_config)\n\n# Define reward-specific training arguments\ntraining_args = RewardConfig(\n    output_dir=\"./output_dir/my_reward_model\",  # Adjust the path for local storage\n    eval_strategy=\"steps\",\n    center_rewards_coefficient=0.01,\n    eval_steps=50,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    per_device_train_batch_size=8,  # Adjusted batch size\n    max_length=50,\n    do_eval=True,\n)\n\n# Define the RewardTrainer with TRL\ntrainer = RewardTrainer(\n    model=model,\n    args=training_args,\n    tokenizer=tokenizer,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['test'],\n    peft_config=peft_config\n)\n\n# Train the model\ntrainer.train()\n\n# Save the model\ntrainer.save_model(training_args.output_dir)\n\n# Evaluate and log metrics\nmetrics = trainer.evaluate()\ntrainer.log_metrics(\"eval\", metrics)\ntrainer.save_metrics(\"eval\", metrics)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T17:40:39.592031Z","iopub.execute_input":"2024-10-06T17:40:39.592433Z","iopub.status.idle":"2024-10-06T17:42:12.795148Z","shell.execute_reply.started":"2024-10-06T17:40:39.592387Z","shell.execute_reply":"2024-10-06T17:42:12.794222Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nCould not estimate the number of tokens of the input, floating-point operations will not be computed\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [81/81 01:24, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>No log</td>\n      <td>0.702054</td>\n      <td>0.333333</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mchosen_text                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrejected_text                               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlogits          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n│ This dataset contains persona descriptions    │ The tl;dr summary of the dataset based on    │ [0.5153, 0.4847] │\n│ and traits, which can be used to develop      │ the provided card is unavailable, as no      │                  │\n│ chatbots or virtual assistants that can       │ dataset card has been supplied. Please       │                  │\n│ engage in more personalized and human-like    │ provide a dataset card with information      │                  │\n│ conversations.<pad><pad><pad><pad><pad><pad>… │ about a specific dataset to generate an      │                  │\n│                                               │ accurate tl;dr                               │                  │\n│                                               │ summary.<pad><pad><pad><pad><pad><pad><pad>… │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ The HelpSteer dataset contains 37,120 samples │ HelpSteer is a dataset containing 37,120     │ [0.5259, 0.4741] │\n│ with prompts and responses, along with        │ samples of prompts, responses, and           │                  │\n│ human-annotated attributes for helpfulness,   │ human-annotated attributes, enabling the     │                  │\n│ correctness, coherence, complexity, and       │ training of models to become more helpful,   │                  │\n│ verbosity, designed to aid in aligning models │ factually correct, coherent, and adjustable  │                  │\n│ to provide more helpful, factually correct,   │ in terms of complexity and                   │                  │\n│ and coherent                                  │ verbosity.<pad><pad><pad><pad><pad><pad><pa… │                  │\n│ responses.<pad><pad><pad><pad><pad><pad><pad… │                                              │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ The Roboflow Website Screenshots dataset is a │ This dataset contains annotated screenshots  │ [0.5507, 0.4493] │\n│ synthetic dataset containing over 1000        │ from over 1000 top websites, labeled with    │                  │\n│ annotated screenshots from popular websites,  │ various HTML elements such as buttons,       │                  │\n│ useful for Robotic Process Automation and     │ headings, links, and images, which can be    │                  │\n│ offering a cost-effective alternative to      │ used for Robotic Process Automation and      │                  │\n│ human                                         │ related                                      │                  │\n│ labeling.<pad><pad><pad><pad><pad><pad><pad>… │ applications.<pad><pad><pad><pad><pad><pad>… │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ MultiCoNER is a fine-grained multilingual     │ The MultiCoNER dataset, available in 12      │ [0.5328, 0.4672] │\n│ named entity recognition dataset covering 12  │ languages, is a fine-grained multilingual    │                  │\n│ languages, including Bangla, Chinese,         │ named entity recognition (NER) resource      │                  │\n│ English, and others, annotated with 11 entity │ designed for use in the SemEval-2023 Task    │                  │\n│ types such as Location, Person, and Medical,  │ 2.<pad><pad><pad><pad><pad><pad><pad><pad><… │                  │\n│ suitable for training and evaluating models   │                                              │                  │\n│ for named entity recognition                  │                                              │                  │\n│ tasks.<pad><pad><pad><pad><pad><pad><pad><pa… │                                              │                  │\n└───────────────────────────────────────────────┴──────────────────────────────────────────────┴──────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> chosen_text                                   </span>┃<span style=\"font-weight: bold\"> rejected_text                                </span>┃<span style=\"font-weight: bold\"> logits           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n│ This dataset contains persona descriptions    │ The tl;dr summary of the dataset based on    │ [0.5153, 0.4847] │\n│ and traits, which can be used to develop      │ the provided card is unavailable, as no      │                  │\n│ chatbots or virtual assistants that can       │ dataset card has been supplied. Please       │                  │\n│ engage in more personalized and human-like    │ provide a dataset card with information      │                  │\n│ conversations.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;… │ about a specific dataset to generate an      │                  │\n│                                               │ accurate tl;dr                               │                  │\n│                                               │ summary.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;… │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ The HelpSteer dataset contains 37,120 samples │ HelpSteer is a dataset containing 37,120     │ [0.5259, 0.4741] │\n│ with prompts and responses, along with        │ samples of prompts, responses, and           │                  │\n│ human-annotated attributes for helpfulness,   │ human-annotated attributes, enabling the     │                  │\n│ correctness, coherence, complexity, and       │ training of models to become more helpful,   │                  │\n│ verbosity, designed to aid in aligning models │ factually correct, coherent, and adjustable  │                  │\n│ to provide more helpful, factually correct,   │ in terms of complexity and                   │                  │\n│ and coherent                                  │ verbosity.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa… │                  │\n│ responses.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad… │                                              │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ The Roboflow Website Screenshots dataset is a │ This dataset contains annotated screenshots  │ [0.5507, 0.4493] │\n│ synthetic dataset containing over 1000        │ from over 1000 top websites, labeled with    │                  │\n│ annotated screenshots from popular websites,  │ various HTML elements such as buttons,       │                  │\n│ useful for Robotic Process Automation and     │ headings, links, and images, which can be    │                  │\n│ offering a cost-effective alternative to      │ used for Robotic Process Automation and      │                  │\n│ human                                         │ related                                      │                  │\n│ labeling.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;… │ applications.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;… │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ MultiCoNER is a fine-grained multilingual     │ The MultiCoNER dataset, available in 12      │ [0.5328, 0.4672] │\n│ named entity recognition dataset covering 12  │ languages, is a fine-grained multilingual    │                  │\n│ languages, including Bangla, Chinese,         │ named entity recognition (NER) resource      │                  │\n│ English, and others, annotated with 11 entity │ designed for use in the SemEval-2023 Task    │                  │\n│ types such as Location, Person, and Medical,  │ 2.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;… │                  │\n│ suitable for training and evaluating models   │                                              │                  │\n│ for named entity recognition                  │                                              │                  │\n│ tasks.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa… │                                              │                  │\n└───────────────────────────────────────────────┴──────────────────────────────────────────────┴──────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mchosen_text                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrejected_text                               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlogits          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n│ This dataset contains persona descriptions    │ The tl;dr summary of the dataset based on    │ [0.4909, 0.5091] │\n│ and traits, which can be used to develop      │ the provided card is unavailable, as no      │                  │\n│ chatbots or virtual assistants that can       │ dataset card has been supplied. Please       │                  │\n│ engage in more personalized and human-like    │ provide a dataset card with information      │                  │\n│ conversations.<pad><pad><pad><pad><pad><pad>… │ about a specific dataset to generate an      │                  │\n│                                               │ accurate tl;dr                               │                  │\n│                                               │ summary.<pad><pad><pad><pad><pad><pad><pad>… │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ The HelpSteer dataset contains 37,120 samples │ HelpSteer is a dataset containing 37,120     │ [0.5879, 0.4121] │\n│ with prompts and responses, along with        │ samples of prompts, responses, and           │                  │\n│ human-annotated attributes for helpfulness,   │ human-annotated attributes, enabling the     │                  │\n│ correctness, coherence, complexity, and       │ training of models to become more helpful,   │                  │\n│ verbosity, designed to aid in aligning models │ factually correct, coherent, and adjustable  │                  │\n│ to provide more helpful, factually correct,   │ in terms of complexity and                   │                  │\n│ and coherent                                  │ verbosity.<pad><pad><pad><pad><pad><pad><pa… │                  │\n│ responses.<pad><pad><pad><pad><pad><pad><pad… │                                              │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ The Roboflow Website Screenshots dataset is a │ This dataset contains annotated screenshots  │ [0.5049, 0.4951] │\n│ synthetic dataset containing over 1000        │ from over 1000 top websites, labeled with    │                  │\n│ annotated screenshots from popular websites,  │ various HTML elements such as buttons,       │                  │\n│ useful for Robotic Process Automation and     │ headings, links, and images, which can be    │                  │\n│ offering a cost-effective alternative to      │ used for Robotic Process Automation and      │                  │\n│ human                                         │ related                                      │                  │\n│ labeling.<pad><pad><pad><pad><pad><pad><pad>… │ applications.<pad><pad><pad><pad><pad><pad>… │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ MultiCoNER is a fine-grained multilingual     │ The MultiCoNER dataset, available in 12      │ [0.5486, 0.4514] │\n│ named entity recognition dataset covering 12  │ languages, is a fine-grained multilingual    │                  │\n│ languages, including Bangla, Chinese,         │ named entity recognition (NER) resource      │                  │\n│ English, and others, annotated with 11 entity │ designed for use in the SemEval-2023 Task    │                  │\n│ types such as Location, Person, and Medical,  │ 2.<pad><pad><pad><pad><pad><pad><pad><pad><… │                  │\n│ suitable for training and evaluating models   │                                              │                  │\n│ for named entity recognition                  │                                              │                  │\n│ tasks.<pad><pad><pad><pad><pad><pad><pad><pa… │                                              │                  │\n└───────────────────────────────────────────────┴──────────────────────────────────────────────┴──────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> chosen_text                                   </span>┃<span style=\"font-weight: bold\"> rejected_text                                </span>┃<span style=\"font-weight: bold\"> logits           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n│ This dataset contains persona descriptions    │ The tl;dr summary of the dataset based on    │ [0.4909, 0.5091] │\n│ and traits, which can be used to develop      │ the provided card is unavailable, as no      │                  │\n│ chatbots or virtual assistants that can       │ dataset card has been supplied. Please       │                  │\n│ engage in more personalized and human-like    │ provide a dataset card with information      │                  │\n│ conversations.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;… │ about a specific dataset to generate an      │                  │\n│                                               │ accurate tl;dr                               │                  │\n│                                               │ summary.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;… │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ The HelpSteer dataset contains 37,120 samples │ HelpSteer is a dataset containing 37,120     │ [0.5879, 0.4121] │\n│ with prompts and responses, along with        │ samples of prompts, responses, and           │                  │\n│ human-annotated attributes for helpfulness,   │ human-annotated attributes, enabling the     │                  │\n│ correctness, coherence, complexity, and       │ training of models to become more helpful,   │                  │\n│ verbosity, designed to aid in aligning models │ factually correct, coherent, and adjustable  │                  │\n│ to provide more helpful, factually correct,   │ in terms of complexity and                   │                  │\n│ and coherent                                  │ verbosity.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa… │                  │\n│ responses.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad… │                                              │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ The Roboflow Website Screenshots dataset is a │ This dataset contains annotated screenshots  │ [0.5049, 0.4951] │\n│ synthetic dataset containing over 1000        │ from over 1000 top websites, labeled with    │                  │\n│ annotated screenshots from popular websites,  │ various HTML elements such as buttons,       │                  │\n│ useful for Robotic Process Automation and     │ headings, links, and images, which can be    │                  │\n│ offering a cost-effective alternative to      │ used for Robotic Process Automation and      │                  │\n│ human                                         │ related                                      │                  │\n│ labeling.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;… │ applications.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;… │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ MultiCoNER is a fine-grained multilingual     │ The MultiCoNER dataset, available in 12      │ [0.5486, 0.4514] │\n│ named entity recognition dataset covering 12  │ languages, is a fine-grained multilingual    │                  │\n│ languages, including Bangla, Chinese,         │ named entity recognition (NER) resource      │                  │\n│ English, and others, annotated with 11 entity │ designed for use in the SemEval-2023 Task    │                  │\n│ types such as Location, Person, and Medical,  │ 2.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;… │                  │\n│ suitable for training and evaluating models   │                                              │                  │\n│ for named entity recognition                  │                                              │                  │\n│ tasks.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa… │                                              │                  │\n└───────────────────────────────────────────────┴──────────────────────────────────────────────┴──────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7/7 00:02]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"***** eval metrics *****\n  epoch                   =        3.0\n  eval_accuracy           =     0.3333\n  eval_loss               =      0.702\n  eval_runtime            = 0:00:03.57\n  eval_samples_per_second =     29.389\n  eval_steps_per_second   =      1.959\n","output_type":"stream"}]},{"cell_type":"code","source":"# To push the new model to HuggingFace:\ntrainer.save_model(\"/kaggle/working/output_dir/my_reward_model\")","metadata":{"execution":{"iopub.status.busy":"2024-10-06T17:42:12.796686Z","iopub.execute_input":"2024-10-06T17:42:12.797008Z","iopub.status.idle":"2024-10-06T17:42:12.948767Z","shell.execute_reply.started":"2024-10-06T17:42:12.796974Z","shell.execute_reply":"2024-10-06T17:42:12.948003Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, pipeline, DataCollatorForLanguageModeling\nfrom transformers import LogitsProcessorList, MinLengthLogitsProcessor\nfrom trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\nfrom tqdm import tqdm\n\n# Load the dataset\ndataset = load_dataset(\"scitldr\")\n\n# Remove unwanted columns from all dataset splits\ncolumns_to_remove = ['source_labels', 'rouge_scores', 'paper_id']\nfor split in ['train', 'validation', 'test']:\n    dataset[split] = dataset[split].remove_columns(columns_to_remove)\n\n# Set up PPO config with a GPT-like model (causal language model)\nconfig = PPOConfig(\n    model_name=\"gpt2\",  # Use GPT-2 or another causal language model\n    learning_rate=1.41e-5,\n    mini_batch_size=1,\n    batch_size=1,\n    gradient_accumulation_steps=1\n)\n\n# Union function to concatenate the source field\ndef union(sample):\n    return {'union': \" \".join(sample['source']).strip()}\n\n# Apply the union function to all dataset splits\nunion_dataset = dataset.map(\n    union,\n    batched=False,  # Process samples one at a time\n    num_proc=4,     # Parallel processing for speed\n    remove_columns=dataset['train'].column_names\n)\n\n# Load the GPT-2 model and tokenizer\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\ntokenizer = AutoTokenizer.from_pretrained(config.model_name)\ntokenizer.pad_token = tokenizer.eos_token  # Ensure padding token is set correctly\n\n# Set up reward model pipeline (use the model trained for rewards)\nreward_model = pipeline(\n    \"summarization\",\n    model=\"/kaggle/working/output_dir/my_reward_model\",  # Path to your reward model\n    device=0 if torch.cuda.is_available() else -1\n)\n\n# Tokenization function\ndef tokenize(sample):\n    return tokenizer(sample['union'], padding='max_length', max_length=200, truncation=True)\n\n# Tokenize the dataset\ntokenized_dataset = union_dataset.map(\n    tokenize,\n    batched=False,\n    num_proc=4,\n    remove_columns=union_dataset['train'].column_names\n)\n\n# Data collator to handle padding\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n\neos_token_id = torch.tensor(tokenizer.eos_token_id).to(device)\n\nlogits_processor = LogitsProcessorList([\n    MinLengthLogitsProcessor(min_length=10, eos_token_id=eos_token_id),  # Ensure min length\n])\n\n# Initialize PPOTrainer\nppo_trainer = PPOTrainer(\n    model=model,\n    config=config,\n    dataset=tokenized_dataset['train'],\n    tokenizer=tokenizer,\n    data_collator=data_collator\n)\n\n# Keywords to pass to model.generate at every SFT step\ngeneration_kwargs = {\n    \"min_length\": -1,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.eos_token_id,\n    \"max_length\": 400,\n    \"logits_processor\":logits_processor\n}\n\n# Training loop\nepochs = 1\nfor epoch in tqdm(range(epochs), desc=\"epoch\"):\n    for batch in tqdm(ppo_trainer.dataloader, desc=\"batch\"): \n        query_tensors = batch[\"input_ids\"]\n\n        # Loop through the batch, generating responses for each sample individually\n        for query_tensor in query_tensors:\n            # Ensure query tensors are correctly shaped (1D tensors)\n            query_tensor = query_tensor.squeeze()\n\n            print(tokenizer.decode(query_tensor))\n\n            # Generate responses using the model (wrap query_tensor in a list)\n            response_tensors = ppo_trainer.generate([query_tensor], **generation_kwargs)\n            batch[\"target\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n\n            # Combine queries and responses for reward calculation\n            texts = [tokenizer.decode(query_tensor) + tokenizer.decode(r.squeeze()) for r in response_tensors]\n            \n            # Get the summaries from the reward model (summarization)\n            summaries = reward_model(texts)\n            print(summaries)\n\n            # Calculate a simple reward based on length difference (convert rewards to float)\n            rewards = []\n            for query, summary in zip(tokenizer.decode(query_tensor), summaries):\n                # A simple reward calculation: length difference between original and summary\n                reward = torch.tensor(len(summary[\"summary_text\"]) - len(query)).float()  # Cast to float\n                rewards.append(reward)\n\n            # Perform PPO step (pass query_tensor and response_tensors as lists)\n            stats = ppo_trainer.step([query_tensor], response_tensors, rewards)\n            ppo_trainer.log_stats(stats, batch, rewards)\n\n# Save the PPO-trained model\nppo_trainer.save_model(\"/kaggle/working/output_dir/my_ppo_model\")\n\n# Evaluate the results\neval_results = ppo_trainer.evaluate()\nprint(f\"Perplexity: {torch.exp(torch.tensor(eval_results['eval_loss'])):.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T17:43:07.447407Z","iopub.execute_input":"2024-10-06T17:43:07.448379Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"epoch:   0%|          | 0/1 [00:00<?, ?it/s]\nbatch:   0%|          | 0/1992 [00:00<?, ?it/s]\u001b[AYou're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"name":"stdout","text":"In this paper, we explore new approaches to combining information encoded within the learned representations of autoencoders. We explore models that are capable of combining the attributes of multiple inputs such that a resynthesised output is trained to fool an adversarial discriminator for real versus synthesised data. Furthermore, we explore the use of such an architecture in the context of semi-supervised learning, where we learn a mixing function whose objective is to produce interpolations of hidden states, or masked combinations of latent representations that are consistent with a conditioned class label. We show quantitative and qualitative evidence that such a formulation is an interesting avenue of research.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n","output_type":"stream"},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (812 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"[{'summary_text': 'in this paper, we explore new approaches to combining information encoded within the learned representations of autoencoders . we explore models that are capable of combining attributes of multiple inputs such that a resynthesised output is trained to fool an adversarial discriminator . also explore the use of such an architecture in the context of semi-supervised learning .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   0%|          | 1/1992 [00:03<2:12:28,  3.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recently, there has been a surge in interest in safe and robust techniques within reinforcement learning (RL). \n Current notions of risk in RL fail to capture the potential for systemic failures such as abrupt stoppages from system failures or surpassing of safety thresholds and the appropriate responsive controls in such instances. We propose a novel approach to fault-tolerance within RL in which the controller learns a policy can cope with adversarial attacks and random stoppages that lead to failures of the system subcomponents. The results of the paper also cover fault-tolerant (FT) control so that the controller learns to avoid states that carry risk of system failures. By demonstrating that the class of problems is represented by a variant of SGs, we prove the existence of a solution which is a unique fixed point equilibrium of the game and characterise the optimal controller behaviour. We then introduce a value function approximation algorithm that converges to the solution through simulation in unknown environments.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'current notions of risk in RL fail to capture potential for systemic failures such as abrupt stoppages from system failures or surpassing of safety thresholds . we propose a novel approach to fault-tolerance within RL in which the controller learns a policy can cope with adversarial attacks . by demonstrating that the class of problems is represented by a variant of SGs, we prove the existence of a solution which is unique fixed point equilibrium of the game .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   0%|          | 2/1992 [00:08<2:15:28,  4.08s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The verification of planning domain models is crucial to ensure the safety, integrity and correctness of planning-based automated systems. This task is usually performed using model checking techniques.   However, directly applying model checkers to verify planning domain models can result in false positives, i.e. counterexamples that are unreachable by a sound planner when using the domain under verification during a planning task. In this paper, we discuss the downside of unconstrained planning domain model verification. We then propose a fail-safe practice for designing planning domain models that can inherently guarantee the safety of the produced plans in case of undetected errors in domain models.   In addition, we demonstrate how model checkers, as well as state trajectory constraints planning techniques, should be used to verify planning domain models so that unreachable counterexamples are not returned.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'directly applying model checkers to verify planning domain models can result in false positives, i.e. counterexamples that are unreachable by a sound planner when using the domain under verification during a planning task . in this paper, we discuss the downside of unconstrained planning domain model verification .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   0%|          | 3/1992 [00:12<2:15:28,  4.09s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generating complex discrete distributions remains as one of the challenging problems in machine learning. Existing techniques for generating complex distributions with high degrees of freedom depend on standard generative models like Generative Adversarial Networks (GAN), Wasserstein GAN, and associated variations. Such models are based on an optimization involving the distance between two continuous distributions. We introduce a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions. We derive a novel training algorithm and corresponding network architecture based on the formulation. Experimental results are provided for both synthetic discrete data, and real discretized data from MNIST handwritten digits.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'existing techniques for generating complex distributions depend on standard generative models like Generative Adversarial Networks (GAN), Wasserstein GAN, and associated variations . such models are based on an optimization involving the distance between two continuous distributions . experimental results are provided for both synthetic discrete data, and real discretized data from MNIST handwritten digits.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   0%|          | 4/1992 [00:16<2:11:57,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The large memory requirements of deep neural networks strain the capabilities of many devices, limiting their deployment and adoption. Model compression methods effectively reduce the memory requirements of these models, usually through applying transformations such as weight pruning or quantization. In this paper, we present a novel scheme for lossy weight encoding which complements conventional compression techniques. The encoding is based on the Bloomier filter, a probabilistic data structure that can save space at the cost of introducing random errors. Leveraging the ability of neural networks to tolerate these imperfections and by re-training around the errors, the proposed technique, Weightless, can compress DNN weights by up to 496x; with the same model accuracy, this results in up to a 1.51x improvement over the state-of-the-art.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the encoding is based on the Bloomier filter, a probabilistic data structure that can save space at the cost of introducing random errors . the proposed technique, Weightless, can compress DNN weights by up to 496x . this results in up to a 1.51x improvement over the state-of-the-art .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   0%|          | 5/1992 [00:18<1:55:37,  3.49s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Answering complex logical queries on large-scale incomplete knowledge graphs (KGs) is a fundamental yet challenging task. Recently, a promising approach to this problem has been to embed KG entities as well as the query into a vector space such that entities that answer the query are embedded close to the query. However, prior work models queries as single points in the vector space, which is problematic because a complex query represents a potentially large set of its answer entities, but it is unclear how such a set can be represented as a single point. Furthermore, prior work can only handle queries that use conjunctions ($\\wedge$) and existential quantifiers ($\\exists$). Handling queries with logical disjunctions ($\\vee$) remains an open problem. Here we propose query2box, an embedding-based framework for reasoning over arbitrary queries with $\\wedge$, $\\vee$, and $\\exists$ operators in massive and incomplete KGs. Our main insight is that\n[{'summary_text': 'query2box is an embedding-based framework for reasoning over arbitrary queries with $wedge$, $vee$, and $exists$ operators in massive and incomplete KGs . a complex query represents a potentially large set of its answer entities, but it is unclear how such a set can be represented as a single point . prior work models queries as single points in the vector space .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   0%|          | 6/1992 [00:22<1:56:50,  3.53s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"A framework for efficient Bayesian inference in probabilistic programs is introduced by embedding a sampler inside a variational posterior approximation. Its strength lies in both ease of implementation and automatically tuning sampler parameters to speed up mixing time. Several strategies to approximate the evidence lower bound (ELBO) computation are introduced, including a rewriting of the ELBO objective. Experimental evidence is shown by performing experiments on an unconditional VAE on density estimation tasks; solving an influence diagram in a high-dimensional space with a conditional variational autoencoder (cVAE) as a deep Bayes classifier; and state-space models for time-series data.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'a framework for efficient Bayesian inference in probabilistic programs is introduced by embedding a sampler inside a variational posterior approximation . its strength lies in both ease of implementation and automatically tuning sampler parameters to speed up mixing time . experimental evidence is shown by performing experiments on an unconditional VAE on density estimation tasks .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   0%|          | 7/1992 [00:26<1:59:00,  3.60s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose a neural clustering model that jointly learns both latent features and how they cluster. Unlike similar methods our model does not require a predefined number of clusters. Using a supervised approach, we agglomerate latent features towards randomly sampled targets within the same space whilst progressively removing the targets until we are left with only targets which represent cluster centroids. To show the behavior of our model across different modalities we apply our model on both text and image data and very competitive results on MNIST. Finally, we also provide results against baseline models for fashion-MNIST, the 20 newsgroups dataset, and a Twitter dataset we ourselves create.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we propose a neural clustering model that jointly learns both latent features and how they cluster . Unlike similar methods our model does not require a predefined number of clusters . we apply our model on both text and image data and very competitive results on MNIST .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   0%|          | 8/1992 [00:29<1:53:50,  3.44s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"While real brain networks exhibit functional modularity, we investigate whether functional mod- ularity also exists in Deep Neural Networks (DNN) trained through back-propagation. Under the hypothesis that DNN are also organized in task-specific modules, in this paper we seek to dissect a hidden layer into disjoint groups of task-specific hidden neurons with the help of relatively well- studied neuron attribution methods. By saying task-specific, we mean the hidden neurons in the same group are functionally related for predicting a set of similar data samples, i.e. samples with similar feature patterns.\n We argue that such groups of neurons which we call Functional Modules can serve as the basic functional unit in DNN. We propose a preliminary method to identify Functional Modules via bi- clustering attribution scores of hidden neurons.\n We find that first, unsurprisingly, the functional neurons are highly sparse, i.e., only a small sub- set of neurons are important for predicting a small subset\n[{'summary_text': 'functional mod- ularity also exists in Deep Neural Networks (DNN) trained through back-propagation . by saying task-specific, we mean the hidden neurons in the same group are functionally related for predicting a set of similar data samples, i.e. samples with similar feature patterns .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   0%|          | 9/1992 [00:33<1:59:09,  3.61s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep neural networks, in particular convolutional neural networks, have become highly effective tools for compressing images and solving inverse problems including denoising, inpainting, and reconstruction from few and noisy measurements. This success can be attributed in part to their ability to represent and generate natural images well. Contrary to classical tools such as wavelets, image-generating deep neural networks have a large number of parameters---typically a multiple of their output dimension---and need to be trained on large datasets. \n In this paper, we propose an untrained simple image model, called the deep decoder, which is a deep neural network that can generate natural images from very few weight parameters.\n The deep decoder has a simple architecture with no convolutions and fewer weight parameters than the output dimensionality. This underparameterization enables the deep decoder to compress images into a concise set of network weights, which we show is on par with wavelet-based thresholding. Further, underparam\n[{'summary_text': 'deep neural networks have become highly effective tools for compressing images . this success can be attributed in part to their ability to represent and generate natural images well . the deep decoder has a simple architecture with no convolutions and fewer weight parameters than the output dimensionality .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 10/1992 [00:37<2:02:17,  3.70s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Convolutional networks are not aware of an object's geometric variations, which leads to inefficient utilization of model and data capacity. To overcome this issue, recent works on deformation modeling seek to spatially reconfigure the data towards a common arrangement such that semantic recognition suffers less from deformation. This is typically done by augmenting static operators with learned free-form sampling grids in the image space, dynamically tuned to the data and task for adapting the receptive field. Yet adapting the receptive field does not quite reach the actual goal -- what really matters to the network is the *effective* receptive field (ERF), which reflects how much each pixel contributes. It is thus natural to design other approaches to adapt the ERF directly during runtime. In this work, we instantiate one possible solution as Deformable Kernels (DKs), a family of novel and generic convolutional operators for handling object deformations by directly adapting the ERF while leaving the receptive field untouched. At the heart of\n","output_type":"stream"},{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"},{"name":"stdout","text":"[{'summary_text': 'recent works on deformation modeling seek to spatially reconfigure data towards a common arrangement such that semantic recognition suffers less from deformation . this is typically done by augmenting static operators with learned free-form sampling grids in the image space . but adapting the receptive field does not quite reach the actual goal .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 11/1992 [00:39<1:45:44,  3.20s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Multi-hop question answering requires models to gather information from different parts of a text to answer a question. Most current approaches learn to address this task in an end-to-end way with neural networks, without maintaining an explicit representation of the reasoning process. We propose a method to extract a discrete reasoning chain over the text, which consists of a series of sentences leading to the answer. We then feed the extracted chains to a BERT-based QA model to do final answer prediction. Critically, we do not rely on gold annotated chains or ``supporting facts:'' at training time, we derive pseudogold reasoning chains using heuristics based on named entity recognition and coreference resolution. Nor do we rely on these annotations at test time, as our model learns to extract chains from raw text alone.   We test our approach on two recently proposed large multi-hop question answering datasets: WikiHop and HotpotQA, and achieve state-of-art performance on\n[{'summary_text': 'multi-hop question answering requires models to gather information from different parts of a text to answer a question . most current approaches learn to address this task in an end-to-end way with neural networks, without maintaining an explicit representation of the reasoning process . at training time, we derive pseudogold reasoning chains using heuristics based on named entity recognition and coreference resolution .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 12/1992 [00:41<1:40:13,  3.04s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"While it has not yet been proven, empirical evidence suggests that model generalization is related to local properties of the optima which can be described via the Hessian. We connect model generalization with the local property of a solution under the PAC-Bayes paradigm. In particular, we prove that model generalization ability is related to the Hessian, the higher-order \"smoothness\" terms characterized by the Lipschitz constant of the Hessian, and the scales of the parameters. Guided by the proof, we propose a metric to score the generalization capability of the model, as well as an algorithm that optimizes the perturbed model accordingly.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'empirical evidence suggests that model generalization is related to local properties of the optima which can be described via the Hessian . guided by the proof, we propose a metric to score the generalization capability of the model, as well as an algorithm that optimizes the perturbed model accordingly .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 13/1992 [00:44<1:41:45,  3.09s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Predictive coding theories suggest that the brain learns by predicting observations at various levels of abstraction. One of the most basic prediction tasks is view prediction: how would a given scene look from an alternative viewpoint? Humans excel at this task. Our ability to imagine and fill in missing visual information is tightly coupled with perception: we feel as if we see  the world in 3 dimensions, while in fact, information from only the front surface of the world hits our (2D) retinas. This paper explores the connection between view-predictive representation learning and its role in the development of 3D visual recognition. We propose inverse graphics networks, which take as input 2.5D video streams captured by a moving camera, and map to stable 3D feature maps of the scene, by disentangling the scene content from the motion of the camera. The model can also project its 3D feature maps to novel viewpoints, to predict and match against target views. We propose contrastive prediction losses that\n[{'summary_text': 'predictive coding theories suggest that the brain learns by predicting observations at various levels of abstraction . one of the most basic prediction tasks is view prediction: how would a given scene look from an alternative viewpoint? we feel as if we see the world in 3 dimensions, while information from only the front surface of the world hits our (2D) retinas .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 14/1992 [00:49<1:51:46,  3.39s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36 hours down to 1 hour.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts . we also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36 hours down to 1 hour .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 15/1992 [00:52<1:55:00,  3.49s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Graph neural networks have recently achieved great successes in predicting quantum mechanical properties of molecules. These models represent a molecule as a graph using only the distance between atoms (nodes) and not the spatial direction from one atom to another. However, directional information plays a central role in empirical potentials for molecules, e.g. in angular potentials. To alleviate this limitation we propose directional message passing, in which we embed the messages passed between atoms instead of the atoms themselves. Each message is associated with a direction in coordinate space. These directional message embeddings are rotationally equivariant since the associated directions rotate with the molecule. We propose a message passing scheme analogous to belief propagation, which uses the directional information by transforming messages based on the angle between them. Additionally, we use spherical Bessel functions to construct a theoretically well-founded, orthogonal radial basis that achieves better performance than the currently prevalent Gaussian radial basis functions while using more than 4x fewer parameters. We leverage these innovations to\n[{'summary_text': 'these models represent a molecule as a graph using only the distance between atoms (nodes) and not the spatial direction from one atom to another . however, directional information plays a central role in empirical potentials for molecules, e.g. in angular potentials .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 16/1992 [00:56<1:59:42,  3.63s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The gradient of a deep neural network (DNN) w.r.t. the input provides information that can be used to explain the output prediction in terms of the input features and has been widely studied to assist in interpreting DNNs.  In a linear model (i.e., $g(x)=wx+b$), the gradient corresponds solely to the weights $w$. Such a model can reasonably locally linearly approximate a smooth nonlinear DNN, and hence the weights of this local model are the gradient. The other part, however, of a local linear model, i.e., the bias $b$, is usually overlooked in attribution methods since it is not part of the gradient. In this paper, we observe that since the bias in a DNN also has a non-negligible contribution to the correctness of predictions, it can also play a significant role in understanding DNN behaviors. In particular, we study how to attribute a DNN's bias to its\n[{'summary_text': 'the gradient of a deep neural network (DNN) w.r.t. the input provides information that can be used to explain the output prediction in terms of the input features . in a linear model (i.e., $g(x)=wx+b$), the gradient corresponds solely to the weights $w$ . the other part, however, is usually overlooked in attribution methods since it is not part of the gradient .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 17/1992 [01:01<2:06:52,  3.85s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Inverse problems are ubiquitous in natural sciences and refer to the challenging task of inferring complex and potentially multi-modal posterior distributions over hidden parameters given a set of observations. Typically, a model of the physical process in the form of differential equations is available but leads to intractable inference over its parameters. While the forward propagation of parameters through the model simulates the evolution of the system, the inverse problem of finding the parameters given the sequence of states is not unique. In this work, we propose a generalisation of the Bayesian optimisation framework to approximate inference. The resulting method learns approximations to the posterior distribution by applying Stein variational gradient descent on top of estimates from a Gaussian process model. Preliminary results demonstrate the method's performance on likelihood-free inference for reinforcement learning environments.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'inverse problems are ubiquitous in natural sciences and refer to the challenging task of inferring complex and potentially multi-modal posterior distributions over hidden parameters given a set of observations . Typically, a model of the physical process in the form of differential equations is available but leads to intractable inference over its parameters .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 18/1992 [01:03<1:52:56,  3.43s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Spectral embedding is a popular technique for the representation of graph data. Several regularization techniques have been proposed to improve the quality of the embedding with respect to downstream tasks like clustering. In this paper, we explain on a simple block model the impact of the complete graph regularization, whereby a constant is added to all entries of the adjacency matrix. Specifically, we show that the regularization forces the spectral embedding  to focus on  the  largest blocks, making the representation less sensitive to noise or outliers. We illustrate these results on both  on both synthetic and real data, showing how regularization improves standard clustering scores.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Spectral embedding is a popular technique for the representation of graph data . in this paper, we explain on a simple block model the impact of the complete graph regularization, whereby a constant is added to all entries of the adjacency matrix . this paper shows how regularization improves standard clustering scores .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 19/1992 [01:07<1:57:42,  3.58s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recently, researchers have discovered that the state-of-the-art object classifiers can be fooled easily by small perturbations in the input unnoticeable to human eyes.  It is known that an attacker can generate strong adversarial examples if she knows the classifier parameters . Conversely, a defender can robustify the classifier by retraining if she has the adversarial examples . \nThe cat-and-mouse game nature of attacks and defenses raises the question of the presence of equilibria in the dynamics .\nIn this paper, we present a neural-network based attack class to approximate a larger but intractable class of attacks, and \n formulate the attacker-defender interaction as a zero-sum leader-follower game. We present sensitivity-penalized optimization algorithms to find minimax solutions, which are the best worst-case defenses against whitebox attacks. Advantages of the learning-based attacks and defenses compared to gradient-based attacks and defenses\n[{'summary_text': 'the state-of-the-art object classifiers can be fooled easily by small perturbations in the input unnoticeable to human eyes . it is known that an attacker can generate strong adversarial examples if she knows the classifier parameters . in this paper, we present a neural-network based attack class to approximate a larger but intractable class of attacks .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 20/1992 [01:11<2:04:01,  3.77s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recent efforts to combine Representation Learning with Formal Methods, commonly known as the Neuro-Symbolic Methods, have given rise to a new trend of applying rich neural architectures to solve classical combinatorial optimization problems. In this paper, we propose a neural framework that can learn to solve the Circuit Satisfiability problem. Our framework is built upon two fundamental contributions: a rich embedding architecture that encodes the problem structure and an end-to-end differentiable training procedure that mimics Reinforcement Learning and trains the model directly toward solving the SAT problem. The experimental results show the superior out-of-sample generalization performance of our framework compared to the recently developed NeuroSAT method.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this paper, we propose a neural framework that can learn to solve the Circuit Satisfiability problem . our framework is built upon two fundamental contributions: a rich embedding architecture that encodes the problem structure and an end-to-end differentiable training procedure that mimics Reinforcement Learning . the experimental results show the superior out-of-sample generalization performance of our framework compared to the recently developed NeuroSAT method .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 21/1992 [01:13<1:46:48,  3.25s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Intuitively, image classification should profit from using spatial information. Recent work, however, suggests that this might be overrated in standard CNNs. In this paper, we are pushing the envelope and aim to further investigate the reliance on and necessity of spatial information. We propose and analyze three methods, namely Shuffle Conv, GAP+FC and 1x1 Conv, that destroy spatial information during both training and testing phases. We extensively evaluate these methods on several object recognition datasets (CIFAR100, Small-ImageNet, ImageNet) with a wide range of CNN architectures (VGG16, ResNet50, ResNet152, MobileNet, SqueezeNet). Interestingly, we consistently observe that spatial information can be completely deleted from a significant number of layers with no or only small performance drops.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this paper, we are pushing the envelope and aim to further investigate the reliance on and necessity of spatial information . we propose and analyze three methods, namely Shuffle Conv, GAP+FC and 1x1 Conv that destroy spatial information during both training and testing phases .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 22/1992 [01:16<1:41:06,  3.08s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper, we propose two methods, namely Trace-norm regression (TNR) and Stable Trace-norm Analysis (StaTNA), to improve performances of recommender systems with side information. Our trace-norm regression approach extracts low-rank latent factors underlying the side information that drives user preference under different context. Furthermore, our novel recommender framework StaTNA not only captures latent low-rank common drivers for user preferences, but also considers idiosyncratic taste for individual users. We compare performances of TNR and StaTNA on the MovieLens datasets against state-of-the-art models, and demonstrate that StaTNA and TNR in general outperforms these methods.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'our trace-norm regression approach extracts low-rank latent factors . underlying the side information that drives user preference under different context . our novel recommender framework StaTNA also considers idiosyncratic taste .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 23/1992 [01:18<1:32:42,  2.83s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Answering questions that require multi-hop reasoning at web-scale necessitates retrieving multiple evidence documents, one of which often has little lexical or semantic relationship to the question. This paper introduces a new graph-based recurrent retrieval approach that learns to retrieve reasoning paths over the Wikipedia graph to answer multi-hop open-domain questions. Our retriever model trains a recurrent neural network that learns to sequentially retrieve evidence paragraphs in the reasoning path by conditioning on the previously retrieved documents. \n Our reader model ranks the reasoning paths and extracts the answer span included in the best reasoning path.\n Experimental results show state-of-the-art results in three open-domain QA datasets, showcasing the effectiveness and robustness of our method. Notably, our method achieves significant improvement in HotpotQA, outperforming the previous best model by more than 14 points.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'our retriever model trains a recurrent neural network that learns to sequentially retrieve evidence paragraphs in the reasoning path . our reader model ranks the reasoning paths and extracts the answer span included in the best reasoning path, notably, our method achieves significant improvement in HotpotQA .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 24/1992 [01:20<1:22:10,  2.51s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.<|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). we show that one cause for such failures is the exponential moving average used in the algorithms .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|▏         | 25/1992 [01:22<1:20:28,  2.45s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Prefrontal cortex (PFC) is a part of the brain which is responsible for behavior repertoire. Inspired by PFC functionality and connectivity,  as well as human behavior formation process, we propose a novel modular architecture of neural networks with a Behavioral Module (BM) and corresponding end-to-end training strategy.   This approach allows the efficient learning of behaviors and preferences representation. This property is particularly useful for user modeling (as for dialog agents) and recommendation tasks, as allows learning personalized representations of different user states.   In the experiment with video games playing, the resultsshow that the proposed method allows separation of main task’s objectives andbehaviors between different BMs. The experiments also show network extendability through independent learning of new behavior patterns. Moreover, we demonstrate a strategy for an efficient transfer of newly learned BMs to unseen tasks.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'prefrontal cortex (PFC) is a part of the brain which is responsible for behavior repertoire . inspired by PFC functionality and connectivity, as well as human behavior formation process, we propose a novel modular architecture of neural networks with a Behavioral Module (BM) and corresponding training strategy . this approach allows the efficient learning of behaviors and preferences representation .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|▏         | 26/1992 [01:25<1:21:29,  2.49s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We introduce a systematic framework for quantifying the robustness of classifiers to naturally occurring perturbations of images found in videos. As part of this framework, we construct ImageNet-Vid-Robust, a human-expert--reviewed dataset of 22,668 images grouped into 1,145 sets of perceptually similar images derived from frames in the ImageNet Video Object Detection dataset. We evaluate a diverse array of classifiers trained on ImageNet, including models trained for robustness, and show a median classification accuracy drop of 16\\%. Additionally, we evaluate the Faster R-CNN and R-FCN models for detection, and show that natural perturbations induce both classification as well as localization errors, leading to a median drop in detection mAP of 14 points. Our analysis shows that natural perturbations in the real world are heavily problematic for current CNNs, posing a significant challenge to their deployment in safety-critical environments that require reliable, low-latency predictions.<|endoftext|>\n[{'summary_text': 'we construct ImageNet-Vid-Robust, a human-expert--reviewed dataset of 22,668 images grouped into 1,145 sets of perceptually similar images derived from frames in the ImageNet Video Object Detection dataset . we evaluate a diverse array of classifiers trained on ImageNet, including models trained for robustness, and show a median classification accuracy drop of 16% .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|▏         | 27/1992 [01:29<1:36:36,  2.95s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present a novel black-box adversarial attack algorithm with state-of-the-art model evasion rates for query efficiency under $\\ell_\\infty$ and $\\ell_2$ metrics. It exploits a \\textit{sign-based}, rather than magnitude-based, gradient estimation approach that shifts the gradient estimation from continuous to binary black-box optimization. It adaptively constructs queries to estimate the gradient, one query relying upon the previous, rather than re-estimating the gradient each step with random query construction. Its reliance on sign bits yields  a smaller memory footprint and it requires neither hyperparameter tuning or dimensionality reduction. Further, its theoretical performance is guaranteed and it can characterize  adversarial subspaces better than white-box gradient-aligned subspaces. On two public black-box attack challenges and a model robustly trained against transfer attacks, the algorithm's evasion rates surpass all submitted attacks. For a suite of published models,  the algorithm is $\n[{'summary_text': 'algorithm exploits a textitsign-based, rather than magnitude-based, gradient estimation approach that shifts the gradient estimation from continuous to binary black-box optimization . it adaptively constructs queries to estimate the gradient, one query relying upon the previous, instead of re-estimating the gradient each step with random query construction . its theoretical performance is guaranteed and it can characterize adversarial subspaces better than white-box gradient .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|▏         | 28/1992 [01:33<1:50:23,  3.37s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Although there are more than 65,000 languages in the world, the pronunciations of many phonemes sound similar across the languages. When people learn a foreign language, their pronunciation often reflect their native language's characteristics. That motivates us to investigate how the speech synthesis network learns the pronunciation when multi-lingual dataset is given. In this study, we train the speech synthesis network bilingually in English and Korean, and analyze how the network learns the relations of phoneme pronunciation between the languages. Our experimental result shows that the learned phoneme embedding vectors are located closer if their pronunciations are similar across the languages. Based on the result, we also show that it is possible to train networks that synthesize English speaker's Korean speech and vice versa. In another experiment, we train the network with limited amount of English dataset and large Korean dataset, and analyze the required amount of dataset to train a resource-poor language with the help of resource-rich languages.<|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': \"the pronunciations of many phonemes sound similar across the languages . when people learn a foreign language, their pronunciation often reflect their native language's characteristics . this motivates us to investigate how the speech synthesis network learns the relations of phoneme pronunciation between the languages.\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|▏         | 29/1992 [01:36<1:47:55,  3.30s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recent progress on physics-based character animation has shown impressive breakthroughs on human motion synthesis, through imitating motion capture data via deep reinforcement learning. However, results have mostly been demonstrated on imitating a single distinct motion pattern, and do not generalize to interactive tasks that require flexible motion patterns due to varying human-object spatial configurations. To bridge this gap, we focus on one class of interactive tasks---sitting onto a chair. We propose a hierarchical reinforcement learning framework which relies on a collection of subtask controllers trained to imitate simple, reusable mocap motions, and a meta controller trained to execute the subtasks properly to complete the main task. We experimentally demonstrate the strength of our approach over different single level and hierarchical baselines. We also show that our approach can be applied to motion prediction given an image input. A video highlight can be found at https://youtu.be/XWU3wzz1ip8/.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'recent progress on physics-based character animation has shown impressive breakthroughs on human motion synthesis . but results have mostly been demonstrated on imitating motion capture data via deep reinforcement learning . to bridge this gap, we focus on one class of interactive tasks---sitting onto a chair .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 30/1992 [01:40<1:50:50,  3.39s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Challenges in natural sciences can often be phrased as optimization problems. Machine learning techniques have recently been applied to solve such problems. One example in chemistry is the design of tailor-made organic materials and molecules, which requires efficient methods to explore the chemical space. We present a genetic algorithm (GA) that is enhanced with a neural network (DNN) based discriminator model to improve the diversity of generated molecules and at the same time steer the GA. We show that our algorithm outperforms other generative models in optimization tasks. We furthermore present a way to increase interpretability of genetic algorithms, which helped us to derive design principles<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'machine learning techniques have recently been applied to solve such problems . one example in chemistry is the design of tailor-made organic materials and molecules, which requires efficient methods to explore the chemical space . we present a genetic algorithm (GA) that is enhanced with a neural network (DNN) based discriminator model .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 31/1992 [01:44<1:53:11,  3.46s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"It is difficult for the beginners of etching latte art to make well-balanced patterns by using two fluids with different viscosities such as foamed milk and syrup. Even though making etching latte art while watching making videos which show the procedure, it is difficult to keep balance. Thus well-balanced etching latte art cannot be made easily. \n In this paper, we propose a system which supports the beginners to make well-balanced etching latte art by projecting a making procedure of etching latte art directly onto a cappuccino. \n The experiment results show the progress by using our system.   We also discuss about the similarity of the etching latte art and the design templates by using background subtraction.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'it is difficult for the beginners of etching latte art to make well-balanced patterns by using two fluids with different viscosities such as foamed milk and syrup . even though making etchers while watching making videos which show the procedure, it is hard to keep balance . the experiment results show the progress by using our system.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 32/1992 [01:47<1:55:39,  3.54s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We improve previous end-to-end differentiable neural networks (NNs) with fast\n weight memories. A gate mechanism updates fast weights at every time step of\n a sequence through two separate outer-product-based matrices generated by slow\n parts of the net. The system is trained on a complex sequence to sequence variation\n of the Associative Retrieval Problem with roughly 70 times more temporal\n memory (i.e. time-varying variables) than similar-sized standard recurrent NNs\n (RNNs). In terms of accuracy and number of parameters, our architecture outperforms\n a variety of RNNs, including Long Short-Term Memory, Hypernetworks,\n and related fast weight architectures.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'gate mechanism updates fast weights at every time step of a sequence through two separate outer-product-based matrices generated by slow parts of the net . system is trained on a complex sequence to sequence variation of the Associative Retrieval Problem with roughly 70 times more temporal memory (i.e. time-varying variables) .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 33/1992 [01:50<1:46:11,  3.25s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Auto-encoding and generative models have made tremendous successes in image and signal representation learning and generation. These models, however, generally employ the full Euclidean space or a bounded subset (such as $[0,1]^l$) as the latent space, whose trivial geometry is often too simplistic to meaningfully reflect the structure of the data. This paper aims at exploring a nontrivial geometric structure of the latent space for better data representation. Inspired by differential geometry, we propose \\textbf{Chart Auto-Encoder (CAE)}, which captures the manifold structure of the data with multiple charts and transition functions among them. CAE translates the mathematical definition of manifold through parameterizing the entire data set as a collection of overlapping charts, creating local latent representations. These representations are an enhancement of the single-charted latent space commonly employed in auto-encoding models, as they reflect the intrinsic structure of the manifold.   Therefore, CAE achieves a more accurate\n[{'summary_text': 'auto-encoding and generative models have made tremendous successes in image and signal representation learning and generation . these models, however, generally employ the full Euclidean space or a bounded subset (such as $[0,1]l$) as the latent space, whose trivial geometry is often too simplistic to meaningfully reflect the structure of the data .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 34/1992 [01:54<1:55:36,  3.54s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Presence of bias and confounding effects is inarguably one of the most critical challenges in machine learning applications that has alluded to pivotal debates in the recent years. Such challenges range from spurious associations of confounding variables in medical studies to the bias of race in gender or face recognition systems. One solution is to enhance datasets and organize them such that they do not reflect biases, which is a cumbersome and intensive task. The alternative is to make use of available data and build models considering these biases. Traditional statistical methods apply straightforward techniques such as residualization or stratification to precomputed features to account for confounding variables. However, these techniques are not in general applicable to end-to-end deep learning methods. In this paper, we propose a method based on the adversarial training strategy to learn discriminative features unbiased and invariant to the confounder(s). This is enabled by incorporating a new adversarial loss function that encourages a vanished correlation between the bias and learned features. We apply our method\n[{'summary_text': 'the alternative is to make use of available data and build models considering these biases . traditional statistical methods apply straightforward techniques such as residualization or stratification to precomputed features to account for confounding variables . this is enabled by incorporating a new adversarial loss function that encourages a vanished correlation between the bias and learned features .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 35/1992 [01:58<2:03:06,  3.77s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep learning natural language processing models often use vector word embeddings, such as word2vec or GloVe, to represent words. A discrete sequence of words can be much more easily integrated with downstream neural layers if it is represented as a  sequence of continuous vectors. Also, semantic relationships between words, learned from a text corpus, can be encoded in the relative configurations of the embedding vectors. However, storing and accessing embedding vectors for all words in a dictionary requires large amount of space, and may stain systems with limited GPU memory. Here, we used approaches inspired by quantum computing to propose two related methods, word2ket and word2ketXS, for storing word embedding matrix during training and inference in a highly efficient way. Our approach achieves a hundred-fold or more reduction in the space required to store the embeddings with almost no relative drop in accuracy in practical natural language processing tasks.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'deep learning natural language processing models often use vector word embeddings to represent words . a discrete sequence of words can be much more easily integrated with downstream neural layers if it is represented as a sequence of continuous vectors . also semantic relationships between words, learned from a text corpus, can be encoded in the relative configurations of the embedded vectors, . however, storing and accessing embeddeding vectors for all words in a dictionary requires large amount of space .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 36/1992 [02:03<2:07:14,  3.90s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Adam is shown not being able to converge to the optimal solution in certain cases. Researchers recently propose several algorithms to avoid the issue of non-convergence of Adam, but their efficiency turns out to be unsatisfactory in practice. In this paper, we provide a new insight into the non-convergence issue of Adam as well as other adaptive learning rate methods. We argue that there exists an inappropriate correlation between gradient $g_t$ and the second moment term $v_t$ in Adam ($t$ is the timestep), which results in that a large gradient is likely to have small step size while a small gradient may have a large step size. We demonstrate that such unbalanced step sizes are the fundamental cause of non-convergence of Adam, and we further prove that decorrelating $v_t$ and $g_t$ will lead to unbiased step size for each gradient, thus solving the non-convergence problem of Adam. Finally, we\n[{'summary_text': 'researchers recently propose several algorithms to avoid the issue of non-convergence of Adam . but their efficiency turns out to be unsatisfactory in practice . weAdam is shown not being able to converge to the optimal solution in certain cases .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 37/1992 [02:07<2:08:34,  3.95s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In some important computer vision domains, such as medical or hyperspectral imaging, we care about the classification of tiny objects in large images. However, most Convolutional Neural Networks (CNNs) for image classification were developed using biased datasets that contain large objects, in mostly central image positions. To assess whether classical CNN architectures work well for tiny object classification we build a comprehensive testbed containing two datasets: one derived from MNIST digits and one from histopathology images. This testbed allows controlled experiments to stress-test CNN architectures with a broad spectrum of signal-to-noise ratios. Our observations indicate that: (1) There exists a limit to signal-to-noise below which CNNs fail to generalize and that this limit is affected by dataset size - more data leading to better performances; however, the amount of training data required for the model to generalize scales rapidly with the inverse of the object-to-image ratio (2) in general, higher capacity models\n[{'summary_text': 'some important computer vision domains, such as medical or hyperspectral imaging, we care about the classification of tiny objects in large images . but most Convolutional Neural Networks (CNNs) for image classification were developed using biased datasets that contain large objects, in mostly central image positions . this testbed allows controlled experiments to stress-test CNN architectures with a broad spectrum of signal-to-noise ratios .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 38/1992 [02:11<2:12:05,  4.06s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We study the robust one-bit compressed sensing problem whose goal is to design an algorithm that faithfully recovers any sparse target vector\n $\\theta_0\\in\\mathbb{R}^d$ \\emph{uniformly} from $m$ quantized noisy measurements. Under the assumption that the measurements are sub-Gaussian,  to recover any $k$-sparse $\\theta_0$ ($k\\ll d$) \\emph{uniformly} up to an error $\\varepsilon$ with high probability, the best known computationally tractable algorithm requires\\footnote{Here, an algorithm is ``computationally tractable'' if it has provable convergence guarantees. The notation $\\tilde{\\mathcal{O}}(\\cdot)$ omits a logarithm factor of $\\varepsilon^{-1}$. } $m\\geq\\tilde{\\mathcal{O}}(k\n[{'summary_text': \"the notation $tildemathcalO(cdot)$ omits a logarithm factor of $varepsilon-1$ . the best known computationally tractable algorithm requiresfootnoteHere, an algorithm is computationally tractable'' if it has provable convergence guarantees.\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 39/1992 [02:15<2:14:12,  4.12s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In multiagent systems (MASs), each agent makes individual decisions but all of them contribute globally to the system evolution. Learning in MASs is difficult since each agent's selection of actions must take place in the presence of other co-learning agents. Moreover, the environmental stochasticity and uncertainties increase exponentially with the increase in the number of agents. Previous works borrow various multiagent coordination mechanisms into deep learning architecture to facilitate multiagent coordination. However, none of them explicitly consider action semantics between agents that different actions have different influences on other agents. In this paper, we propose a novel network architecture, named Action Semantics Network (ASN), that explicitly represents such action semantics between agents. ASN characterizes different actions' influence on other agents using neural networks based on the action semantics between them. ASN can be easily combined with existing deep reinforcement learning (DRL) algorithms to boost their performance. Experimental results on StarCraft II micromanagement and Neural MMO show ASN significantly improves the performance\n[{'summary_text': \"learning in MASs is difficult since each agent's selection of actions must take place in the presence of other co-learning agents . the environmental stochasticity and uncertainties increase exponentially with the increase in the number of agents. in this paper, we propose a novel network architecture, named Action Semantics Network (ASN), that explicitly represents such action semantics between agents.\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 40/1992 [02:19<2:14:58,  4.15s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Adversarial examples have somewhat disrupted the enormous success of machine learning (ML) and are causing concern with regards to its trustworthiness: A small perturbation of an input results in an arbitrary failure of an otherwise seemingly well-trained ML system. While studies are being conducted to discover the intrinsic properties of adversarial examples, such as their transferability and universality, there is insufficient theoretic analysis to help understand the phenomenon in a way that can influence the design process of ML experiments. In this paper, we deduce an information-theoretic model which explains adversarial attacks universally as the abuse of feature redundancies in ML algorithms. We prove that feature redundancy is a necessary condition for the existence of adversarial examples. Our model helps to explain the major questions raised in many anecdotal studies on adversarial examples. Our theory is backed up by empirical measurements of the information content of benign and adversarial examples on both image and text datasets. Our measurements show that typical adversarial examples introduce just enough\n[{'summary_text': 'a small perturbation of an input results in an arbitrary failure of an otherwise seemingly well-trained ML system . studies are being conducted to discover the intrinsic properties of adversarial examples, such as their transferability and universality . there is insufficient theoretic analysis to help understand the phenomenon in a way that can influence the design process of ML experiments .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 41/1992 [02:24<2:15:26,  4.17s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose an approach for sequence modeling based on autoregressive normalizing flows. Each autoregressive transform, acting across time, serves as a moving reference frame for modeling higher-level dynamics. This technique provides a simple, general-purpose method for improving sequence modeling, with connections to existing and classical techniques. We demonstrate the proposed approach both with standalone models, as well as a part of larger sequential latent variable models. Results are presented on three benchmark video datasets, where flow-based dynamics improve log-likelihood performance over baseline models.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'each autoregressive transform serves as a moving reference frame for modeling higher-level dynamics . results are presented on three benchmark video datasets, where flow-based dynamics improve log-likelihood performance over baseline models .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 42/1992 [02:27<2:07:42,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Beyond understanding what is being discussed, human communication requires an awareness of what someone is feeling. One challenge for dialogue agents is recognizing feelings in the conversation partner and replying accordingly, a key communicative skill that is trivial for humans. Research in this area is made difficult by the paucity of suitable publicly available datasets both for emotion and dialogues. This work proposes a new task for empathetic dialogue generation and EmpatheticDialogues, a dataset of 25k conversations grounded in emotional situations to facilitate training and evaluating dialogue systems. Our experiments indicate that dialogue models that use our dataset are perceived to be more empathetic by human evaluators, while improving on other metrics as well (e.g. perceived relevance of responses, BLEU scores), compared to models merely trained on large-scale Internet conversation data. We also present empirical comparisons of several ways to improve the performance of a given model by leveraging existing models or datasets without requiring lengthy re-training of the full model.<|endoftext|>\n[{'summary_text': 'research in this area is made difficult by the paucity of suitable publicly available datasets both for emotion and dialogues . this work proposes a new task for empathetic dialogue generation and EmpatheticDialogues, a dataset of 25k conversations grounded in emotional situations .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 43/1992 [02:29<1:48:13,  3.33s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks---as function approximators, and logic programming---as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers.   After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the Neural Logic Machine (NLM) is a neural-symbolic architecture for inductive learning and logic reasoning . NLMs exploit the power of both neural networks---as function approximators, and logic programming --as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers . most of these tasks are hard to accomplish for neural networks or inductive logic programming alone .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 44/1992 [02:33<1:54:32,  3.53s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"When an image classifier makes a prediction, which parts of the image are relevant and why? We can rephrase this question to ask: which parts of the image, if they were not seen by the classifier, would most change its decision? Producing an answer requires marginalizing over images that could have been seen but weren't. We can sample plausible image in-fills by conditioning a generative model on the rest of the image. We then optimize to find the image regions that most change the classifier's decision after in-fill. Our approach contrasts with ad-hoc in-filling approaches, such as blurring or injecting noise, which generate inputs far from the data distribution, and ignore informative relationships between different parts of the image. Our method produces more compact and relevant saliency maps, with fewer artifacts compared to previous methods.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': \"we can sample plausible image in-fills by conditioning a generative model on the rest of the image . we then optimize to find the image regions that most change the classifier's decision after infill . our approach contrasts with ad-hoc infilling approaches, such as blurring or injecting noise .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 45/1992 [02:37<1:55:46,  3.57s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The inference of models, prediction of future symbols, and entropy rate estimation of discrete-time, discrete-event processes is well-worn ground. However, many time series are better conceptualized as continuous-time, discrete-event processes. Here, we provide new methods for inferring models, predicting future symbols, and estimating the entropy rate of continuous-time, discrete-event processes. The methods rely on an extension of Bayesian structural inference that takes advantage of neural network’s universal approximation power. Based on experiments with simple synthetic data, these new methods seem to be competitive with state-of- the-art methods for prediction and entropy rate estimation as long as the correct model is inferred.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the methods rely on an extension of Bayesian structural inference that takes advantage of neural network’s universal approximation power . these new methods seem to be competitive with state-of-the-art methods for prediction and entropy rate estimation as long as the correct model is inferred .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 46/1992 [02:40<1:56:58,  3.61s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The fields of artificial intelligence and neuroscience have a long history of fertile bi-directional interactions. On the one hand, important inspiration for the development of artificial intelligence systems has come from the study of natural systems of intelligence, the mammalian neocortex in particular. On the other, important inspiration for models and theories of the brain have emerged from artificial intelligence research. A central question at the intersection of these two areas is concerned with the processes by which neocortex learns, and the extent to which they are analogous to the back-propagation training algorithm of deep networks. Matching the data efficiency, transfer and generalisation properties of neocortical learning remains an area of active research in the field of deep learning. Recent advances in our understanding of neuronal, synaptic and dendritic physiology of the neocortex suggest new approaches for unsupervised representation learning, perhaps through a new class of objective functions, which could act alongside or in lieu of back-propagation. Such local learning rules have implicit rather\n[{'summary_text': 'important inspiration for the development of artificial intelligence systems has come from the study of natural systems of intelligence . a central question at the intersection of these two areas is concerned with the processes by which neocortex learns, and the extent to which they are analogous to the back-propagation training algorithm .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 47/1992 [02:44<2:01:13,  3.74s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The prohibitive energy cost of running high-performance Convolutional Neural Networks (CNNs) has been limiting their deployment on resource-constrained platforms including mobile and wearable devices. We propose a CNN for energy-aware dynamic routing, called the EnergyNet, that achieves adaptive-complexity inference based on the inputs, leading to an overall reduction of run time energy cost without noticeably losing (or even improving) accuracy. That is achieved by proposing an energy loss that captures both computational and data movement costs. We combine it with the accuracy-oriented loss, and learn a dynamic routing policy for skipping certain layers in the networks, that optimizes the hybrid loss.   Our empirical results demonstrate that, compared to the baseline CNNs, EnergyNetcan trim down the energy cost up to 40% and 65%, during inference on the CIFAR10 and Tiny ImageNet testing sets, respectively, while maintaining the same testing accuracies.   It is further encouraging to observe that the energy awareness\n[{'summary_text': 'prohibitive energy cost of running high-performance Convolutional Neural Networks (CNNs) has been limiting their deployment on resource-constrained platforms including mobile and wearable devices . we propose a CNN for energy-aware dynamic routing, called the EnergyNet, that achieves adaptive-complexity inference based on the inputs . that is achieved by proposing an energy loss that captures both computational and data movement costs .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 48/1992 [02:49<2:06:42,  3.91s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Partially observable Markov decision processes (POMDPs) are a natural model for scenarios where one has to deal with incomplete knowledge and random events.\n Applications include, but are not limited to, robotics and motion planning.\n However, many relevant properties of POMDPs are either undecidable or very expensive to compute in terms of both runtime and memory consumption.\n In our work, we develop a game-based abstraction method that is able to deliver safe bounds and tight\n  approximations for important sub-classes of such properties.\n We discuss the theoretical implications and showcase the applicability of our results on a broad spectrum of benchmarks.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'applications include, but are not limited to, robotics and motion planning . many relevant properties of POMDPs are either undecidable or very expensive to compute in terms of both runtime and memory consumption . we discuss the theoretical implications and showcase the applicability of our results on a broad spectrum of benchmarks .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 49/1992 [02:52<2:03:16,  3.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present Line-Storm, an interactive computer system for creative performance. The context we investigated was writing on paper using Line-Storm. We used self-report questionnaires as part of research involving human participants, to evaluate Line-Storm. Line-Storm consisted of a writing stylus and writing pad, augmented with electronics. The writing pad was connected to a contact microphone, and the writing stylus had a small micro-controller board and peripherals attached to it. The signals from these electronic augmentations were fed into the audio-synthesis environment Max/MSP to produce an interactive soundscape. We attempted to discover whether Line-Storm enhanced a self-reported sense of being present and engaged during a writing task, and we compared Line-Storm to a non-interactive control condition. After performing statistical analysis in SPSS, we were unable to support our research hypothesis, that presence and engagement were enhanced by Line-Storm. Participants reported they were, on average, no more\n[{'summary_text': 'line-Storm consisted of a writing stylus and writing pad, augmented with electronics . the writing pad was connected to a contact microphone, and the pen stylus had a small micro-controller board and peripherals attached to it . participants reported they were, on average, no more than sexualienced .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 50/1992 [02:56<2:06:14,  3.90s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We study the precise mechanisms which allow autoencoders to encode and decode a simple geometric shape, the disk. In this carefully controlled setting, we are able to describe the specific form of the optimal solution to the minimisation problem of the training step. We show that the autoencoder indeed approximates this solution during training. Secondly, we identify a clear failure in the generalisation capacity of the autoencoder, namely its inability to interpolate data. Finally, we explore several regularisation schemes to resolve the generalisation problem. Given the great attention that has been recently given to the generative capacity of neural networks, we believe that studying in depth simple geometric cases sheds some light on the generation process and can provide a minimal requirement experimental setup for more complex architectures.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the autoencoder indeed approximates this solution during training . in this carefully controlled setting, we are able to describe the specific form of the optimal solution to the minimisation problem of the training step.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 51/1992 [02:58<1:48:44,  3.36s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Bayesian optimization (BO) is a popular methodology to tune the hyperparameters of expensive black-box functions. Despite its success, standard BO focuses on a single task at a time and is not designed to leverage information from related functions, such as tuning performance metrics of the same algorithm across multiple datasets. In this work, we introduce a novel approach to achieve transfer learning across different datasets as well as different metrics. The main idea is to regress the mapping from hyperparameter to metric quantiles with a semi-parametric Gaussian Copula distribution, which provides robustness against different scales or outliers that can occur in different tasks. We introduce two methods to leverage this estimation: a Thompson sampling strategy as well as a Gaussian Copula process using such quantile estimate as a prior. We show that these strategies can combine the estimation of multiple metrics such as runtime and accuracy, steering the optimization toward cheaper hyperparameters for the same level of accuracy. Experiments on an extensive set of hyper\n[{'summary_text': 'despite its success, standard BO focuses on a single task at a time . it is not designed to leverage information from related functions, such as tuning performance metrics of the same algorithm across multiple datasets . the main idea is to regress the mapping from hyperparameter to metric quantiles with a semi-parametric Gaussian Copula distribution .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 52/1992 [03:03<1:56:37,  3.61s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Unsupervised and semi-supervised learning are important problems that are especially challenging with complex data like natural images. Progress on these problems would accelerate if we had access to appropriate generative models under which to pose the associated inference tasks. Inspired by the success of Convolutional Neural Networks (CNNs) for supervised prediction in images, we design the Neural Rendering Model (NRM), a new hierarchical probabilistic generative model whose inference calculations correspond to those in a CNN. The NRM introduces a small set of latent variables at each level of the model and enforces dependencies among all the latent variables via a conjugate prior distribution. The conjugate prior yields a new regularizer for learning based on the paths rendered in the generative model for training CNNs–the Rendering Path Normalization (RPN). We demonstrate that this regularizer improves generalization both in theory and in practice. Likelihood estimation in the NRM yields the new Max-Min cross entropy training loss\n[{'summary_text': 'unsupervised and semi-supervised learning are important problems that are especially challenging with complex data like natural images . progress on these problems would accelerate if we had access to appropriate generative models under which to pose the associated inference tasks . the NRM introduces a small set of latent variables at each level of the model .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 53/1992 [03:07<2:00:09,  3.72s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others . we show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 54/1992 [03:10<1:59:30,  3.70s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Sentiment classification is an active research area with several applications including analysis of political opinions, classifying comments, movie reviews, news reviews and product reviews. To employ rule based sentiment classification, we require sentiment lexicons. However, manual construction of sentiment lexicon is time consuming and costly for resource-limited languages. To bypass manual development time and costs, we tried to build Amharic Sentiment Lexicons relying on corpus based approach. The intention of this approach is to handle sentiment terms specific to Amharic language from Amharic Corpus. Small set of seed terms are manually prepared from three parts of speech such as noun, adjective and verb. We developed algorithms for constructing Amharic sentiment lexicons automatically from Amharic news corpus. Corpus based approach is proposed relying on the word co-occurrence distributional embedding including frequency based embedding (i.e. Positive Point-wise Mutual Information PPMI). First we build word-context unigram frequency count matrix and transform\n[{'summary_text': 'Sentiment classification is an active research area with several applications including analysis of political opinions, classifying comments, movie reviews, news reviews and product reviews . to employ rule based sentiment classification, we require sentiment lexicons .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 55/1992 [03:14<2:02:05,  3.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The information bottleneck method provides an information-theoretic method for representation learning, by training an encoder to retain all information which is relevant for predicting the label, while minimizing the amount of other, superfluous information in the representation. The original formulation, however, requires labeled data in order to identify which information is superfluous.   In this work, we extend this ability to the multi-view unsupervised setting, in which two views of the same underlying entity are provided but the label is unknown. This enables us to identify superfluous information as that which is not shared by both views. A theoretical analysis leads to the definition of a new multi-view model that produces state-of-the-art results on the Sketchy dataset and on label-limited versions of the MIR-Flickr dataset.   We also extend our theory to the single-view setting by taking advantage of standard data augmentation techniques, empirically showing better generalization capabilities when compared to traditional unsuper\n[{'summary_text': 'the information bottleneck method provides an information-theoretic method for representation learning . the original formulation requires labeled data in order to identify which information is superfluous . a theoretical analysis leads to the definition of a new multi-view model that produces state-of-the-art results on the Sketchy dataset and on label-limited versions of the MIR-Flickr dataset .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 56/1992 [03:18<2:05:31,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep generative models such as Variational AutoEncoder (VAE) and Generative Adversarial Network (GAN) play an increasingly important role in machine learning and computer vision. However, there are two fundamental issues hindering their real-world applications: the difficulty of conducting variational inference in VAE and the functional absence of encoding real-world samples in GAN. In this paper, we propose a novel algorithm named Latently Invertible Autoencoder (LIA) to address the above two issues in one framework. An invertible network and its inverse mapping are symmetrically embedded in the latent space of VAE. Thus the partial encoder first transforms the input into feature vectors and then the distribution of these feature vectors is reshaped to fit a prior by the invertible network. The decoder proceeds in the reverse order of the encoder's composite mappings. A two-stage stochasticity-free training scheme is designed to train LIA via\n[{'summary_text': 'the difficulty of conducting variational inference in VAE and the functional absence of encoding real-world samples in GAN . in this paper, we propose a novel algorithm named Latently Invertible Autoencoder (LIA) to address the above two issues in one framework . an invertible network and its inverse mapping are symmetrically embedded in the latent space of VAE .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 57/1992 [03:23<2:08:52,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM, which uses DropConnect on hidden-to-hidden weights, as a form of recurrent regularization. Further, we introduce NT-ASGD, a non-monotonically triggered  (NT) variant of the averaged stochastic gradient method (ASGD), wherein the averaging trigger is determined using a NT condition as opposed to being tuned by the user. Using these and other regularization strategies, our ASGD Weight-Dropped LSTM (AWD-LSTM) achieves state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art\n[{'summary_text': 'we propose the weight-dropped LSTM, which uses DropConnect on hidden-to-hidden weights, as a form of recurrent regularization . we introduce NT-ASGD, a non-monotonically triggered (NT) variant of the averaged stochastic gradient method (ASGD), wherein the averaging trigger is determined using a NT condition as opposed to being tuned by the user.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 58/1992 [03:27<2:12:25,  4.11s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We show implicit filter level sparsity manifests in convolutional neural networks (CNNs) which employ Batch Normalization and ReLU activation, and are trained using adaptive gradient descent techniques with L2 regularization or weight decay. Through an extensive empirical study (Anonymous, 2019) we hypothesize the mechanism be hind the sparsification process. We find that the interplay  of  various  phenomena  influences  the strength of L2 and weight decay regularizers, leading the supposedly non sparsity inducing regularizers to induce filter sparsity.   In this workshop article we summarize some of our key findings and experiments, and present additional results on modern network architectures such as ResNet-50.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'implicit filter level sparsity manifests in convolutional neural networks (CNNs) which employ Batch Normalization and ReLU activation . we hypothesize the mechanism be hind the sparsification process . the interplay of various phenomena influences the strength of L2 and weight decay regularizers .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 59/1992 [03:31<2:07:35,  3.96s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Inferring the most likely configuration for a subset of variables of a joint distribution given the remaining ones – which we refer to as co-generation – is an important challenge that is computationally demanding for all but the simplest settings. This task has received a considerable amount of attention, particularly for classical ways of modeling distributions like structured prediction. In contrast, almost nothing is known about this task when considering recently proposed techniques for modeling high-dimensional distributions, particularly generative adversarial nets (GANs). Therefore, in this paper, we study the occurring challenges for co-generation with GANs. To address those challenges we develop an annealed importance sampling (AIS) based Hamiltonian Monte Carlo (HMC) co-generation algorithm. The presented approach significantly outperforms classical gradient-based methods on synthetic data and on CelebA.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'this task has received considerable amount of attention, particularly for classical ways of modeling distributions like structured prediction . almost nothing is known about this task when considering recently proposed techniques for modeling high-dimensional distributions, particularly generative adversarial nets (GANs)'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 60/1992 [03:34<2:03:30,  3.84s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We introduce the open-ended, modular, self-improving Omega AI unification architecture which is a refinement of Solomonoff's Alpha architecture, as considered from first principles. The architecture embodies several crucial principles of general intelligence including diversity of representations, diversity of data types, integrated memory, modularity, and higher-order cognition. We retain the basic design of a fundamental algorithmic substrate called an ``AI kernel'' for problem solving and basic cognitive functions like memory, and a larger, modular architecture that re-uses the kernel in many ways. Omega includes eight representation languages and six classes of neural networks, which are briefly introduced. The architecture is intended to initially address data science automation, hence it includes many problem solving methods for statistical tasks. We review the broad software architecture, higher-order cognition, self-improvement, modular neural architectures, intelligent agents, the process and memory hierarchy, hardware abstraction, peer-to-peer computing, and data abstraction facility.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we introduce the open-ended, modular, self-improving Omega AI unification architecture . the architecture embodies several crucial principles of general intelligence including diversity of representations, diversity of data types, integrated memory, modularity, and higher-order cognition . Omega includes eight representation languages and six classes of neural networks, which are briefly introduced .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 61/1992 [03:38<2:02:35,  3.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Which generative model is the most suitable for Continual Learning? This paper aims at evaluating and comparing generative models on disjoint sequential image generation tasks. We investigate how several models learn and forget, considering various strategies: rehearsal, regularization, generative replay and fine-tuning. We used two quantitative metrics to estimate the generation quality and memory ability. We experiment with sequential tasks on three commonly used benchmarks for Continual Learning (MNIST, Fashion MNIST and CIFAR10). We found that among all models, the original GAN performs best and among Continual Learning strategies, generative replay outperforms all other methods. Even if we found satisfactory combinations on MNIST and Fashion MNIST, training generative models sequentially on CIFAR10 is particularly instable, and remains a challenge.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'this paper aims at evaluating and comparing generative models on disjoint sequential image generation tasks . we investigate how several models learn and forget, considering various strategies: rehearsal, regularization, generative replay and fine-tuning .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 62/1992 [03:42<2:04:22,  3.87s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU). We give an algorithm to train a ReLU DNN with one hidden layer to {\\em global optimality} with runtime polynomial in the data size albeit exponential in the input dimension. Further, we improve on the known lower bounds on size (from exponential to super exponential) for approximating a ReLU deep net function by a shallower ReLU net. Our gap theorems hold for smoothly parametrized families of ``hard'' functions, contrary to countable, discrete families known in the literature.   An example consequence of our gap theorems is the following: for every natural number $k$ there exists a function representable by a ReLU DNN with $k^2$ hidden layers and total size $k^3$, such that any ReLU DNN with at most $k$ hidden layers will require at\n[{'summary_text': 'in this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU) we give an algorithm to train a ReLU DNN with one hidden layer to em global optimality with runtime polynomial in the data size albeit exponential in the input dimension .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 63/1992 [03:46<2:08:47,  4.01s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, this places on environment designers the onus of designing language-conditional reward functions which may not be easily or tractably implemented as the complexity of the environment and the language scales. To overcome this limitation, we present a framework within which instruction-conditional RL agents are trained using rewards obtained not from the environment, but from reward models which are jointly trained from expert examples.   As reward models improve, they learn to accurately reward agents for completing tasks for environment configurations---and for instructions---not present amongst the expert data. This framework effectively separates the representation of what instructions require from how they can be executed.\n In a simple grid world, it enables an agent to learn a range of commands requiring interaction with blocks and understanding of spatial relations and underspecified abstract arrangements. We further show the method allows our agent to adapt to changes in the environment without requiring new\n[{'summary_text': 'deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards . however, this places on environment designers the onus of designing language-conditional reward functions which may not be easily or tractably implemented as the complexity of the environment and the language scales . as reward models improve, they learn to accurately reward agents for completing tasks for environment configurations---and for instructions---not present amongst the expert data.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 64/1992 [03:51<2:11:23,  4.09s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy.   Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes large step updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent . achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work . our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 65/1992 [03:54<2:09:24,  4.03s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Implementing correct method invocation is an important task for software developers. However, this is challenging work, since the structure of method invocation can be complicated. In this paper, we propose InvocMap, a code completion tool allows developers to obtain an implementation of multiple method invocations from a list of method names inside code context. InvocMap is able to predict the nested method invocations which their names didn’t appear in the list of input method names given by developers. To achieve this, we analyze the Method Invocations by four levels of abstraction. We build a Machine Translation engine to learn the mapping from the first level to the third level of abstraction of multiple method invocations, which only requires developers to manually add local variables from generated expression to get the final code. We evaluate our proposed approach on six popular libraries: JDK, Android, GWT, Joda-Time, Hibernate, and Xstream. With the training corpus of 2.86 million method inv\n[{'summary_text': 'invocMap is able to predict the nested method invocations which their names didn’t appear in the list of input method names given by developers . to achieve this, we analyze the Method Invocations by four levels of abstraction . we evaluate our proposed approach on six popular libraries: JDK, Android, GWT, Joda-Time, Hibernate, and Xstream.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 66/1992 [03:59<2:11:51,  4.11s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"This work investigates unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality in the input into the objective can significantly improve a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and compares favorably with fully-supervised learning on several classification tasks in with some standard architectures. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation learning objectives for specific end-goals.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': \"this work investigates unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder . incorporating knowledge about locality in the input into the objective can significantly improve a representation's suitability for downstream tasks .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 67/1992 [04:02<2:05:55,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present a simple nearest-neighbor (NN) approach that synthesizes high-frequency photorealistic images from an ``incomplete'' signal such as a low-resolution image, a surface normal map, or edges. Current state-of-the-art deep generative models designed for such conditional image synthesis lack two important things: (1) they are unable to generate a large set of diverse outputs, due to the mode collapse problem. (2) they are not interpretable, making it difficult to control the synthesized output. We demonstrate that NN approaches potentially address such limitations, but suffer in accuracy on small datasets. We design a simple pipeline that combines the best of both worlds:  the first stage uses a convolutional neural network (CNN) to map the input to a (overly-smoothed) image, and the second stage uses a pixel-wise nearest neighbor method to map the smoothed output to multiple high-quality, high-frequency outputs\n[{'summary_text': 'current state-of-the-art deep generative models designed for such conditional image synthesis lack two important things: (1) they are unable to generate a large set of diverse outputs, due to the mode collapse problem . (2) they are not interpretable, making it difficult to control the synthesized output . we design a simple pipeline that combines the best of both worlds .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 68/1992 [04:06<2:08:05,  3.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We argue that symmetry is an important consideration in addressing the problem\n of systematicity and investigate two forms of symmetry relevant to symbolic processes. \n We implement this approach in terms of convolution and show that it can\n be used to achieve effective generalisation in three toy problems: rule learning,\n composition and grammar learning.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we argue that symmetry is an important consideration in addressing the problem of systematicity . we implement this approach in terms of convolution and show that it can be used to achieve effective generalisation in three toy problems .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 69/1992 [04:10<2:01:47,  3.80s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In multi-agent systems, complex interacting behaviors arise due to the high correlations among agents. However, previous work on modeling multi-agent interactions from demonstrations is primarily constrained by assuming the independence among policies and their reward structures. \n In this paper, we cast the multi-agent interactions modeling problem into a multi-agent imitation learning framework with explicit modeling of correlated policies by approximating opponents’ policies, which can recover agents' policies that can regenerate similar interactions. Consequently, we develop a Decentralized Adversarial Imitation Learning algorithm with Correlated policies (CoDAIL), which allows for decentralized training and execution. Various experiments demonstrate that CoDAIL can better regenerate complex interactions close to the demonstrators and outperforms state-of-the-art multi-agent imitation learning methods. Our code is available at \\url{https://github.com/apexrl/CoDAIL}.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'previous work on modeling multi-agent interactions from demonstrations is primarily constrained by assuming the independence among policies and their reward structures . in this paper, we cast the modeling problem into an imitation learning framework with explicit modeling of correlated policies by approximating opponents’ policies . we develop a Decentralized Adversarial Imitation Learning algorithm with Correlated policies (CoDAIL), which allows for decentralized training and execution .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▎         | 70/1992 [04:14<2:03:57,  3.87s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Meta-learning algorithms learn to acquire new tasks more quickly from past experience. In the context of reinforcement learning, meta-learning algorithms can acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta-learning methods generalize best to tasks from the same distribution as the meta-training tasks. In effect, meta-reinforcement learning offloads the design burden from algorithm design to task design. If we can automate the process of task design as well, we can devise a meta-learning algorithm that is truly automated. In this work, we take a step in this direction, proposing a family of unsupervised meta-learning algorithms for reinforcement learning. We motivate and describe a general recipe for unsupervised meta-reinforcement learning, and present an instantiation\n[{'summary_text': 'meta-learning algorithms can acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks . supervised learning generalizes best to test points drawn from the same distribution as the training points . in effect, meta-reinforcement learning offloads the design burden from algorithm design to task design .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▎         | 71/1992 [04:18<2:05:15,  3.91s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Our work offers a new method for domain translation from semantic label maps\n and Computer Graphic (CG) simulation edge map images to photo-realistic im-\n ages. We train a Generative Adversarial Network (GAN) in a conditional way to\n generate a photo-realistic version of a given CG scene. Existing architectures of\n GANs still lack the photo-realism capabilities needed to train DNNs for computer\n vision tasks, we address this issue by embedding edge maps, and training it in an\n adversarial mode. We also offer an extension to our model that uses our GAN\n architecture to create visually appealing and temporally coherent videos.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'our work offers a new method for domain translation from semantic label maps and Computer Graphic (CG) simulation edge map images to photo-realistic im- ages . existing architectures of GANs still lack the photo-realism capabilities needed to train DNNs for computer vision tasks .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▎         | 72/1992 [04:21<2:01:48,  3.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Capsule Networks have shown encouraging results on defacto benchmark computer vision datasets such as MNIST, CIFAR and smallNORB. Although, they are yet to be tested on tasks where (1) the entities detected inherently have more complex internal representations and (2) there are very few instances per class to learn from and (3) where point-wise classification is not suitable. Hence, this paper carries out experiments on face verification in both controlled and uncontrolled settings that together address these points. In doing so we introduce Siamese Capsule Networks, a new variant that can be used for pairwise learning tasks. The model is trained using contrastive loss with l2-normalized capsule encoded pose features. We find that Siamese Capsule Networks perform well against strong baselines on both pairwise learning datasets, yielding best results in the few-shot learning setting where image pairs in the test set contain unseen subjects.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Capsule Networks have shown encouraging results on defacto benchmark computer vision datasets such as MNIST, CIFAR and smallNORB . although they are yet to be tested on tasks where (1) the entities detected inherently have more complex internal representations and (2) there are very few instances per class to learn from .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▎         | 73/1992 [04:25<2:01:00,  3.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We apply canonical forms of gradient complexes (barcodes) to explore neural networks loss surfaces. We present an algorithm for calculations of the objective function's barcodes of minima.   Our experiments confirm two principal observations: (1) the barcodes of minima are located in a small lower part of the range of values of objective function and (2) increase of the neural network's depth brings down the minima's barcodes. This has natural implications for the neural network learning and the ability to generalize.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the barcodes of minima are located in a small lower part of the range of values of objective function . this has natural implications for the neural network learning and the ability to generalize .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▎         | 74/1992 [04:29<1:58:44,  3.71s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Learning an efficient update rule from data that promotes rapid learning of new tasks from the same distribution remains an open problem in meta-learning. Typically, previous works have approached this issue either by attempting to train a neural network that directly produces updates or by attempting to learn better initialisations or scaling factors for a gradient-based update rule. Both these approaches pose challenges. On one hand, directly producing an update forgoes a useful inductive bias and can easily lead to non-converging behaviour. On the other hand, approaches that try to control a gradient-based update rule typically resort to computing gradients through the learning process to obtain their meta-gradients, leading to methods that can not scale beyond few-shot task adaptation. In this work we propose Warped Gradient Descent (WarpGrad), a method that intersects these approaches to mitigate their limitations. WarpGrad meta-learns an efficiently parameterised preconditioning matrix that facilitates gradient descent across the task distribution.\n[{'summary_text': 'previous works have approached this issue either by attempting to train a neural network that directly produces updates . directly producing an update forgoes a useful inductive bias and can easily lead to non-converging behaviour . in this work we propose Warped Gradient Descent, a method that intersects these approaches to mitigate their limitations .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 75/1992 [04:33<2:02:31,  3.83s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Search space is a key consideration for neural architecture search. Recently, Xie et al. (2019a) found that randomly generated networks from the same distribution perform similarly, which suggest we should search for random graph distributions instead of graphs. We propose graphon as a new search space. A graphon is the limit of Cauchy sequence of graphs and a scale-free probabilistic distribution, from which graphs of different number of vertices can be drawn. This property enables us to perform NAS using fast, low-capacity models and scale the found models up when necessary. We develop an algorithm for NAS in the space of graphons and empirically demonstrate that it can find stage-wise graphs that outperform DenseNet and other baselines on ImageNet.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Xie et al. (2019a) found that randomly generated networks from the same distribution perform similarly . this suggests we should search for random graph distributions instead of graphs . graphon is the limit of Cauchy sequences and a scale-free probabilistic distribution, from which graphs of different number of vertices can be drawn .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 76/1992 [04:36<2:01:47,  3.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Positive-unlabeled (PU) learning addresses the problem of learning a binary classifier from positive (P) and unlabeled (U) data. It is often applied to situations where negative (N) data are difficult to be fully labeled. However, collecting a non-representative N set that contains only a small portion of all possible N data can be much easier in many practical situations. This paper studies a novel classification framework which incorporates such biased N (bN) data in PU learning. The fact that the training N data are biased also makes our work very different from those of standard semi-supervised learning. We provide an empirical risk minimization-based method to address this PUbN classification problem. Our approach can be regarded as a variant of traditional example-reweighting algorithms, with the weight of each example computed through a preliminary step that draws inspiration from PU learning. We also derive an estimation error bound for the proposed method. Experimental results demonstrate the effectiveness of\n[{'summary_text': 'positive-unlabeled (PU) learning addresses the problem of learning a binary classifier from positive (P) and unlabed (U) data . it is often applied to situations where negative (N) data are difficult to be fully labeled . however, collecting a non-representative N set that contains only a small portion of all possible N data can be much easier in many practical situations .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 77/1992 [04:41<2:06:25,  3.96s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"LSTM-based language models exhibit compositionality in their representations, but how this behavior emerges over the course of training has not been explored. Analyzing synthetic data experiments with contextual decomposition, we find that LSTMs learn long-range dependencies compositionally by building them from shorter constituents during training.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'LSTM-based language models exhibit compositionality in their representations . how this behavior emerges over the course of training has not been explored . analysis of synthetic data experiments with contextual decomposition .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 78/1992 [04:44<2:02:46,  3.85s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We outline new approaches to incorporate ideas from deep learning into wave-based least-squares imaging. The aim, and main contribution of this work, is the combination of handcrafted constraints with deep convolutional neural networks, as a way to harness their remarkable ease of generating natural images. The mathematical basis underlying our method is the expectation-maximization framework, where data are divided in batches and coupled to additional \"latent\" unknowns. These unknowns are pairs of elements from the original unknown space (but now coupled to a specific data batch) and network inputs. In this setting, the neural network controls the similarity between these additional parameters, acting as a \"center\" variable. The resulting problem amounts to a maximum-likelihood estimation of the network parameters when the augmented data model is marginalized over the latent variables.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we outline new approaches to incorporate ideas from deep learning into wave-based least-squares imaging . the aim, and main contribution of this work, is the combination of handcrafted constraints with deep convolutional neural networks, as a way to harness their remarkable ease of generating natural images .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 79/1992 [04:48<2:00:13,  3.77s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"While deep neural networks are a highly successful model class, their large memory footprint puts considerable strain on energy consumption, communication bandwidth, and storage requirements. Consequently, model size reduction has become an utmost goal in deep learning. A typical approach is to train a set of deterministic weights, while applying certain techniques such as pruning and quantization, in order that the empirical weight distribution becomes amenable to Shannon-style coding schemes. However, as shown in this paper, relaxing weight determinism and using a full variational distribution over weights allows for more efficient coding schemes and consequently higher compression rates. In particular, following the classical bits-back argument, we encode the network weights using a random sample, requiring only a number of bits corresponding to the Kullback-Leibler divergence between the sampled variational distribution and the encoding distribution. By imposing a constraint on the Kullback-Leibler divergence, we are able to explicitly control the compression rate, while optimizing the expected loss on the training\n[{'summary_text': 'deep neural networks are a highly successful model class, but their large memory footprint puts considerable strain on energy consumption, communication bandwidth, and storage requirements. Consequently, model size reduction has become an utmost goal in deep learning . a typical approach is to train a set of deterministic weights, while applying certain techniques such as pruning and quantization, in order that the empirical weight distribution becomes amenable to Shannon-style coding schemes .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 80/1992 [04:52<2:05:42,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pre-trained ResNet. An attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations. In particular, the proposed method enables the visual information to be integrated into large-scale text-only NMT in addition to the multimodel NMT. Experiments on four widely used translation datasets,\n[{'summary_text': 'a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs . an attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 81/1992 [04:56<2:06:20,  3.97s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Board games often rely on visual information such as the location of the game pieces and textual information on cards. Due to this reliance on visual feedback, blind players are at a disadvantage because they cannot read the cards or see the location of the game pieces and may be unable to play a game without sighted help. We present Game Changer, an augmented workspace that provides both audio descriptions and tactile additions to make the state of the board game accessible to blind and visually impaired players. In this paper, we describe the design of Game Changer and present findings from a user study in which 7 blind participants used Game Changer to play against a sighted partner. Most players stated the game was more accessible with the additions from Game Changer and felt that Game Changer could be used to augment other games.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'blind players are at a disadvantage because they cannot read the cards or see the location of the game pieces and may be unable to play a game without sighted help . we present Game Changer, an augmented workspace that provides both audio descriptions and tactile additions to make the state of the board game accessible to blind and visually impaired players .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 82/1992 [05:00<2:06:44,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"This work presents the Poincaré Wasserstein Autoencoder, a reformulation of\n the recently proposed Wasserstein autoencoder framework on a non-Euclidean\n manifold, the Poincaré ball model of the hyperbolic space H n . By assuming the\n latent space to be hyperbolic, we can use its intrinsic hierarchy to impose structure\n on the learned latent space representations. We show that for datasets with latent\n hierarchies, we can recover the structure in a low-dimensional latent space. We\n also demonstrate the model in the visual domain to analyze some of its properties\n and show competitive results on a graph link prediction task.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'this work presents the Poincaré Wasserstein Autoencoder, a reformulation of the recently proposed Wasserstein framework on a non-euclidean manifold . we show that for datasets with latent hierarchies, we can recover the structure in a low-dimensional latent space . also demonstrate the model in the visual domain to analyze some of its properties .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 83/1992 [05:04<2:05:42,  3.95s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Data parallelism has become a dominant method to scale Deep Neural Network (DNN) training across multiple nodes.   Since the synchronization of the local models or gradients can be a bottleneck for large-scale distributed training, compressing communication traffic has gained widespread attention recently.   Among several recent proposed compression algorithms, \nResidual Gradient Compression (RGC) is one of the most successful approaches---it can significantly compress the transmitting message size (0.1% of the gradient size) of each node and still preserve accuracy. However, the literature on compressing deep networks focuses almost exclusively on achieving good compression rate, while the efficiency of RGC in real implementation has been less investigated. In this paper, we develop an RGC method that achieves significant training time improvement in real-world multi-GPU systems. Our proposed RGC system design called RedSync, introduces a set of optimizations to reduce communication bandwidth while introducing limited overhead. We examine the performance of RedSync on\n[{'summary_text': 'synchronization of the local models or gradients can be a bottleneck for large-scale distributed training, compressing communication traffic has gained widespread attention recently . a proposed RGC system design called RedSync introduces a set of optimizations to reduce communication bandwidth while introducing limited overhead .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 84/1992 [05:08<2:05:18,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Understanding object motion is one of the core problems in computer vision. It requires segmenting and tracking objects over time. Significant progress has been made in instance segmentation, but such models cannot track objects, and more crucially, they are unable to reason in both 3D space and time.\n We propose a new spatio-temporal embedding loss on videos that generates temporally consistent video instance segmentation. Our model includes a temporal network that learns to model temporal context and motion, which is essential to produce smooth embeddings over time. Further, our model also estimates monocular depth, with a self-supervised loss, as the relative distance to an object effectively constrains where it can be next, ensuring a time-consistent embedding. Finally, we show that our model can accurately track and segment instances, even with occlusions and missed detections, advancing the state-of-the-art on the KITTI Multi-Object and Tracking Dataset\n[{'summary_text': 'significant progress has been made in instance segmentation . but such models cannot track objects, and more crucially, they are unable to reason in both 3D space and time . we propose a new spatio-temporal embedding loss on videos .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 85/1992 [05:12<2:05:19,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generative neural networks map a standard, possibly distribution to a complex high-dimensional distribution, which represents the real world data set. However, a determinate input distribution as well as a specific architecture of neural networks may impose limitations on capturing the diversity in the high dimensional target space. To resolve this difficulty, we propose a training framework that greedily produce a series of generative adversarial networks that incrementally capture the diversity of the target space. We show theoretically and empirically that our training algorithm converges to the theoretically optimal distribution, the projection of the real distribution onto the convex hull of the network's distribution space.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'a determinate input distribution as well as a specific architecture of neural networks may impose limitations on capturing the diversity in the high dimensional target space . to resolve this difficulty, we propose a training framework that greedily produce a series of generative adversarial networks that incrementally capture the diversity of the target space.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 86/1992 [05:16<2:02:59,  3.87s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': \"our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications . then we propose a new dataset called imageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations . Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations.\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 87/1992 [05:20<2:04:22,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep neural networks trained on large supervised datasets have led to impressive results in recent years. However, since well-annotated datasets can be prohibitively expensive and time-consuming to collect, recent work has explored the use of larger but noisy datasets that can be more easily obtained. In this paper, we investigate the behavior of deep neural networks on training sets with massively noisy labels. We show on multiple datasets such as MINST, CIFAR-10 and ImageNet that successful learning is possible even with an essentially arbitrary amount of noise. For example, on MNIST we find that accuracy of above 90 percent is still attainable even when the dataset has been diluted with 100 noisy examples for each clean example. Such behavior holds across multiple patterns of label noise, even when noisy labels are biased towards confusing classes. Further, we show how the required dataset size for successful training increases with higher label noise. Finally, we present simple actionable techniques for improving learning in the regime of high label noise.<|endoftext|>\n[{'summary_text': 'deep neural networks trained on large supervised datasets have led to impressive results in recent years . but since well-annotated datasets can be prohibitively expensive and time-consuming to collect, recent work has explored the use of larger but noisy datasets that can be more easily obtained .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 88/1992 [05:23<2:01:21,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Self-training is one of the earliest and simplest semi-supervised methods. The key idea is to augment the original labeled dataset with unlabeled data paired with the model’s prediction. Self-training has mostly been well-studied to classification problems. However, in complex sequence generation tasks such as machine translation, it is still not clear how self-training woks due to the compositionality of the target space. In this work, we first show that it is not only possible but recommended to apply self-training in sequence generation. Through careful examination of the performance gains, we find that the noise added on the hidden states (e.g. dropout) is critical to the success of self-training, as this acts like a regularizer which forces the model to yield similar predictions for similar inputs from unlabeled data. To further encourage this mechanism, we propose to inject noise to the input space, resulting in a “noisy” version of self-\n[{'summary_text': 'self-training is one of the earliest and simplest semi-supervised methods . the key idea is to augment the original labeled dataset with unlabeled data paired with the model’s prediction . however, in complex sequence generation tasks such as machine translation, it is still not clear how self -training woks due to the compositionality of the target space . to further encourage this mechanism, we propose to inject noise to the input space, resulting in a “noisy” version of self- ('}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 89/1992 [05:28<2:08:28,  4.05s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Exploration while learning representations is one of the main challenges Deep\n Reinforcement Learning (DRL) faces today. As the learned representation is dependant in the observed data, the exploration strategy has a crucial role. The popular DQN algorithm has improved significantly the capabilities of Reinforcement\n Learning (RL) algorithms to learn state representations from raw data, yet, it uses\n a naive exploration strategy which is statistically inefficient. The Randomized\n Least Squares Value Iteration (RLSVI) algorithm (Osband et al., 2016), on the\n other hand, explores and generalizes efficiently via linearly parameterized value\n functions. However, it is based on hand-designed state representation that requires\n prior engineering work for every environment. In this paper, we propose a Deep\n Learning adaptation for RLSVI. Rather than using hand-design state representation, we use a state representation that is being learned directly from the data by a\n DQN agent. As the representation\n[{'summary_text': 'the popular DQN algorithm has improved significantly the capabilities of Reinforcement Learning (RL) algorithms to learn state representations from raw data . however, it is based on hand-designed state representation that requires prior engineering work for every environment . in this paper, we propose a Deep Learning adaptation for RLSVI .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▍         | 90/1992 [05:32<2:08:06,  4.04s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The complexity of large-scale neural networks can lead to poor understanding of their internal  details. We show that this opaqueness provides an opportunity for adversaries to embed unintended functionalities into the network in the form of Trojan horse attacks. Our novel framework hides the existence of a malicious network within a benign transport network. Our attack is flexible, easy to execute, and difficult to detect. We prove theoretically that the malicious network's detection is computationally infeasible and demonstrate empirically that the transport network does not compromise its disguise. Our attack exposes an important, previously unknown loophole that unveils a new direction in machine learning security.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': \"complex neural networks can lead to poor understanding of their internal details . our attack is flexible, easy to execute, and difficult to detect . we prove theoretically that the malicious network's detection is computationally infeasible .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▍         | 91/1992 [05:35<2:02:20,  3.86s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present a new approach to assessing the robustness of neural networks based on estimating the proportion of inputs for which a property is violated. Specifically, we estimate the probability of the event that the property is violated under an input model. Our approach critically varies from the formal verification framework in that when the property can be violated, it provides an informative notion of how robust the network is, rather than just the conventional assertion that the network is not verifiable. Furthermore, it provides an ability to scale to larger networks than formal verification approaches. Though the framework still provides a formal guarantee of satisfiability whenever it successfully finds one or more violations, these advantages do come at the cost of only providing a statistical estimate of unsatisfiability whenever no violation is found. Key to the practical success of our approach is an adaptation of multi-level splitting, a Monte Carlo approach for estimating the probability of rare events, to our statistical robustness framework. We demonstrate that our approach is able to emulate formal verification procedures on benchmark problems,\n[{'summary_text': 'we present a new approach to assessing the robustness of neural networks . based on estimating the proportion of inputs for which a property is violated . our approach critically varies from the formal verification framework in that when the property can be violated, it provides an informative notion of how robust the network is .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▍         | 92/1992 [05:39<2:03:42,  3.91s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Existing public face image datasets are strongly biased toward Caucasian faces, and other races (e.g., Latino) are significantly underrepresented. The models trained from such datasets suffer from inconsistent classification accuracy, which limits the applicability of face analytic systems to non-White race groups. To mitigate the race bias problem in these datasets, we constructed a novel face image dataset containing 108,501 images which is balanced on race. We define 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups. Evaluations were performed on existing face attribute datasets as well as novel image datasets to measure the generalization performance. We find that the model trained from our dataset is substantially more accurate on novel datasets and the accuracy is consistent across race and gender groups. We also compare several commercial computer vision APIs and report their balanced accuracy across gender, race, and\n[{'summary_text': 'models trained from such datasets suffer from inconsistent classification accuracy, which limits the applicability of face analytic systems to non-White race groups . images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups. evaluations were performed on existing face attribute datasets as well as novel image datasets to measure the generalization performance.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▍         | 93/1992 [05:44<2:06:52,  4.01s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Standard image captioning tasks such as COCO and Flickr30k are factual, neutral in tone and (to a human) state the obvious (e.g., “a man playing a guitar”). While such tasks are useful to verify that a machine understands the content of an image,  they are not engaging to humans as captions.    With this in mind we define a new task, Personality-Captions, where the goal is to be as engaging to humans as possible by incorporating controllable style and personality traits. We collect and release a large dataset of 201,858 of such captions conditioned over 215 possible traits.   We build models that combine existing work from (i) sentence representations (Mazaré et al., 2018) with Transformers trained on 1.7 billion dialogue examples; and (ii) image representations (Mahajan et al., 2018) with ResNets trained on 3.5 billion social media images.  \n[{'summary_text': 'standard image captioning tasks such as COCO and Flickr30k are factual, neutral in tone and (to a human) state the obvious (e.g., “a man playing a guitar”). with this in mind we define a new task, Personality-Captions, where the goal is to be as engaging to humans as possible by incorporating controllable style and personality traits .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▍         | 94/1992 [05:48<2:08:20,  4.06s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We introduce two approaches for conducting efficient Bayesian inference in stochastic simulators containing nested stochastic sub-procedures, i.e., internal procedures for which the density cannot be calculated directly such as rejection sampling loops. The resulting class of simulators are used extensively throughout the sciences and can be interpreted as probabilistic generative models. However, drawing inferences from them poses a substantial challenge due to the inability to evaluate even their unnormalised density, preventing the use of many standard inference procedures like Markov Chain Monte Carlo (MCMC). To address this, we introduce inference algorithms based on a two-step approach that first approximates the conditional densities of the individual sub-procedures, before using these approximations to run MCMC methods on the full program. Because the sub-procedures can be dealt with separately and are lower-dimensional than that of the overall problem, this two-step process allows them to be isolated and thus be tractably\n[{'summary_text': 'the resulting class of simulators are used extensively throughout the sciences . they can be interpreted as probabilistic generative models . drawing inferences from them poses a substantial challenge due to the inability to evaluate even their unnormalised density .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▍         | 95/1992 [05:52<2:06:56,  4.01s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Batch normalization (batch norm) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and noise by double-digit percentages, as we show on five standard datasets. Furthermore, substituting weight decay for batch norm is sufficient to nullify the relationship between adversarial vulnerability and the input dimension. Our work is consistent with a mean-field analysis that found that batch norm causes exploding gradients.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'batch normalization (batch norm) is often used in an attempt to stabilize and accelerate training in deep neural networks . in many cases it decreases the number of parameter updates required to achieve low training error . however, it also reduces robustness to small adversarial input perturbations and noise by double-digit percentages .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▍         | 96/1992 [05:55<2:03:21,  3.90s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Meta learning has been making impressive progress for fast model adaptation. However, limited work has been done on learning fast uncertainty adaption for Bayesian modeling. In this paper, we propose to achieve the goal by placing meta learning on the space of probability measures, inducing the concept of meta sampling for fast uncertainty adaption. Specifically, we propose a Bayesian meta sampling framework consisting of two main components: a meta sampler and a sample adapter. The meta sampler is constructed by adopting a neural-inverse-autoregressive-flow (NIAF) structure, a variant of the recently proposed neural autoregressive flows, to efficiently generate meta samples to be adapted. The sample adapter moves meta samples to task-specific samples, based on a newly proposed and general Bayesian sampling technique, called optimal-transport Bayesian sampling. The combination of the two components allows a simple learning procedure for the\n meta sampler to be developed, which can be efficiently optimized via standard back-\n[{'summary_text': 'in this paper, we propose to achieve the goal by placing meta learning on the space of probability measures . the meta sampler is constructed by adopting a neural-inverse-autoregressive-flow (NIAF) structure, a variant of the recently proposed neural autoregressive flows, to efficiently generate meta samples to be adapted .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▍         | 97/1992 [06:00<2:05:27,  3.97s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We study the Cross-Entropy Method (CEM) for the non-convex optimization of a continuous and parameterized objective function and introduce a differentiable variant (DCEM) that enables us to differentiate the output of CEM with respect to the objective function's parameters. In the machine learning setting this brings CEM inside of the end-to-end learning pipeline in cases this has otherwise been impossible. We show applications in a synthetic energy-based structured prediction task and in non-convex continuous control. In the control setting we show on the simulated cheetah and walker tasks that we can embed their optimal action sequences with DCEM and then use policy optimization to fine-tune components of the controller as a step towards combining model-based and model-free RL.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we show applications in a synthetic energy-based structured prediction task . in the control setting this brings CEM inside of the end-to-end learning pipeline in cases this has otherwise been impossible .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▍         | 98/1992 [06:03<2:00:22,  3.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"One of the main challenges in applying graph convolutional neural networks on gene-interaction data is the lack of understanding of the vector space  to which they belong and also the inherent difficulties involved in representing those interactions on a significantly lower dimension, viz Euclidean spaces. The challenge becomes more prevalent when dealing with various types of heterogeneous data. We introduce a systematic, generalized method, called iSOM-GSN, used to transform ``multi-omic'' data with higher dimensions onto a two-dimensional grid. Afterwards, we apply a convolutional neural network to predict disease states of various types. Based on the idea of Kohonen's self-organizing map, we generate a two-dimensional grid for each sample for a given set of genes that represent a gene similarity network.   We have tested the model to predict breast and prostate cancer using gene expression, DNA methylation and copy number alteration, yielding prediction accuracies in the 94-98% range for tumor stages of\n[{'summary_text': \"the challenge becomes more prevalent when dealing with heterogeneous data . we introduce a systematic, generalized method, called iSOM-GSN, used to transform multi-omic'' data with higher dimensions onto a two-dimensional grid . subsequently, we apply a convolutional neural network to predict disease states of various types .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▍         | 99/1992 [06:07<2:02:54,  3.90s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Keyword spotting—or wakeword detection—is an essential feature for hands-free operation of modern voice-controlled devices. With such devices becoming ubiquitous, users might want to choose a personalized custom wakeword. In this work, we present DONUT, a CTC-based algorithm for online query-by-example keyword spotting that enables custom wakeword detection. The algorithm works by recording a small number of training examples from the user, generating a set of label sequence hypotheses from these training examples, and detecting the wakeword by aggregating the scores of all the hypotheses given a new audio recording. Our method combines the generalization and interpretability of CTC-based keyword spotting with the user-adaptation and convenience of a conventional query-by-example system. DONUT has low computational requirements and is well-suited for both learning and inference on embedded systems without requiring private user data to be uploaded to the cloud.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'keyword spotting is an essential feature for hands-free operation of modern voice-controlled devices . the algorithm works by recording a small number of training examples from the user, generating a set of label sequence hypotheses from these training examples, and detecting the wakeword .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▌         | 100/1992 [06:11<1:59:54,  3.80s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Human reasoning involves recognising common underlying principles across many examples by utilising variables. The by-products of such reasoning are invariants that capture patterns across examples such as \"if someone went somewhere then they are there\" without mentioning specific people or places. Humans learn what variables are and how to use them at a young age, and the question this paper addresses is whether machines can also learn and use variables solely from examples without requiring human pre-engineering. We propose Unification Networks that incorporate soft unification into neural networks to learn variables and by doing so lift examples into invariants that can then be used to solve a given task. We evaluate our approach on four datasets to demonstrate that learning invariants captures patterns in the data and can improve performance over baselines.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'by-products of such reasoning are invariants that capture patterns across examples . humans learn what variables are and how to use them at a young age . we propose Unification Networks that incorporate soft unification into neural networks .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▌         | 101/1992 [06:14<1:56:22,  3.69s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Learning rate decay (lrDecay) is a \\emph{de facto} technique for training modern neural networks. It starts with a large learning rate and then decays it multiple times. It is empirically observed to help both optimization and generalization. Common beliefs in how lrDecay works come from the optimization analysis of (Stochastic) Gradient Descent: 1) an initially large learning rate accelerates training or helps the network escape spurious local minima; 2) decaying the learning rate helps the network converge to a local minimum and avoid oscillation. Despite the popularity of these common beliefs, experiments suggest that they are insufficient in explaining the general effectiveness of lrDecay in training modern neural networks that are deep, wide, and nonconvex. We provide another novel explanation: an initially large learning rate suppresses the network from memorizing noisy data while decaying the learning rate improves the learning of complex patterns. The proposed explanation is validated on a carefully-constructed\n[{'summary_text': 'learning rate decay (lrDecay) is a emphde facto technique for training modern neural networks . it starts with a large learning rate and then decays it multiple times . common beliefs come from the optimization analysis of (Stochastic) Gradient Descent .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▌         | 102/1992 [06:18<2:02:22,  3.88s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recent studies show that widely used Deep neural networks (DNNs) are vulnerable to the carefully crafted adversarial examples.\n Many advanced algorithms have been proposed to generate adversarial examples by leveraging the L_p distance for penalizing perturbations.\n Different defense methods have also been explored to defend against such adversarial attacks. \n While the effectiveness of L_p distance as a metric of perceptual quality remains an active research area, in this paper we will instead focus on a different type of perturbation, namely spatial transformation, as opposed to manipulating the pixel values directly as in prior works.\n Perturbations generated through spatial transformation could result in large L_p distance measures, but our extensive experiments show that such spatially transformed adversarial examples are perceptually realistic and more difficult to defend against with existing defense systems. This potentially provides a new direction in adversarial example generation and the design of corresponding defenses.\n We visualize the spatial transformation based perturbation for different examples and\n[{'summary_text': 'many advanced algorithms have been proposed to generate adversarial examples by leveraging the L_p distance for penalizing perturbations . different defense methods have also been explored to defend against such adversarials . a different type of perturbation could result in large pixel values as in prior works .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▌         | 103/1992 [06:22<2:02:33,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Among multiple ways of interpreting a machine learning model, measuring the importance of a set of features tied to a prediction is probably one of the most intuitive way to explain a model. In this paper, we establish the link between a set of features to a prediction with a new evaluation criteria, robustness analysis, which measures the minimum tolerance of adversarial perturbation. By measuring the tolerance level for an adversarial attack, we can extract a set of features that provides most robust support for a current prediction, and also can extract a set of features that contrasts the current prediction to a target class by setting a targeted adversarial attack. By applying this methodology to various prediction tasks across multiple domains, we observed the derived explanations are indeed capturing the significant feature set qualitatively and quantitatively.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'by measuring the tolerance level for an adversarial attack, we can extract a set of features that contrasts the current prediction to a target class . by applying this methodology to various prediction tasks across multiple domains, we observed the derived explanations are indeed capturing the significant feature set qualitatively and quantitatively .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▌         | 104/1992 [06:26<2:00:30,  3.83s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The practical usage of reinforcement learning agents is often bottlenecked by the duration of training time. To accelerate training, practitioners often turn to distributed reinforcement learning architectures to parallelize and accelerate the training process. However, modern methods for scalable reinforcement learning (RL) often tradeoff between the throughput of samples that an RL agent can learn from (sample throughput) and the quality of learning from each sample (sample efficiency). In these scalable RL architectures, as one increases sample throughput (i.e. increasing parallelization in IMPALA (Espeholt et al., 2018)), sample efficiency drops significantly. To address this, we propose a new distributed reinforcement learning algorithm, IMPACT. IMPACT extends PPO with three changes: a target network for stabilizing the surrogate objective, a circular buffer, and truncated importance sampling. In discrete action-space environments, we show that IMPACT attains higher reward and, simultaneously, achieves up to 30% decrease in training wall-time than that of IMPAL\n[{'summary_text': 'practitioners often turn to distributed reinforcement learning architectures to parallelize and accelerate training process . modern methods for scalable reinforcement learning (RL) often tradeoff between throughput of samples that an RL agent can learn from (sample throughput) and the quality of learning from each sample . in discrete action-space environments, we show that IMPACT attains higher reward and achieves up to 30% decrease in training wall-time than that of IMPAL .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▌         | 105/1992 [06:30<2:05:19,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Emoji suggestion systems based on typed text have been proposed to encourage emoji usage and enrich text messaging; however, such systems’ actual effects on the chat experience remain unknown. We built an Android keyboard with both lexical (word-based) and semantic (meaning-based) emoji suggestion capabilities and compared these in two different studies. To investigate the effect of emoji suggestion in online conversations, we conducted a laboratory text-messaging study with 24 participants, and also a 15-day longitudinal field deployment with 18 participants. We found that lexical emoji suggestions increased emoji usage by 31.5% over a keyboard without suggestions, while semantic suggestions increased emoji usage by 125.1%. However, suggestion mechanisms did not affect the chatting experience significantly. From these studies, we formulate a set of design guidelines for future emoji suggestion systems that better support users’ needs.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we built an Android keyboard with both lexical (word-based) and semantic emoji suggestion capabilities and compared these in two different studies . to investigate the effect of emeoji suggestion in online conversations, we conducted a laboratory text-messaging study with 24 participants . we found that lexicals increased emmoji usage by 31.5% over a keyboard without suggestions, while semantic suggestions increased a 125.1% . however, suggestion mechanisms did not affect the chatting experience significantly .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▌         | 106/1992 [06:35<2:08:11,  4.08s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recent advances in Neural Variational Inference allowed for a renaissance in latent variable models in a variety of domains involving high-dimensional data. In this paper, we introduce two generic Variational Inference frameworks for generative models of Knowledge Graphs; Latent Fact Model and Latent Information Model.   While traditional variational methods derive an analytical approximation for the intractable distribution over the latent variables, here we construct an inference network conditioned on the symbolic representation of entities and relation types in the Knowledge Graph, to provide the variational distributions. The new framework can create models able to discover underlying probabilistic semantics for the symbolic representation by utilising parameterisable distributions which permit training by back-propagation in the context of neural variational inference, resulting in a highly-scalable method. Under a Bernoulli sampling framework, we provide an alternative justification for commonly used techniques in large-scale stochastic variational inference, which drastically reduces training time at a cost of an additional\n[{'summary_text': 'in this paper, we introduce two generic Variational Inference frameworks for generative models of Knowledge Graphs . the new framework can create models able to discover underlying probabilistic semantics for the symbolic representation by using parameterisable distributions which permit training by back-propagation in the context of neural variational inference .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▌         | 107/1992 [06:39<2:07:55,  4.07s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Conversational question answering (CQA) is a novel QA task that requires the understanding of dialogue context. Different from traditional single-turn machine reading comprehension (MRC), CQA is a comprehensive task comprised of passage reading, coreference resolution, and contextual understanding. In this paper, we propose an innovative contextualized attention-based deep neural network, SDNet, to fuse context into traditional MRC models. Our model leverages both inter-attention and self-attention to comprehend the conversation and passage. Furthermore, we demonstrate a novel method to integrate the BERT contextual model as a sub-module in our network. Empirical results show the effectiveness of SDNet. On the CoQA leaderboard, it outperforms the previous best model's F1 score by 1.6%. Our ensemble model further improves the F1 score by 2.7%.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': \"Conversational question answering (CQA) is a novel QA task that requires the understanding of dialogue context . on the CoQA leaderboard, it outperforms the previous best model's F1 score by 1.6% .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▌         | 108/1992 [06:42<2:03:29,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"This work presents a scalable solution to continuous visual speech recognition. To achieve this, we constructed the largest existing visual speech recognition dataset, consisting of pairs of text and video clips of faces speaking (3,886 hours of video). In tandem, we designed and trained an integrated lipreading system, consisting of a video processing pipeline that maps raw video to stable videos of lips and sequences of phonemes, a scalable deep neural network that maps the lip videos to sequences of phoneme distributions, and a production-level speech decoder that outputs sequences of words. The proposed system achieves a word error rate (WER) of 40.9% as measured on a held-out set. In comparison, professional lipreaders achieve either 86.4% or 92.9% WER on the same dataset when having access to additional types of contextual information. Our approach significantly improves on previous lipreading approaches, including variants of LipNet and of Watch, Attend, and Spell (WAS), which are only\n[{'summary_text': 'proposed system achieves a word error rate (WER) of 40.9% as measured on a held-out set . professional lipreaders achieve either 86.4% or 92.9% WER on the same dataset when having access to additional types of contextual information .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▌         | 109/1992 [06:46<2:02:59,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper we study generative modeling via autoencoders while using the elegant geometric properties of the optimal transport (OT) problem and the Wasserstein distances. We introduce Sliced-Wasserstein Autoencoders (SWAE), which are generative models that enable one to shape the distribution of the latent space into any samplable probability distribution without the need for training an adversarial network or defining a closed-form for the distribution. In short, we regularize the autoencoder loss with the sliced-Wasserstein distance between the distribution of the encoded training samples and a predefined samplable distribution. We show that the proposed formulation has an efficient numerical solution that provides similar capabilities to Wasserstein Autoencoders (WAE) and Variational Autoencoders (VAE), while benefiting from an embarrassingly simple implementation.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we introduce Sliced-Wasserstein Autoencoders (SWAE) which are generative models that enable one to shape the distribution of the latent space into any samplable probability distribution without the need for training an adversarial network or defining a closed-form for the distribution .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 110/1992 [06:50<2:02:59,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The purpose of an encoding model is to predict brain activity given a stimulus. In this contribution, we attempt at estimating a whole brain encoding model of auditory perception in a naturalistic stimulation setting. We analyze data from an open dataset, in which 16 subjects watched a short movie while their brain activity was being measured using functional MRI. We extracted feature vectors aligned with the timing of the audio from the movie, at different layers of a Deep Neural Network pretrained on the classification of auditory scenes. fMRI data was parcellated using hierarchical clustering on 500 parcels, and encoding models were estimated using a fully connected neural network with one hidden layer, trained to predict the signals for each parcel from the DNN features. Individual encoding models were successfully trained and predicted brain activity on unseen data, in parcels located in the superior temporal lobe, as well as dorsolateral prefrontal regions, which are usually considered as areas involved in auditory and language processing. Taken together, this contribution extends previous attempts on estimating encoding models,\n[{'summary_text': 'the purpose of an encoding model is to predict brain activity given a stimulus . we analyze data from an open dataset, in which 16 subjects watched a short movie while their brain activity was being measured using functional MRI . fMRI data was parcellated using hierarchical clustering on 500 parcels .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 111/1992 [06:54<2:04:39,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features . more specifically, we explore three aspects of CNN design: depth of the network, use of residual blocks along with the use of grouped convolution, and global aggregation over time .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 112/1992 [06:58<2:02:47,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Data augmentation is one of the most effective approaches for improving the accuracy of modern machine learning models, and it is also indispensable to train a deep model for meta-learning. However, most current data augmentation implementations applied in meta-learning are the same as those used in the conventional image classification. In this paper, we introduce a new data augmentation method for meta-learning, which is named as ``Task Level Data Augmentation'' (referred to Task Aug). The basic idea of Task Aug is to increase the number of image classes rather than the number of images in each class. In contrast, with a larger amount of classes, we can sample more diverse task instances during training. This allows us to train a deep network by meta-learning methods with little over-fitting. Experimental results show that our approach achieves state-of-the-art performance on miniImageNet, CIFAR-FS, and FC100 few-shot learning benchmarks. Once paper is accepted, we will\n[{'summary_text': 'most current data augmentation implementations applied in meta-learning are the same as those used in the conventional image classification . the basic idea of Task Aug is to increase the number of image classes rather than the numbers of images in each class . a larger amount of classes, we can sample more diverse task instances during training . this allows us to train a deep network .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 113/1992 [07:02<2:04:32,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Can the success of reinforcement learning methods for simple combinatorial optimization problems be extended to multi-robot sequential assignment planning? In addition to the challenge of achieving near-optimal performance in large problems, transferability to an unseen number of robots and tasks is another key challenge for real-world applications. In this paper, we suggest a method that achieves the first success in both challenges for robot/machine scheduling problems.\n  \n Our method comprises of three components. First, we show any robot scheduling problem can be expressed as a random probabilistic graphical model (PGM). We develop a mean-field inference method for random PGM and use it for Q-function inference. Second, we show that transferability can be achieved by carefully designing two-step sequential encoding of problem state. Third, we resolve the computational scalability issue of fitted Q-iteration by suggesting a heuristic auction-based Q-iteration fitting method enabled by transferability we achieved.\n  \n[{'summary_text': 'can the success of reinforcement learning methods for simple combinatorial optimization problems be extended to multi-robot sequential assignment planning? in this paper, we suggest a method that achieves the first success in both challenges for robot/machine scheduling problems . our method comprises of three components: first, we show any robot scheduling problem can be expressed as a random probabilistic graphical model (PGM)'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 114/1992 [07:06<2:06:50,  4.05s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In many partially observable scenarios, Reinforcement Learning (RL) agents must rely on long-term memory in order to learn an optimal policy. We demonstrate that using techniques from NLP and supervised learning fails at RL tasks due to stochasticity from the environment and from exploration. Utilizing our insights on the limitations of traditional memory methods in RL, we propose AMRL, a class of models that can learn better policies with greater sample efficiency and are resilient to noisy inputs. Specifically, our models use a standard memory module to summarize short-term context, and then aggregate all prior states from the standard model without respect to order. We show that this provides advantages both in terms of gradient decay and signal-to-noise ratio over time. Evaluating in Minecraft and maze environments that test long-term memory, we find that our model improves average return by 19% over a baseline that has the same number of parameters and by 9% over a stronger baseline that has far more parameters.<|endoftext|>\n[{'summary_text': 'in many partially observable scenarios, Reinforcement Learning agents must rely on long-term memory in order to learn an optimal policy . using techniques from NLP and supervised learning fails at RL tasks due to stochasticity from the environment and from exploration . we propose AMRL, a class of models that can learn better policies with greater sample efficiency and are resilient to noisy inputs .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 115/1992 [07:10<2:06:02,  4.03s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present a new approach and a novel architecture, termed WSNet, for learning compact and efficient deep neural networks. Existing approaches conventionally learn full model parameters independently and then compress them via \\emph{ad hoc} processing such as model pruning or filter factorization. Alternatively, WSNet proposes learning model parameters by sampling from a compact set of learnable parameters, which naturally enforces {parameter sharing} throughout the learning process. We demonstrate that such a novel weight sampling approach (and induced WSNet) promotes both weights and computation sharing favorably. By employing this method, we can more efficiently learn much smaller networks with competitive performance compared to baseline networks with equal numbers of convolution filters. Specifically, we consider learning compact and efficient 1D convolutional neural networks for audio classification. Extensive experiments on multiple audio classification datasets verify the effectiveness of WSNet. Combined with weight quantization, the resulted models are up to \\textbf{180$\\times$} smaller and theoretically up\n[{'summary_text': 'existing approaches conventionally learn full model parameters independently . then compress them via emphad hoc processing . WSNet proposes learning model parameters by sampling from a compact set of learnable parameters, which naturally enforces parameter sharing throughout the learning process .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 116/1992 [07:14<2:05:04,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The use of deep learning for a wide range of data problems has increased the need for understanding and diagnosing these models, and deep learning interpretation techniques have become an essential tool for data analysts. Although numerous model interpretation methods have been proposed in recent years, most of these procedures are based on heuristics with little or no theoretical guarantees. In this work, we propose a statistical framework for saliency estimation for black box computer vision models. We build a model-agnostic estimation procedure that is statistically consistent and passes the saliency checks of Adebayo et al. (2018). Our method requires solving a linear program, whose solution can be efficiently computed in polynomial time. Through our theoretical analysis, we establish an upper bound on the number of model evaluations needed to recover the region of importance with high probability, and build a new perturbation scheme for estimation of local gradients that is shown to be more efficient than the commonly used random perturbation schemes. Validity of the new\n[{'summary_text': 'the use of deep learning for a wide range of data problems has increased the need for understanding and diagnosing these models . many model interpretation methods have been proposed in recent years, but most of these procedures are based on heuristics with little or no theoretical guarantees . in this work, we propose a statistical framework for saliency estimation for black box computer vision models.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 117/1992 [07:18<2:06:16,  4.04s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Convolutional neural networks (CNNs) in recent years have made a dramatic impact in science, technology and industry, yet the theoretical mechanism of CNN architecture design remains surprisingly vague. The CNN neurons, including its distinctive element, convolutional filters, are known to be learnable features, yet their individual role in producing the output is rather unclear. The thesis of this work is that not all neurons are equally important and some of them contain more useful information to perform a given task. Hence, we propose to quantify and rank neuron importance, and directly incorporate neuron importance in the objective function under two formulations: (1) a game theoretical approach based on Shapley value which computes the marginal contribution of each filter; and (2) a probabilistic approach based on what-we-call, the importance switch using variational inference. Using these two methods we confirm the general theory that some of the neurons are inherently more important than the others. Various experiments illustrate that learned ranks can be readily\n[{'summary_text': 'Various experiments illustrate that learned ranks can be readilyConvolutional neural networks (CNNs) in recent years have made a dramatic impact in science, technology and industry . but the theoretical mechanism of CNN architecture design remains surprisingly vague . a game theoretical approach based on Shapley value computes the marginal contribution of each filter .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 118/1992 [07:23<2:06:41,  4.06s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The standard variational lower bounds used to train latent variable models produce biased estimates of most quantities of interest. We introduce an unbiased estimator of the log marginal likelihood and its gradients for latent variable models based on randomized truncation of infinite series. If parameterized by an encoder-decoder architecture, the parameters of the encoder can be optimized to minimize its variance of this estimator. We show that models trained using our estimator give better test-set likelihoods than a standard importance-sampling based approach for the same average computational cost. This estimator also allows use of latent variable models for tasks where unbiased estimators, rather than marginal likelihood lower bounds, are preferred, such as minimizing reverse KL divergences and estimating score functions.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we introduce an unbiased estimator of the log marginal likelihood and its gradients for latent variable models based on randomized truncation of infinite series . models trained using our estimator give better test-set likelihoods than a standard importance-sampling based approach for the same average computational cost .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 119/1992 [07:26<2:05:23,  4.02s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Adaptive regularization methods pre-multiply a descent direction by a preconditioning matrix. Due to the large number of parameters of machine learning problems, full-matrix preconditioning methods are prohibitively expensive. We show how to modify full-matrix adaptive regularization in order to make it practical and effective. We also provide novel theoretical analysis\n for adaptive regularization in non-convex optimization settings. The core of our algorithm, termed GGT, consists of efficient inverse computation of square roots of low-rank matrices. Our preliminary experiments underscore improved convergence rate of GGT across a variety of synthetic tasks and standard deep learning benchmarks.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'full-matrix preconditioning methods are prohibitively expensive . the core of our algorithm, termed GGT, consists of efficient inverse computation of square roots of low-rank matrices .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 120/1992 [07:30<1:59:06,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"State-of-the-art Unsupervised Domain Adaptation (UDA) methods learn transferable features by minimizing the feature distribution discrepancy between the source and target domains. Different from these methods which do not model the feature distributions explicitly, in this paper, we explore explicit feature distribution modeling for UDA. In particular, we propose Distribution Matching Prototypical Network (DMPN) to model the deep features from each domain as Gaussian mixture distributions. With explicit feature distribution modeling, we can easily measure the discrepancy between the two domains. In DMPN, we propose two new domain discrepancy losses with probabilistic interpretations. The first one minimizes the distances between the corresponding Gaussian component means of the source and target data. The second one minimizes the pseudo negative log likelihood of generating the target features from source feature distribution. To learn both discriminative and domain invariant features, DMPN is trained by minimizing the classification loss on the labeled source data and the domain discrepancy\n[{'summary_text': 'state-of-the-art unsupervised domain Adaptation (UDA) methods learn transferable features by minimizing the feature distribution discrepancy between the source and target domains . in this paper, we explore explicit feature distribution modeling for UDA . we propose Distribution Matching Prototypical Network (DMPN) to model the deep features from each domain as Gaussian mixture distributions.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 121/1992 [07:34<2:02:55,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Typical amortized inference in variational autoencoders is specialized for a single probabilistic query. Here we propose an inference network architecture that generalizes to unseen probabilistic queries. Instead of an encoder-decoder pair, we can train a single inference network directly from data, using a cost function that is stochastic not only over samples, but also over queries. We can use this network to perform the same inference tasks as we would in an undirected graphical model with hidden variables, without having to deal with the intractable partition function. The results can be mapped to the learning of an actual undirected model, which is a notoriously hard problem. Our network also marginalizes nuisance variables as required.   We show that our approach generalizes to unseen probabilistic queries on also unseen test data, providing fast and flexible inference. Experiments show that this approach outperforms or matches PCD and AdVIL on 9 benchmark datasets.<|endoftext|><|endoftext|>\n[{'summary_text': 'Typical amortized inference in variational autoencoders is specialized for a single probabilistic query . instead of an encoder-decoder pair, we can train an inference network directly from data, using a cost function that is stochastic not only over samples, but also over queries . results can be mapped to learning of an actual undirected model, which is a notoriously hard problem .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 122/1992 [07:38<2:03:57,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"A variety of cooperative multi-agent control problems require agents to achieve individual goals while contributing to collective success. This multi-goal multi-agent setting poses difficulties for recent algorithms, which primarily target settings with a single global reward, due to two new challenges: efficient exploration for learning both individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents. To address both challenges, we restructure the problem into a novel two-stage curriculum, in which single-agent goal attainment is learned prior to learning multi-agent cooperation, and we derive a new multi-goal multi-agent policy gradient with a credit function for localized credit assignment. We use a function augmentation scheme to bridge value and policy functions across the curriculum. The complete architecture, called CM3, learns significantly faster than direct adaptations of existing algorithms on three challenging multi-goal multi-agent problems: cooperative navigation in difficult formations, negotiating multi-vehicle lane changes in the SUMO\n[{'summary_text': \"the complete architecture, called CM3, learns significantly faster than direct adaptations of existing algorithms on three challenging multi-goal multi-agent problems . efficient exploration for learning individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents . to address both challenges, we restructure the problem into a novel two-stage curriculum .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 123/1992 [07:42<2:06:27,  4.06s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We show that there exists an inherent tension between the goal of adversarial robustness and that of standard generalization. \n Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists even in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed in practice. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than  standard classifiers. These differences, in particular, seem to result in unexpected benefits: the features learned by robust models tend to align better with salient data characteristics and human perception.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy . this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists even in a fairly simple and natural setting . findings also corroborate a similar phenomenon observed in practice .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 124/1992 [07:46<2:03:10,  3.96s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper, we first identify \\textit{angle bias}, a simple but remarkable phenomenon that causes the vanishing gradient problem in a multilayer perceptron (MLP) with sigmoid activation functions. We then propose \\textit{linearly constrained weights (LCW)} to reduce the angle bias in a neural network, so as to train the network under the constraints that the sum of the elements of each weight vector is zero. A reparameterization technique is presented to efficiently train a model with LCW by embedding the constraints on weight vectors into the structure of the network. Interestingly, batch normalization (Ioffe & Szegedy, 2015) can be viewed as a mechanism to correct angle bias. Preliminary experiments show that LCW helps train a 100-layered MLP more efficiently than does batch normalization.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this paper, we first identify textitangle bias, a simple but remarkable phenomenon that causes the vanishing gradient problem in a multilayer perceptron (MLP) with sigmoid activation functions . a reparameterization technique is presented to efficiently train a model with LCW by embedding the constraints on weight vectors into the structure of the network .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▋         | 125/1992 [07:50<2:04:40,  4.01s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"While neural networks can be trained to map from one specific dataset to another, they usually do not learn a generalized transformation that can extrapolate accurately outside the space of training. For instance, a generative adversarial network (GAN) exclusively trained to transform images of cars from light to dark might not have the same effect on images of horses. This is because neural networks are good at generation within the manifold of the data that they are trained on. However, generating new samples outside of the manifold or extrapolating \"out-of-sample\" is a much harder problem that has been less well studied. To address this, we introduce a technique called neuron editing that learns how neurons encode an edit for a particular transformation in a latent space. We use an autoencoder to decompose the variation within the dataset into activations of different neurons and generate transformed data by defining an editing transformation on those neurons. By performing the transformation in a latent trained space, we encode fairly complex and non-linear\n[{'summary_text': 'neuron editing learns how neurons encode an edit for a particular transformation in a latent space . a generative adversarial network (GAN) exclusively trained to transform images of cars might not have the same effect on images of horses . this is because neural networks are good at generation within the manifold of the data they are trained on .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▋         | 126/1992 [07:54<2:05:51,  4.05s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Long-term video prediction is highly challenging since it entails simultaneously capturing spatial and temporal information across a long range of image frames.Standard recurrent models are ineffective since they are prone to error propagation and cannot effectively capture higher-order correlations. A potential solution is to extend to higher-order spatio-temporal recurrent models. However, such a model requires  a  large number of parameters and operations, making it intractable  to learn in practice and is prone to overfitting. In this work, we propose convolutional tensor-train LSTM (Conv-TT-LSTM), which  learns higher-orderConvolutional LSTM (ConvLSTM) efficiently using convolutional  tensor-train decomposition (CTTD). Our proposed model naturally incorporates higher-order spatio-temporal information at a small cost of memory and computation by using efficient low-rank tensor representations. We evaluate our model on Moving-MNIST\n[{'summary_text': 'long-term video prediction is highly challenging because it entails simultaneously capturing spatial and temporal information across a long range of image frames . a potential solution is to extend to higher-order spatio-temporal recurrent models . however, such a model requires a large number of parameters and operations, making it intractable to learn in practice .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▋         | 127/1992 [07:58<2:06:34,  4.07s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We introduce a novel method that enables parameter-efficient transfer and multi-task learning with deep neural networks. The basic approach is to learn a model patch - a small set of parameters - that will specialize to each task, instead of fine-tuning the last layer or the entire network. For instance, we show that learning a set of scales and biases is sufficient to convert a pretrained network to perform well on qualitatively different problems (e.g. converting a Single Shot MultiBox Detection (SSD) model into a 1000-class image classification model while reusing 98% of parameters of the SSD feature extractor). Similarly, we show that re-learning existing low-parameter layers (such as depth-wise convolutions) while keeping the rest of the network frozen also improves transfer-learning accuracy significantly. Our approach allows both simultaneous (multi-task) as well as sequential transfer learning. In several multi-task learning problems, despite using much fewer parameters than traditional logits-\n[{'summary_text': 'basic approach is to learn a model patch - a small set of parameters . learning a set of scales and biases is sufficient to convert a pretrained network to perform well on qualitatively different problems . re-learning existing low-parameter layers while keeping rest of the network frozen also improves transfer-learning accuracy significantly .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▋         | 128/1992 [08:03<2:06:33,  4.07s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Machine learning workloads are often expensive to train, taking weeks to converge. The current generation of frameworks relies on custom back-ends in order to achieve efficiency, making it impractical to train models on less common hardware where no such back-ends exist. Knossos builds on recent work that avoids the need for hand-written libraries, instead compiles machine learning models in much the same way one would compile other kinds of software. In order to make the resulting code efficient, the Knossos complier directly optimises the abstract syntax tree of the program. However in contrast to traditional compilers that employ hand-written optimisation passes, we take a rewriting approach driven by the $A^\\star$ search algorithm and a learn value function that evaluates future potential cost reduction of taking various rewriting actions to the program. We show that Knossos can automatically learned optimisations that past compliers had to implement by hand. Furthermore, we demonstrate that Knossos can achieve wall time reduction compared to\n[{'summary_text': 'the current generation relies on custom back-ends in order to achieve efficiency . Knossos builds on recent work that avoids the need for hand-written libraries, instead compiles machine learning models in much the same way one would compile other kinds of software .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▋         | 129/1992 [08:07<2:07:47,  4.12s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Differentiable planning network architecture has shown to be powerful in solving transfer planning tasks while possesses a simple end-to-end training feature. Many great planning architectures that have been proposed later in literature are inspired by this design principle in which a recursive network architecture is applied to emulate backup operations of a value  iteration algorithm. However existing frame-works can only learn and plan effectively on domains with a lattice structure, i.e. regular graphs embedded in a certain Euclidean space. In this paper, we propose a general planning network, called Graph-based Motion Planning Networks (GrMPN), that will be able to i) learn and plan on general irregular graphs, hence ii) render existing planning network architectures special cases. The proposed GrMPN framework is invariant to task graph permutation, i.e. graph isormophism. As a result, GrMPN possesses the generalization strength and data-efficiency ability. We demonstrate the performance of the proposed GrMPN method\n[{'summary_text': 'many great planning architectures that have been proposed later in literature are inspired by this design principle in which a recursive network architecture is applied to emulate backup operations of a value iteration algorithm . existing frame-works can only learn and plan effectively on domains with a lattice structure, i.e. regular graphs embedded in a certain Euclidean space .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 130/1992 [08:11<2:08:27,  4.14s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Conversational machine comprehension requires a deep understanding of the conversation history. To enable traditional, single-turn models to encode the history comprehensively, we introduce Flow, a mechanism that can incorporate intermediate representations generated during the process of answering previous questions, through an alternating parallel processing structure. Compared to shallow approaches that concatenate previous questions/answers as input, Flow integrates the latent semantics of the conversation history more deeply. Our model, FlowQA, shows superior performance on two recently proposed conversational challenges (+7.2% F1 on CoQA and +4.0% on QuAC). The effectiveness of Flow also shows in other tasks. By reducing sequential instruction understanding to conversational machine comprehension, FlowQA outperforms the best models on all three domains in SCONE, with +1.8% to +4.4% improvement in accuracy.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'to enable traditional, single-turn models to encode the history comprehensively, we introduce Flow, a mechanism that can incorporate intermediate representations generated during the process of answering previous questions . by reducing sequential instruction understanding to conversational machine comprehension, FlowQA outperforms the best models on all three domains in SCONE, with +1.8% to +4.4% improvement in accuracy .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 131/1992 [08:15<2:05:56,  4.06s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Human-computer conversation systems have attracted much attention in Natural Language Processing. Conversation systems can be roughly divided into two categories: retrieval-based and generation-based systems. Retrieval systems search a user-issued utterance (namely a query) in a large conversational repository and return a reply that best matches the query. Generative approaches synthesize new replies. Both ways have certain advantages but suffer from their own disadvantages. We propose a novel ensemble of retrieval-based and generation-based conversation system. The retrieved candidates, in addition to the original query, are fed to a reply generator via a neural network, so that the model is aware of more information. The generated reply together with the retrieved ones then participates in a re-ranking process to find the final reply to output. Experimental results show that such an ensemble system outperforms each single module by a large margin.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'retrieved candidates are fed to a reply generator via a neural network . the generated reply together with the retrieved ones participates in a re-ranking process to find the final reply to output .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 132/1992 [08:18<1:59:35,  3.86s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Human brain function as measured by functional magnetic resonance imaging\n (fMRI), exhibits a rich diversity. In response, understanding the individual variability\n of brain function and its association with behavior has become one of the\n major concerns in modern cognitive neuroscience. Our work is motivated by the\n view that generative models provide a useful tool for understanding this variability.\n To this end, this manuscript presents two novel generative models trained\n on real neuroimaging data which synthesize task-dependent functional brain images.\n Brain images are high dimensional tensors which exhibit structured spatial\n correlations. Thus, both models are 3D conditional Generative Adversarial networks\n (GANs) which apply Convolutional Neural Networks (CNNs) to learn an\n abstraction of brain image representations. Our results show that the generated\n brain images are diverse, yet task dependent. In addition to qualitative evaluation,\n we utilize the generated synthetic brain volumes as additional training data to improve\n downstream fMRI classifiers (also\n[{'summary_text': 'human brain function as measured by functional magnetic resonance imaging (fMRI), exhibits a rich diversity . to this end, this manuscript presents two novel generative models trained on real neuroimaging data which synthesize task-dependent functional brain images . both models are 3D conditional Generative Adversarial networks (GANs) which apply Convolutional Neural networks (CNNs) to learn an abstraction of brain image representations .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 133/1992 [08:23<2:04:04,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Large Transformer models routinely achieve state-of-the-art results on\n a number of tasks but training these models can be prohibitively costly,\n especially on long sequences. We introduce two techniques to improve\n the efficiency of Transformers. For one, we replace dot-product attention\n by one that uses locality-sensitive hashing, changing its complexity\n from O(L^2) to O(L), where L is the length of the sequence.\n Furthermore, we use reversible residual layers instead of the standard\n residuals, which allows storing activations only once in the training\n process instead of N times, where N is the number of layers.\n The resulting model, the Reformer, performs on par with Transformer models\n while being much more memory-efficient and much faster on long sequences.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'large Transformer models routinely achieve state-of-the-art results on a number of tasks . for one, we replace dot-product attention by one that uses locality-sensitive hashing . resulting model, the Reformer, performs on par with Transformer models .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 134/1992 [08:26<2:01:10,  3.91s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Source separation for music is the task of isolating contributions, or stems, from different instruments recorded individually and arranged together to form a song. Such components include voice, bass, drums and any other accompaniments. While end-to-end models that directly generate the waveform are state-of-the-art in many audio synthesis problems, the best multi-instrument source separation models generate masks on the magnitude spectrum and achieve performances far above current end-to-end, waveform-to-waveform models. We present an in-depth analysis of a new architecture, which we will refer to as Demucs, based on a (transposed) convolutional autoencoder, with a bidirectional LSTM at the bottleneck layer and skip-connections as in U-Networks (Ronneberger et al., 2015). Compared to the state-of-the-art waveform-to-waveform model, Wave-U-Net (St\n[{'summary_text': 'source separation for music is the task of isolating contributions, or stems, from different instruments recorded individually and arranged together to form a song . such components include voice, bass, drums and any other accompaniments . the best multi-instrument source separation models generate masks on the magnitude spectrum .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 135/1992 [08:30<2:03:40,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose a framework for extreme learned image compression based on Generative Adversarial Networks (GANs), obtaining visually pleasing images at significantly lower bitrates than previous methods. This is made possible through our GAN formulation of learned compression combined with a generator/decoder which operates on the full-resolution image and is trained in combination with a multi-scale discriminator. Additionally, if a semantic label map of the original image is available, our method can fully synthesize unimportant regions in the decoded image such as streets and trees from the label map, therefore only requiring the storage of the preserved region and the semantic label map. A user study confirms that for low bitrates, our approach is preferred to state-of-the-art methods, even when they use more than double the bits.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we propose a framework for extreme learned image compression based on Generative Adversarial Networks (GANs), obtaining visually pleasing images at significantly lower bitrates than previous methods . this is made possible through our GAN formulation of learned compression combined with a generator/decoder which operates on the full-resolution image . if a semantic label map of the original image is available, our method can fully synthesize unimportant regions in the decoded image such as streets and trees from the label'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 136/1992 [08:35<2:06:15,  4.08s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Stochastic neural networks with discrete random variables are an important class of models for their expressivity and interpretability. Since direct differentiation and backpropagation is not possible, Monte Carlo gradient estimation techniques have been widely employed for training such models. Efficient stochastic gradient estimators, such Straight-Through and Gumbel-Softmax, work well for shallow models with one or two stochastic layers. Their performance, however, suffers with increasing model complexity.\n In this work we focus on stochastic networks with multiple layers of Boolean latent variables. To analyze such such networks, we employ the framework of harmonic analysis for Boolean functions.   We use it to derive an analytic formulation for the source of bias in the biased Straight-Through estimator. Based on the analysis we propose \\emph{FouST}, a simple gradient estimation algorithm that relies on three simple bias reduction steps. Extensive experiments show that FouST performs favorably compared to state-of-the-\n[{'summary_text': 'Monte Carlo gradient estimation techniques have been widely employed for training such models . efficient stochastic gradient estimators, such straight-Through and Gumbel-Softmax, work well for shallow models with one or two stochaastic layers . their performance, however, suffers with increasing model complexity .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 137/1992 [08:39<2:06:00,  4.08s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'momentum-based methods, in conjunction with stochastic gradient descent, are widely used when training machine learning models . however, there is little theoretical understanding on the generalization error of such methods . in practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 138/1992 [08:42<2:01:32,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression.   In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In many applications this expensive initialization is not practical, for example streaming algorithms --- where inputs are ephemeral and can only be inspected a small number of times.   In this paper we explore the learning of approximate set membership over a stream of data in one-shot via meta-learning. We propose a novel memory architecture, the Neural Bloom Filter, which we show to be more compressive than Bloom Filters and several existing memory-augmented neural networks in scenarios of skewed data or structured sets.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence . in many applications this expensive initialization is not practical, for example streaming algorithms . we propose a novel memory architecture, the Neural Bloom Filter .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 139/1992 [08:46<1:58:32,  3.84s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"There is significant recent evidence in supervised learning that, in the over-parametrized setting, wider networks achieve better test error. In other words, the bias-variance tradeoff is not directly observable when increasing network width arbitrarily. We investigate whether a corresponding phenomenon is present in reinforcement learning. We experiment on four OpenAI Gym environments, increasing the width of the value and policy networks beyond their prescribed values. Our empirical results lend support to this hypothesis. However, tuning the hyperparameters of each network width separately remains as important future work in environments/algorithms where the optimal hyperparameters vary noticably across widths, confounding the results when the same hyperparameters are used for all widths.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'bias-variance tradeoff is not directly observable when increasing network width arbitrarily . we investigate whether a corresponding phenomenon is present in reinforcement learning . tuning the hyperparameters of each network width separately remains as important future work in environments/algorithms .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 140/1992 [08:50<1:55:53,  3.75s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Data augmentation is a useful technique to enlarge the size of the training set and prevent overfitting for different machine learning tasks when training data is scarce. However, current data augmentation techniques rely heavily on human design and domain knowledge, and existing automated approaches are yet to fully exploit the latent features in the training dataset. In this paper we propose  \\textit{Parallel Adaptive GAN Data Augmentation}(PAGANDA), where the training set adaptively enriches itself with sample images automatically constructed from Generative Adversarial Networks (GANs) trained in parallel. We demonstrate by experiments that our data augmentation strategy, with little model-specific considerations, can be easily adapted to cross-domain deep learning/machine learning tasks such as image classification and image inpainting, while significantly improving model performance in both tasks. Our source code and experimental details are available at \\url{https://github.com/miaojiang1987/k-folder-data-augment\n[{'summary_text': 'current data augmentation techniques rely heavily on human design and domain knowledge . existing automated approaches are yet to fully exploit the latent features in the training dataset .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 141/1992 [08:53<1:53:57,  3.69s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Inspired by the recent successes of deep generative models for Text-To-Speech (TTS) such as WaveNet (van den Oord et al., 2016) and Tacotron (Wang et al., 2017), this article proposes the use of a deep generative model tailored for Automatic Speech Recognition (ASR) as the primary acoustic model (AM) for an overall recognition system with a separate language model (LM). Two dimensions of depth are considered: (1) the use of mixture density networks, both autoregressive and non-autoregressive, to generate density functions capable of modeling acoustic input sequences with much more powerful conditioning than the first-generation generative models for ASR, Gaussian Mixture Models / Hidden Markov Models (GMM/HMMs), and (2) the use of standard LSTMs, in the spirit of the original tandem approach, to produce discriminative feature vectors for generative modeling. Combining mixture density networks\n[{'summary_text': 'the article proposes the use of a deep generative model tailored for Automatic Speech Recognition (ASR) as the primary acoustic model (AM) for an overall recognition system with a separate language model (LM) two dimensions of depth are considered: (1) use of mixture density networks, both autoregressive and non-autoregressive, to generate density functions . the first-generation generative models for ASR, Gaussian Mixture Models / Hidden Markov Models (GMM/HMMs'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 142/1992 [08:58<2:01:40,  3.95s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recent efforts on combining deep models with probabilistic graphical models are promising in providing flexible models that are also easy to interpret. We propose a variational message-passing algorithm for variational inference in such models. We make three contributions. First, we propose structured inference networks that incorporate the structure of the graphical model in the inference network of variational auto-encoders (VAE). Second, we establish conditions under which such inference networks enable fast amortized inference similar to VAE. Finally, we derive a variational message passing algorithm to perform efficient natural-gradient inference while retaining the efficiency of the amortized inference. By simultaneously enabling structured, amortized, and natural-gradient inference for deep structured models, our method simplifies and generalizes existing methods.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'recent efforts on combining deep models with probabilistic graphical models are promising in providing flexible models that are also easy to interpret . we propose a variational message-passing algorithm for variational inference in such models .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 143/1992 [09:01<1:57:51,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We introduce our Distribution Regression Network (DRN) which performs regression from input probability distributions to output probability distributions. Compared to existing methods, DRN learns with fewer model parameters and easily extends to multiple input and multiple output distributions. On synthetic and real-world datasets, DRN performs similarly or better than the state-of-the-art. Furthermore, DRN generalizes the conventional multilayer perceptron (MLP). In the framework of MLP, each node encodes a real number, whereas in DRN, each node encodes a probability distribution.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'DRN learns with less model parameters and easily extends to multiple input and multiple output distributions . compared to existing methods, DRN performs similarly or better than the state-of-the-art .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 144/1992 [09:05<1:54:53,  3.73s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"This paper presents a generic framework to tackle the crucial class mismatch problem in unsupervised domain adaptation (UDA) for multi-class distributions.   Previous adversarial learning methods condition domain alignment only on pseudo labels, but noisy and inaccurate pseudo labels may perturb the multi-class distribution embedded in probabilistic predictions, hence bringing insufficient alleviation to the latent mismatch problem.   Compared with pseudo labels, class prototypes are more accurate and reliable since they summarize over all the instances and are  able  to  represent  the  inherent  semantic  distribution  shared  across  domains. Therefore, we propose a novel Prototype-Assisted Adversarial Learning (PAAL) scheme, which incorporates instance probabilistic predictions and class prototypes together  to  provide  reliable  indicators  for  adversarial  domain  alignment.    With the PAAL scheme,  we align both the instance feature representations and class prototype  representations  to  alleviate  the  mismatch \n[{'summary_text': 'this paper presents a generic framework to tackle the crucial class mismatch problem in unsupervised domain adaptation . previous adversarial learning methods condition domain alignment only on pseudo labels, but noisy and inaccurate pseudo labels may perturb the multi-class distribution embedded in probabilistic predictions . with the PAAL scheme, we align both the instance feature representations and class prototype representations to alleviate the mismatch .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 145/1992 [09:09<1:58:48,  3.86s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Learning control policies in robotic tasks requires a large number of interactions due to small learning rates, bounds on the updates or unknown constraints. In contrast humans can infer protective and safe solutions after a single failure or unexpected observation. \n In order to reach similar performance, we developed a hierarchical Bayesian optimization algorithm that replicates the cognitive inference and memorization process for avoiding failures in motor control tasks. A Gaussian Process implements the modeling and the sampling of the acquisition function. This enables rapid learning with large learning rates while a mental replay phase ensures that policy regions that led to failures are inhibited during the sampling process.    \n The features of the hierarchical Bayesian optimization method are evaluated in a simulated and physiological humanoid postural balancing task. We quantitatively compare the human learning performance to our learning approach by evaluating the deviations of the center of mass during training. Our results show that we can reproduce the efficient learning of human subjects in postural control tasks which provides a testable model for future physiological motor\n[{'summary_text': 'this enables rapid learning with large learning rates while a mental replay phase ensures that policy regions that led to failures are inhibited during the sampling process . the features of the hierarchical Bayesian optimization method are evaluated in a simulated and physiological humanoid postural balancing task .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 146/1992 [09:13<2:02:01,  3.97s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In order to choose a neural network architecture that will be effective for a particular modeling problem, one must understand the limitations imposed by each of the potential options. These limitations are typically described in terms of information theoretic bounds, or by comparing the relative complexity needed to approximate example functions between different architectures. In this paper, we examine the topological constraints that the architecture of a neural network imposes on the level sets of all the functions that it is able to approximate. This approach is novel for both the nature of the limitations and the fact that they are independent of network depth for a broad family of activation functions.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in order to choose a neural network architecture that will be effective for a particular modeling problem, one must understand the limitations imposed by each of the potential options . these limitations are typically described in terms of information theoretic bounds, or by comparing the relative complexity needed to approximate example functions between different architectures . this approach is novel for both the nature of the limitations and the fact that they are independent of network depth .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 147/1992 [09:17<2:02:32,  3.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Understanding the groundbreaking performance of Deep Neural Networks is one\n of the greatest challenges to the scientific community today. In this work, we\n introduce an information theoretic viewpoint on the behavior of deep networks\n optimization processes and their generalization abilities. By studying the Information\n Plane, the plane of the mutual information between the input variable and\n the desired label, for each hidden layer. Specifically, we show that the training of\n the network is characterized by a rapid increase in the mutual information (MI)\n between the layers and the target label, followed by a longer decrease in the MI\n between the layers and the input variable. Further, we explicitly show that these\n two fundamental information-theoretic quantities correspond to the generalization\n error of the network, as a result of introducing a new generalization bound that is\n exponential in the representation compression. The analysis focuses on typical\n patterns of large-scale problems. For this purpose, we introduce a novel analytic\n bound on the mutual information\n[{'summary_text': 'understanding the groundbreaking performance of Deep Neural Networks is one of the greatest challenges to the scientific community today . in this work, we introduce an information theoretic viewpoint on the behavior of deep networks optimization processes and their generalization abilities . by studying the Information Plane, the plane of the mutual information between the input variable and the desired label, for each hidden layer .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 148/1992 [09:21<2:03:30,  4.02s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The information bottleneck principle is an elegant and useful approach to representation learning. In this paper, we investigate the problem of representation learning in the context of reinforcement learning using the information bottleneck framework, aiming at improving the sample efficiency of the learning algorithms.We analytically derive the optimal conditional distribution of the representation, and provide a variational lower bound. Then, we maximize this lower bound with the Stein variational (SV) gradient method. \n We incorporate this framework in the advantageous actor critic algorithm (A2C) and the proximal policy optimization algorithm (PPO). Our experimental results show that our framework can improve the sample efficiency of vanilla A2C and PPO significantly. Finally, we study the information-bottleneck (IB) perspective in deep RL with the algorithm called mutual information neural estimation(MINE).\n We experimentally verify that the information extraction-compression process also exists in deep RL and our framework is capable of accelerating this process. We also analyze the relationship between\n[{'summary_text': 'in this paper, we investigate the problem of representation learning using the information bottleneck framework . we analyse the relationship between the information-bottleneck (IB) perspective in deep RL with the algorithm called mutual information neural estimation .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 149/1992 [09:25<2:01:12,  3.95s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this work we explore a straightforward variational Bayes scheme for Recurrent Neural Networks.\n Firstly, we show that a simple adaptation of truncated backpropagation through time can yield good quality uncertainty estimates and superior regularisation at only a small extra computational cost during training, also reducing the amount of parameters by 80\\%.\n Secondly, we demonstrate how a novel kind of posterior approximation yields further improvements to the performance of Bayesian RNNs. We incorporate local gradient information into the approximate posterior to sharpen it around the current batch statistics. We show how this technique is not exclusive to recurrent neural networks and can be applied more widely to train Bayesian neural networks.\n We also empirically demonstrate how Bayesian RNNs are superior to traditional RNNs on a language modelling benchmark and an image captioning task, as well as showing how each of these methods improve our model over a variety of other schemes for training them. We also introduce a new benchmark for studying uncertainty for language models\n[{'summary_text': 'a simple adaptation of truncated backpropagation through time can yield good quality uncertainty estimates and superior regularisation at only a small extra computational cost during training, also reducing the amount of parameters by 80% . a novel kind of posterior approximation yields further improvements to the performance of Bayesian RNNs .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 150/1992 [09:29<2:03:21,  4.02s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The goal of the paper is to propose an algorithm for learning the most generalizable solution from given training data. It is shown that Bayesian approach leads to a solution that dependent on statistics of training data and not on particular\n samples. The solution is stable under perturbations of training data because it is defined by an integral contribution of multiple maxima of the likelihood and not by a single global maximum. Specifically, the Bayesian probability distribution\n of parameters (weights) of a probabilistic model given by a neural network is estimated via recurrent variational approximations. Derived recurrent update rules correspond to SGD-type rules for finding a minimum of an effective loss that is an average of an original negative log-likelihood over the Gaussian distributions of weights, which makes it a function of means and variances. The effective loss is convex for large variances and non-convex in the limit of small variances. Among stationary solutions of the update rules there are trivial\n[{'summary_text': 'the goal of the paper is to propose an algorithm for learning the most generalizable solution from given training data . it is shown that Bayesian approach leads to a solution that dependent on statistics of training data and not on particular samples . the solution is stable under perturbations of training because it is defined by an integral contribution of multiple maxima of the likelihood and not by a single global maximum .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 151/1992 [09:33<2:05:08,  4.08s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step ---adding a mean shift to the input data--- to show that a transformation with no effect on the model can cause numerous methods to incorrectly attribute. We define input invariance as the requirement that a saliency method mirror the sensitivity of the model with respect to transformations of the input. We show, through several examples, that saliency methods that do not satisfy a input invariance property are unreliable and can lead to misleading and inaccurate attribution.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Saliency methods aim to explain the predictions of deep neural networks . these methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction . we define input invariance as the requirement that a saliency method mirror the sensitivity of the model with respect to transformations .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 152/1992 [09:37<2:00:39,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Gaussian processes are the leading class of distributions on random functions, but they suffer from well known issues including difficulty scaling and inflexibility with respect to certain shape constraints (such as nonnegativity). Here we propose Deep Random Splines, a flexible class of random functions obtained by transforming Gaussian noise through a deep neural network whose output are the parameters of a spline. Unlike Gaussian processes, Deep Random Splines allow us to readily enforce shape constraints while inheriting the richness and tractability of deep generative models. We also present an observational model for point process data which uses Deep Random Splines to model the intensity function of each point process and apply it to neuroscience data to obtain a low-dimensional representation of spiking activity. Inference is performed via a variational autoencoder that uses a novel recurrent encoder architecture that can handle multiple point processes as input.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Gaussian processes are the leading class of distributions on random functions . but they suffer from well known issues including difficulty scaling and inflexibility with respect to certain shape constraints (such as nonnegativity) deep random Splines allow us to readily enforce shape constraints while inheriting the richness and tractability of deep generative models .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 153/1992 [09:41<1:59:06,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"It is well-known that neural networks are universal approximators, but that deeper networks tend in practice to be more powerful than shallower ones. We shed light on this by proving that the total number of neurons m required to approximate natural classes of multivariate polynomials of n variables grows only linearly with n for deep neural networks, but grows exponentially when merely a single hidden layer is allowed. We also provide evidence that when the number of hidden layers is increased from 1 to k, the neuron requirement grows exponentially not with n but with n^{1/k}, suggesting that the minimum number of layers required for practical expressibility grows only logarithmically with n.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the total number of neurons m required to approximate natural classes of multivariate polynomials of n variables grows only linearly with n for deep neural networks . but when the number of hidden layers is increased from 1 to k, the neuron requirement grows exponentially when merely a single hidden layer is allowed . we also provide evidence that the minimum number of layers required for practical expressibility grows only logarithmically . it is well-known that neural networks are universal approximators, but that deeper'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 154/1992 [09:45<2:02:17,  3.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep neural networks have demonstrated unprecedented success in various knowledge management applications. However, the networks created are often very complex, with large numbers of trainable edges which require extensive computational resources. We note that many successful networks nevertheless often contain large numbers of redundant edges. Moreover, many of these edges may have negligible contributions towards the overall network performance. In this paper, we propose a novel iSparse framework and experimentally show, that we can sparsify the network, by 30-50%, without impacting the network performance. iSparse leverages a novel edge significance score, E, to determine the importance of an edge with respect to the final network output. Furthermore, iSparse can be applied both while training a model or on top of a pre-trained model, making it a  retraining-free approach - leading to a minimal computational overhead. Comparisons of iSparse against PFEC, NISP, DropConnect, and Retraining-Free on benchmark datasets show that i\n[{'summary_text': 'the networks created are often very complex, with large numbers of trainable edges which require extensive computational resources . iSparse leverages a novel edge significance score, E, to determine the importance of an edge with respect to the final network output . many of these edges may have negligible contributions towards the overall network performance .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 155/1992 [09:49<2:02:31,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Knowledge graph has gained increasing attention in recent years for its successful applications of numerous tasks. Despite the rapid growth of knowledge construction, knowledge graphs still suffer from severe incompletion and inevitably involve various kinds of errors. Several attempts have been made to complete knowledge graph as well as to detect noise. However, none of them considers unifying these two tasks even though they are inter-dependent and can mutually boost the performance of each other. In this paper, we proposed to jointly combine these two tasks with a unified Generative Adversarial Networks (GAN) framework to learn noise-aware knowledge graph embedding. Extensive experiments have demonstrated that our approach is superior to existing state-of-the-art algorithms both in regard to knowledge graph completion and error detection.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'knowledge graph has gained increasing attention in recent years for its successful applications of numerous tasks . despite the rapid growth of knowledge construction, knowledge graph still suffer from severe incompletion and inevitably involve various kinds of errors . none considers unifying these two tasks even though they are inter-dependent and can mutually boost the performance of each other .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 156/1992 [09:53<1:59:51,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"As distributed approaches to natural language semantics have developed and diversified, embedders for linguistic units larger than words (e.g., sentences) have come to play an increasingly important role.   To date, such embedders have been evaluated using benchmark tasks (e.g., GLUE) and linguistic probes.   We propose a comparative approach, nearest neighbor overlap (N2O), that quantifies similarity between embedders in a task-agnostic manner.   N2O requires only a collection of examples and is simple to understand: two embedders are more similar if, for the same set of inputs, there is greater overlap between the inputs' nearest neighbors.   We use N2O to compare 21 sentence embedders and show the effects of different design choices and architectures.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'to date, such embedders have been evaluated using benchmark tasks (e.g., GLUE) and linguistic probes . N2O requires only a collection of examples and is simple to understand .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 157/1992 [09:56<1:56:57,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The detection of out of distribution samples for image classification has been widely researched. Safety critical applications, such as autonomous driving, would benefit from the ability to localise the unusual objects causing the image to be out of distribution. This paper adapts state-of-the-art methods for detecting out of distribution images for image classification to the new task of detecting out of distribution pixels, which can localise the unusual objects. It further experimentally compares the adapted methods on two new datasets derived from existing semantic segmentation datasets using PSPNet and DeeplabV3+ architectures, as well as proposing a new metric for the task. The evaluation shows that the performance ranking of the compared methods does not transfer to the new task and every method performs significantly worse than their image-level counterparts.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'safety critical applications, such as autonomous driving, would benefit from the ability to localise the unusual objects causing the image to be out of distribution . this paper adapts state-of-the-art methods for detecting out of distributed images for image classification . it further compares the adapted methods on two new datasets derived from existing semantic segmentation datasets using PSPNet and DeeplabV3+ architectures .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 158/1992 [10:00<1:58:02,  3.86s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"This paper improves upon the line of research that formulates named entity recognition (NER) as a sequence-labeling problem. We use so-called black-box long short-term memory (LSTM) encoders to achieve state-of-the-art results while providing insightful understanding of what the auto-regressive model learns with a parallel self-attention mechanism. Specifically, we decouple the sequence-labeling problem of NER into entity chunking, e.g., Barack_B Obama_E was_O elected_O, and entity typing, e.g., Barack_PERSON Obama_PERSON was_NONE elected_NONE, and analyze how the model learns to, or has difficulties in, capturing text patterns for each of the subtasks. The insights we gain then lead us to explore a more sophisticated deep cross-Bi-LSTM encoder, which proves better at capturing global interactions given both empirical results and a theoretical justification\n[{'summary_text': 'this paper improves upon the line of research that formulates named entity recognition (NER) as a sequence-labeling problem . we use so-called black-box long-term memory (LSTM) encoders to achieve state-of-the-art results while providing insightful understanding of what the auto-regressive model learns with a parallel self-attention mechanism . the insights we gain then lead us to explore a more sophisticated deep cross-Bi-LSTM encoder .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 159/1992 [10:05<2:04:05,  4.06s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations, longer training times, and unexpected model degradation. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks . but further model increases become harder due to GPU/TPU memory limitations, longer training times, and unexpected model degradation . to address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 160/1992 [10:08<2:00:25,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Basis pursuit is a compressed sensing optimization in which the l1-norm is minimized subject to model error constraints. Here we use a deep neural network prior instead of l1-regularization. Using known noise statistics, we jointly learn the prior and reconstruct images without access to ground-truth data. During training, we use alternating minimization across an unrolled iterative network and jointly solve for the neural network weights and training set image reconstructions. At inference, we fix the weights and pass the measurements through the network. We compare reconstruction performance between unsupervised and supervised (i.e. with ground-truth) methods. We hypothesize this technique could be used to learn reconstruction when ground-truth data are unavailable, such as in high-resolution dynamic MRI.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'base pursuit is a compressed sensing optimization in which the l1-norm is minimized subject to model error constraints . using known noise statistics, we jointly learn the prior and reconstruct images without access to ground-truth data . at inference, we fix the weights and pass the measurements through the network .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 161/1992 [10:13<2:01:14,  3.97s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present a deep reinforcement learning approach to minimizing the execution cost of neural network computation graphs in an optimizing compiler. Unlike earlier learning-based works that require training the optimizer on the same graph to be optimized, we propose a learning approach that trains an optimizer offline and then generalizes to previously unseen graphs without further training. This allows our approach to produce high-quality execution decisions on real-world TensorFlow graphs in seconds instead of hours. We consider two optimization tasks for computation graphs: minimizing running time and peak memory usage. In comparison to an extensive set of baselines, our approach achieves significant improvements over classical and other learning-based methods on these two tasks.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we propose a learning approach that trains an optimizer offline and then generalizes to previously unseen graphs without further training . this allows our approach to produce high-quality execution decisions on real-world TensorFlow graphs in seconds instead of hours.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 162/1992 [10:16<1:56:39,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We introduce the notion of property signatures, a representation for programs and\n program specifications meant for consumption by machine learning algorithms.\n Given a function with input type τ_in and output type τ_out, a property is a function\n of type: (τ_in, τ_out) → Bool that (informally) describes some simple property\n of the function under consideration. For instance, if τ_in and τ_out are both lists\n of the same type, one property might ask ‘is the input list the same length as the\n output list?’. If we have a list of such properties, we can evaluate them all for our\n function to get a list of outputs that we will call the property signature. Crucially,\n we can ‘guess’ the property signature for a function given only a set of input/output\n pairs meant to specify that function. We discuss several potential applications of\n property signatures and show experimentally\n[{'summary_text': 'property signatures are a representation for programs and program specifications meant for consumption by machine learning algorithms . given a function with input type and output type _out, a property is a functional of type: (_in, -out)  Bool that (informally) describes some simple property of the function under consideration . if a list of such properties, one property might ask ‘is the input list the same length as the output list?’.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 163/1992 [10:20<2:02:21,  4.01s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The success of reinforcement learning in the real world has been limited to instrumented laboratory scenarios, often requiring arduous human supervision to enable continuous learning. In this work, we discuss the required elements of a robotic system that can continually and autonomously improve with data collected in the real world, and propose a particular instantiation of such a system. Subsequently, we investigate a number of challenges of learning without instrumentation -- including the lack of episodic resets, state estimation, and hand-engineered rewards -- and propose simple, scalable solutions to these challenges. We demonstrate the efficacy of our proposed system on dexterous robotic manipulation tasks in simulation and the real world, and also provide an insightful analysis and ablation study of the challenges associated with this learning paradigm.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the success of reinforcement learning in the real world has been limited to instrumented laboratory scenarios, often requiring arduous human supervision to enable continuous learning . in this work, we discuss the required elements of a robotic system that can continually and autonomously improve with data collected in the world . we propose a particular instantiation of such a system .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 164/1992 [10:24<2:01:12,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"As Machine Learning (ML) gets applied to security-critical or sensitive domains, there is a growing need for integrity and privacy for outsourced ML computations. A pragmatic solution comes from Trusted Execution Environments (TEEs), which use hardware and software protections to isolate sensitive computations from the untrusted software stack. However, these isolation guarantees come at a price in performance, compared to untrusted alternatives. This paper initiates the study of high performance execution of Deep Neural Networks (DNNs) in TEEs by efficiently partitioning DNN computations between trusted and untrusted devices. Building upon an efficient outsourcing scheme for matrix multiplication, we propose Slalom, a framework that securely delegates execution of all linear layers in a DNN from a TEE (e.g., Intel SGX or Sanctum) to a faster, yet untrusted, co-located processor. We evaluate Slalom by running DNNs in an Intel SGX\n[{'summary_text': 'trusted Execution Environments (TEEs) use hardware and software protections to isolate sensitive computations from the untrusted software stack . this paper initiates the study of high performance execution of Deep Neural Networks (DNNs) in TEEs by efficiently partitioning DNN computations between trusted devices . however, these isolation guarantees come at a price in performance, compared to nontrusted alternatives .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 165/1992 [10:29<2:04:05,  4.08s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Adversarial training is one of the main defenses against adversarial attacks. In this paper, we provide the first rigorous study on diagnosing elements of large-scale adversarial training on ImageNet, which reveals two intriguing properties. \n\n First, we study the role of normalization. Batch normalization (BN) is a crucial element for achieving state-of-the-art performance on many vision tasks, but we show it may prevent networks from obtaining strong robustness in adversarial training. One unexpected observation is that, for models trained with BN, simply removing clean images from training data largely boosts adversarial robustness, i.e., 18.3%. We relate this phenomenon to the hypothesis that clean images and adversarial images are drawn from two different domains. This two-domain hypothesis may explain the issue of BN when training with a mixture of clean and adversarial images, as estimating normalization statistics of this mixture distribution is challenging. Guided by this two-domain\n[{'summary_text': 'in this paper, we provide the first rigorous study on diagnosing elements of large-scale adversarial training on ImageNet, which reveals two intriguing properties . first, we study the role of normalization. Batch normalization (BN) is a crucial element for achieving state-of-the-art performance on many vision tasks, but we show it may prevent networks from obtaining strong robustness . one unexpected observation is that, for models trained with BN, simply removing clean images from training data largely boosts adversari'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 166/1992 [10:33<2:08:46,  4.23s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep Infomax~(DIM) is an unsupervised representation learning framework by maximizing the mutual information between the inputs and the outputs of an encoder, while probabilistic constraints are imposed on the outputs. In this paper, we propose Supervised Deep InfoMax~(SDIM), which introduces supervised probabilistic constraints to the encoder outputs. The supervised probabilistic constraints are equivalent to a generative classifier on high-level data representations, where class conditional log-likelihoods of samples can be evaluated. Unlike other works building generative classifiers with conditional generative models, SDIMs scale on complex datasets, and can achieve comparable performance with discriminative counterparts.   With SDIM, we could perform \\emph{classification with rejection}.\nInstead of always reporting a class label, SDIM only makes predictions when test samples' largest logits surpass some pre-chosen thresholds, otherwise they will be deemed as out of the data distributions,\n[{'summary_text': \"supervised probabilistic constraints are equivalent to a generative classifier on high-level data representations . with SDIM, we could perform emphclassification with rejection . instead of always reporting a class label, SDIM only makes predictions when test samples' largest logits surpass some pre-chosen thresholds, otherwise they will be deemed as out of the data distributions.\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 167/1992 [10:38<2:08:52,  4.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Natural Language Processing models lack a unified approach to robustness testing. In this paper we introduce WildNLP - a framework for testing model stability in a natural setting where text corruptions such as keyboard errors or misspelling occur. We compare robustness of models from 4 popular NLP tasks: Q&A, NLI, NER and Sentiment Analysis by testing their performance on aspects introduced in the framework. In particular, we focus on a comparison between recent state-of-the- art text representations and non-contextualized word embeddings. In order to improve robust- ness, we perform adversarial training on se- lected aspects and check its transferability to the improvement of models with various cor- ruption types. We find that the high perfor- mance of models does not ensure sufficient robustness, although modern embedding tech- niques help to improve it. We release cor- rupted datasets and code for WildNLP frame- work for the\n[{'summary_text': 'natural language processing models lack a unified approach to robustness testing . in this paper we introduce WildNLP - a framework for testing model stability in a natural setting where text corruptions such as keyboard errors or misspelling occur . we compare robustness of models from 4 popular NLP tasks: Q&A, NLI, NER and Sentiment Analysis by testing their performance on aspects introduced in the framework .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 168/1992 [10:42<2:09:19,  4.25s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Massively multi-label prediction/classification problems arise in environments like health-care or biology where it is useful to make very precise predictions. One challenge with massively multi-label problems is that there is often a long-tailed frequency distribution for the labels, resulting in few positive examples for the rare labels. We propose a solution to this problem by modifying the output layer of a neural network to create a Bayesian network of sigmoids which takes advantage of ontology relationships between the labels to help share information between the rare and the more common labels.   We apply this method to the two massively multi-label tasks of disease prediction (ICD-9 codes) and protein function prediction (Gene Ontology terms) and obtain significant improvements in per-label AUROC and average precision.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'massively multi-label problems arise in environments like health-care or biology where it is useful to make very precise predictions . we propose a solution by modifying the output layer of a neural network to create a Bayesian network of sigmoids which takes advantage of ontology relationships between the labels to help share information between the rare and more common labels .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 169/1992 [10:46<2:05:38,  4.14s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In health, machine learning is increasingly common, yet neural network embedding (representation) learning is arguably under-utilized for physiological signals.   This inadequacy stands out in stark contrast to more traditional computer science domains, such as computer vision (CV), and natural language processing (NLP).   For physiological signals, learning feature embeddings is a natural solution to data insufficiency caused by patient privacy concerns -- rather than share data, researchers may share informative embedding models (i.e., representation models), which map patient data to an output embedding.    Here, we present the PHASE (PHysiologicAl Signal Embeddings) framework, which consists of three components: i) learning neural network embeddings of physiological signals, ii) predicting outcomes based on the learned embedding, and iii) interpreting the prediction results by estimating feature attributions in the \"stacked\" models (i.e., feature embedding model followed by prediction\n[{'summary_text': 'in health, machine learning is increasingly common, yet neural network embedding (representation) learning is arguably under-utilized for physiological signals . this inadequacy stands out in stark contrast to more traditional computer science domains, such as computer vision (CV), and natural language processing (NLP).'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▊         | 170/1992 [10:50<2:03:56,  4.08s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose a new framework for entity and event extraction based on generative adversarial imitation learning -- an inverse reinforcement learning method using generative adversarial network (GAN). We assume that instances and labels yield to various extents of difficulty and the gains and penalties (rewards) are expected to be diverse. We utilize discriminators to estimate proper rewards according to the difference between the labels committed by ground-truth (expert) and the extractor (agent).   Experiments also demonstrate that the proposed framework outperforms state-of-the-art methods.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we propose a new framework for entity and event extraction based on generative adversarial imitation learning . the proposed framework outperforms state-of-the-art methods . we assume instances and labels yield to various extents of difficulty .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▊         | 171/1992 [10:53<1:59:12,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Cloud Migration transforms customer’s data, application and services from original IT platform to one or more cloud en- vironment, with the goal of improving the performance of the IT system while reducing the IT management cost. The enterprise level Cloud Migration projects are generally com- plex, involves dynamically planning and replanning various types of transformations for up to 10k endpoints. Currently the planning and replanning in Cloud Migration are generally done manually or semi-manually with heavy dependency on the migration expert’s domain knowledge, which takes days to even weeks for each round of planning or replanning. As a result, automated planning engine that is capable of gener- ating high quality migration plan in a short time is particu- larly desirable for the migration industry. In this short paper, we briefly introduce the advantages of using AI planning in Cloud Migration, a preliminary prototype, as well as the challenges the requires attention from the planning and scheduling society.<|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the enterprise level Cloud Migration projects are generally com- plex, involves dynamically planning and replanning various types of transformations for up to 10k endpoints . as a result, automated planning engine that is capable of gener-ating high quality migration plan in a short time is particu- larly desirable for the migration industry .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▊         | 172/1992 [10:57<1:57:53,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We consider the dictionary learning problem, where the aim is to model the given data as a linear combination of a few columns of a matrix known as a dictionary, where the sparse weights forming the linear combination are known as coefficients. Since the dictionary and coefficients, parameterizing the linear model are unknown, the corresponding optimization is inherently non-convex. This was a major challenge until recently, when provable algorithms for dictionary learning were proposed. Yet, these provide guarantees only on the recovery of the dictionary, without explicit recovery guarantees on the coefficients. Moreover, any estimation error in the dictionary adversely impacts the ability to successfully localize and estimate the coefficients. This potentially limits the utility of existing provable dictionary learning methods in applications where coefficient recovery is of interest. To this end, we develop NOODL: a simple Neurally plausible alternating Optimization-based Online Dictionary Learning algorithm, which recovers both the dictionary and coefficients exactly at a geometric rate, when initialized appropriately. Our algorithm, NOODL,\n[{'summary_text': 'we consider the dictionary learning problem, where the aim is to model the given data as a linear combination of a few columns . the sparse weights forming the linear combination are known as coefficients . since the dictionary and coefficients, parameterizing the linear model are unknown, the corresponding optimization is inherently non-convex . this potentially limits the utility of existing provable dictionary learning methods in applications where coefficient recovery is of interest .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▊         | 173/1992 [11:01<2:02:22,  4.04s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"GloVe and Skip-gram word embedding methods learn word vectors by decomposing a denoised matrix of word co-occurrences into a product of low-rank matrices. In this work, we propose an iterative algorithm for computing word vectors based on modeling word co-occurrence matrices with Generalized Low Rank Models. Our algorithm generalizes both Skip-gram and GloVe as well as giving rise to other embedding methods based on the specified co-occurrence matrix, distribution of co-occurences, and the number of iterations in the iterative algorithm. For example, using a Tweedie distribution with one iteration results in GloVe and using a Multinomial distribution with full-convergence mode results in Skip-gram. Experimental results demonstrate that multiple iterations of our algorithm improves results over the GloVe method on the Google word analogy similarity task.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we propose an iterative algorithm for computing word vectors based on modeling word co-occurrence matrices with Generalized Low Rank Models . our algorithm generalizes both Skip-gram and GloVe as well as giving rise to other embedding methods . for example, using a Tweedie distribution with one iteration results in a . Multinomial distribution with full-convergence mode results . in skip-gram .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▊         | 174/1992 [11:05<2:02:58,  4.06s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We develop a new algorithm for imitation learning from a single expert demonstration. In contrast to many previous one-shot imitation learning approaches, our algorithm does not assume access to more than one expert demonstration during the training phase. Instead, we leverage an exploration policy to acquire unsupervised trajectories, which are then used to train both an encoder and a context-aware imitation policy. The optimization procedures for the encoder, imitation learner, and exploration policy are all tightly linked. This linking creates a feedback loop wherein the exploration policy collects new demonstrations that challenge the imitation learner, while the encoder attempts to help the imitation policy to the best of its abilities. We evaluate our algorithm on 6 MujoCo robotics tasks.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'our algorithm does not assume access to more than one expert demonstration during the training phase . instead, we leverage an exploration policy to acquire unsupervised trajectories, which are then used to train both an encoder and a context-aware imitation policy . the optimization procedures for the encoder, imitation learner, and exploration policy are all tightly linked .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 175/1992 [11:09<2:00:16,  3.97s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Temporal difference (TD) learning is a popular algorithm for policy evaluation in reinforcement learning, but the vanilla TD can substantially suffer from the inherent optimization variance. A variance reduced TD (VRTD) algorithm was proposed by Korda and La (2015), which applies the variance reduction technique directly to the online TD learning with Markovian samples. In this work, we first point out the technical errors in the analysis of VRTD in Korda and La (2015), and then provide a mathematically solid analysis of the non-asymptotic convergence of VRTD and its variance reduction performance. We show that VRTD is guaranteed to converge to a neighborhood of the fixed-point solution of TD at a linear convergence rate. Furthermore, the variance error (for both i.i.d. and Markovian sampling) and the bias error (for Markovian sampling) of VRTD are significantly reduced by the batch size of variance reduction in comparison to those of vanilla TD.<|endoftext|>\n[{'summary_text': 'variance reduced TD (VRTD) learning is a popular algorithm for policy evaluation in reinforcement learning . but the vanilla TD can substantially suffer from the inherent optimization variance . we first point out technical errors in the analysis of VRTD in Korda and La (2015) .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 176/1992 [11:13<1:56:54,  3.86s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Holistically exploring the perceptual and neural representations underlying animal communication has traditionally been very difficult because of the complexity of the underlying signal. We present here a novel set of techniques to project entire communicative repertoires into low dimensional spaces that can be systematically sampled from, exploring the relationship between perceptual representations, neural representations, and the latent representational spaces learned by machine learning algorithms. We showcase this method in one ongoing experiment studying sequential and temporal maintenance of context in songbird neural and perceptual representations of syllables. We further discuss how studying the neural mechanisms underlying the maintenance of the long-range information content present in birdsong can inform and be informed by machine sequence modeling.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we present here a novel set of techniques to project entire communicative repertoires into low dimensional spaces that can be systematically sampled from . we showcase this method in one ongoing experiment studying sequential and temporal maintenance of context in songbird neural .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 177/1992 [11:16<1:53:25,  3.75s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Distributed optimization is vital in solving large-scale machine learning problems. A widely-shared feature of distributed optimization techniques is the requirement that all nodes complete their assigned tasks in each computational epoch before the system can proceed to the next epoch. In such settings, slow nodes, called stragglers, can greatly slow progress. To mitigate the impact of stragglers, we propose an online distributed optimization method called Anytime Minibatch. In this approach, all nodes are given a fixed time to compute the gradients of as many data samples as possible. The result is a variable per-node minibatch size. Workers then get a fixed communication time to average their minibatch gradients via several rounds of consensus, which are then used to update primal variables via dual averaging. Anytime Minibatch prevents stragglers from holding up the system without wasting the work that stragglers can complete. We present a convergence analysis and analyze the wall time performance. Our numerical results show that our\n[{'summary_text': 'distributed optimization is vital in solving large-scale machine learning problems . in such settings, slow nodes, called stragglers, can greatly slow progress . we propose an online distributed optimization method called Anytime Minibatch .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 178/1992 [11:20<1:53:14,  3.75s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Multi-relational graph embedding which aims at achieving effective representations with reduced low-dimensional parameters, has been widely used in knowledge base completion. Although knowledge base data usually contains tree-like or cyclic structure, none of existing approaches can embed these data into a compatible space that in line with the structure. To overcome this problem, a novel framework, called Riemannian TransE, is proposed in this paper to embed the entities in a Riemannian manifold. Riemannian TransE models each relation as a move to a point and defines specific novel distance dissimilarity for each relation, so that all the relations are naturally embedded in correspondence to the structure of data. Experiments on several knowledge base completion tasks have shown that, based on an appropriate choice of manifold, Riemannian TransE achieves good performance even with a significantly reduced parameters.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'multi-relational graph embedding has been widely used in knowledge base completion . Riemannian TransE models each relation as a move to a point . no existing approaches can embed these data into a compatible space .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 179/1992 [11:24<1:50:22,  3.65s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Minecraft is a videogame that offers many interesting challenges for AI systems. In this paper, we focus in construction scenarios where an agent must build a complex structure made of individual blocks. As higher-level objects are formed of lower-level objects, the construction can naturally be modelled as a hierarchical task network. We model a house-construction scenario in classical and HTN planning and compare the advantages and disadvantages of both kinds of models.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this paper, we focus in construction scenarios where an agent must build a complex structure made of individual blocks . as higher-level objects are formed, the construction can naturally be modelled as a hierarchical task network .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 180/1992 [11:27<1:47:11,  3.55s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Statistical inference methods are fundamentally important in machine learning. Most state-of-the-art inference algorithms are \n variants of Markov chain Monte Carlo (MCMC) or variational inference (VI). However, both methods struggle with limitations in practice: MCMC methods can be computationally demanding; VI methods may have large bias. \n In this work, we aim to improve upon MCMC and VI by a novel hybrid method based on the idea of reducing simulation bias of finite-length MCMC chains using gradient-based optimisation. The proposed method can generate low-biased samples by increasing the length of MCMC simulation and optimising the MCMC hyper-parameters, which offers attractive balance between approximation bias and computational efficiency. We show that our method produces promising results on popular benchmarks when compared to recent hybrid methods of MCMC and VI.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'most state-of-the-art inference algorithms are variants of markov chain Monte Carlo (MCMC) or variational inference (VI). however, both methods struggle with limitations in practice: MCMC methods can be computationally demanding; VI methods may have large bias .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 181/1992 [11:30<1:47:09,  3.55s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Despite alarm over the reliance of machine learning systems on so-called spurious patterns in training data, the term lacks coherent meaning in standard statistical frameworks. However, the language of causality offers clarity: spurious associations are those due to a common cause (confounding) vs direct or indirect effects. In this paper, we focus on NLP, introducing methods and resources for training models insensitive to spurious patterns. Given documents and their initial labels, we task humans with revise each document to accord with a counterfactual target label, asking that the revised documents be internally coherent while avoiding any gratuitous changes. Interestingly, on sentiment analysis and natural language inference tasks, classifiers trained on original data fail on their counterfactually-revised counterparts and vice versa. Classifiers trained on combined datasets perform remarkably well, just shy of those specialized to either domain. While classifiers trained on either original or manipulated data alone  are sensitive to spurious features (e.g., mentions of genre), models trained on the\n[{'summary_text': 'despite alarm over the reliance of machine learning systems on spurious patterns in training data, the term lacks coherent meaning in standard statistical frameworks . however, the language of causality offers clarity: spurious associations are those due to a common cause (confounding) vs direct or indirect effects .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 182/1992 [11:34<1:50:59,  3.68s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned\n[{'summary_text': 'constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned . in many cases it decreases the number of parameter updates required to achieve low training error . however, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 183/1992 [11:39<1:55:11,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Hashing-based collaborative filtering learns binary vector representations (hash codes) of users and items, such that recommendations can be computed very efficiently using the Hamming distance, which is simply the sum of differing bits between two hash codes. A problem with hashing-based collaborative filtering using the Hamming distance, is that each bit is equally weighted in the distance computation, but in practice some bits might encode more important properties than other bits, where the importance depends on the user. \n To this end, we propose an end-to-end trainable variational hashing-based collaborative filtering approach that uses the novel concept of self-masking: the user hash code acts as a mask on the items (using the Boolean AND operation), such that it learns to encode which bits are important to the user, rather than the user's preference towards the underlying item property that the bits represent. This allows a binary user-level importance weighting of each item without the need to store additional weights for each user\n[{'summary_text': 'hashing-based collaborative filtering learns binary vector representations (hash codes) of users and items, such that recommendations can be computed very efficiently using the Hamming distance . each bit is equally weighted in the distance computation, but in practice some bits might encode more important properties than other bits, where the importance depends on the user .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 184/1992 [11:43<2:00:18,  3.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present a tool for Interactive Visual Exploration of Latent Space (IVELS) for model selection.   Evaluating generative models of discrete sequences from a  continuous  latent  space  is  a  challenging  problem,  since  their  optimization involves multiple competing objective terms.   We introduce a model-selection pipeline  to  compare  and  filter  models  throughout  consecutive  stages  of  more complex and expensive metrics. We present the pipeline in an interactive visual tool to enable the exploration of the metrics, analysis of the learned latent space, and selection of the best model for a given task.   We focus specifically on the variational auto-encoder family in a case study of modeling peptide sequences, which are short sequences of amino acids. This task is especially interesting due to the presence of multiple attributes we want to model. We demonstrate how an interactive visual comparison can assist in evaluating how well an unsupervised auto-encoder meaningfully\n[{'summary_text': 'generative models of discrete sequences from a continuous latent space are a challenging problem, since their optimization involves multiple competing objective terms . we introduce a model-selection pipeline to compare and filter models throughout consecutive stages of more complex and expensive metrics . this task is especially interesting due to the presence of multiple attributes we want to model .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 185/1992 [11:47<2:01:02,  4.02s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Dynamical system models (including RNNs) often lack the ability to adapt the sequence generation or prediction to a given context, limiting their real-world application. In this paper we show that hierarchical multi-task dynamical systems (MTDSs) provide direct user control over sequence generation, via use of a latent  code z that specifies the customization to the\n individual data sequence. This enables style transfer, interpolation and morphing within generated sequences. We show the MTDS can improve predictions via latent code interpolation, and avoid the long-term performance degradation of standard RNN approaches.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'hierarchical multi-task dynamical systems (MTDSs) provide direct user control over sequence generation via use of a latent code z that specifies the customization to the individual data sequence . this enables style transfer, interpolation and morphing within generated sequences .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 186/1992 [11:51<1:58:14,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The ever-increasing size of modern datasets combined with the difficulty of obtaining label information has made semi-supervised learning of significant practical importance in modern machine learning applications. In comparison to supervised learning, the key difficulty in semi-supervised learning is how to make full use of the unlabeled data. In order to utilize manifold information provided by unlabeled data, we propose a novel regularization called the tangent-normal adversarial regularization, which is composed by two parts. The two parts complement with each other and jointly enforce the smoothness along two different directions that are crucial for semi-supervised learning. One is applied along the tangent space of the data manifold, aiming to enforce local invariance of the classifier on the manifold, while the other is performed on the normal space orthogonal to the tangent space, intending to impose robustness on the classifier against the noise causing the observed data deviating from the underlying data manifold.   Both of the two\n[{'summary_text': 'in comparison to supervised learning, the key difficulty in semi-supervised learning is how to make full use of the unlabeled data . in order to utilize manifold information, we propose a novel regularization called the tangent-normal adversarial regularization . the two parts complement with each other and jointly enforce the smoothness along two different directions .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 187/1992 [11:55<2:00:18,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Fine-grained Entity Recognition (FgER) is the task of detecting and classifying entity mentions to a large set of types spanning diverse domains such as biomedical, finance and sports.    We  observe  that  when  the  type  set  spans  several  domains,  detection  of  entity mention becomes a limitation for supervised learning models.   The primary reason being lack  of  dataset  where  entity  boundaries  are  properly  annotated  while  covering  a  large spectrum of entity types.   Our work directly addresses this issue.   We propose Heuristics Allied with Distant Supervision (HAnDS) framework to automatically construct a quality dataset suitable for the FgER task.   HAnDS framework exploits the high interlink among Wikipedia  and  Freebase  in  a  pipelined  manner,  reducing  annotation  errors  introduced by naively using distant supervision approach\n[{'summary_text': 'fine-grained Entity Recognition (FgER) is the task of detecting and classifying entity mentions to a large set of types spanning diverse domains such as biomedical, finance and sports . when the type set spans several domains, detection of entity mention becomes a limitation for supervised learning models . HAnDS framework exploits the high interlink among Wikipedia and Freebase in a pipelined manner .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 188/1992 [11:59<2:03:09,  4.10s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep neural networks (DNNs) perform well on a variety of tasks despite the fact that most used in practice are vastly overparametrized and even capable of perfectly fitting randomly labeled data. Recent evidence suggests that developing \"compressible\" representations is key for adjusting the complexity of overparametrized networks to the task at hand and avoiding overfitting (Arora et al., 2018; Zhou et al., 2018). In this paper, we provide new empirical evidence that supports this hypothesis, identifying two independent mechanisms that emerge when the network’s width is increased: robustness (having units that can be removed without affecting accuracy) and redundancy (having units with similar activity). In a series of experiments with AlexNet, ResNet and Inception networks in the CIFAR-10 and ImageNet datasets, and also using shallow networks with synthetic data, we show that DNNs consistently increase either their robustness, their redundancy, or both at greater widths for a\n[{'summary_text': 'recent evidence suggests developing \"compressible\" representations is key for adjusting complexity of overparametrized networks to task at hand and avoiding overfitting . in a series of experiments with AlexNet, ResNet and Inception networks, we show that DNNs consistently increase either their robustness, their redundancy, or both at greater widths for a . aDeep neural networks (DNNs) perform well on a variety of tasks despite the fact that most used in practice are'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 189/1992 [12:04<2:07:29,  4.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep learning training accesses vast amounts of data at high velocity, posing challenges for datasets retrieved over commodity networks and storage devices. We introduce a way to dynamically reduce the overhead of fetching and transporting training data with a method we term Progressive Compressed Records (PCRs). PCRs deviate from previous formats by leveraging progressive compression to split each training example into multiple examples of increasingly higher fidelity, without adding to the total data size. Training examples of similar fidelity are grouped together, which reduces both the system overhead and data bandwidth needed to train a model. We show that models can be trained on aggressively compressed representations of the training data and still retain high accuracy, and that PCRs can enable a 2x speedup on average over baseline formats using JPEG compression. Our results hold across deep learning architectures for a wide range of datasets: ImageNet, HAM10000, Stanford Cars, and CelebA-HQ.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we introduce a way to dynamically reduce the overhead of fetching and transporting training data . PCRs deviate from previous formats by leveraging progressive compression to split each training example into multiple examples of increasingly higher fidelity . training examples of similar fidelity are grouped together, which reduces both the system overhead and data bandwidth needed to train a model .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|▉         | 190/1992 [12:07<2:02:47,  4.09s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Click Through Rate (CTR) prediction is a critical task in industrial applications, especially for online social and commerce applications. It is challenging to find a proper way to automatically discover the effective cross features in CTR tasks. We propose a novel model for CTR tasks, called Deep neural networks with Encoder enhanced Factorization Machine (DeepEnFM). Instead of learning the cross features directly, DeepEnFM adopts the Transformer encoder as a backbone to align the feature embeddings with the clues of other fields. The embeddings generated from encoder are beneficial for the further feature interactions. Particularly, DeepEnFM utilizes a bilinear approach to generate different similarity functions with respect to different field pairs. Furthermore, the max-pooling method makes DeepEnFM feasible to capture both the supplementary and suppressing information among different attention heads. Our model is validated on the Criteo and Avazu datasets, and achieves state-of-art performance.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we propose a novel model for CTR tasks, called Deep neural networks with Encoder enhanced Factorization Machine (DeepEnFM) instead of learning the cross features directly, DeepEnFM adopts the Transformer encoder as a backbone to align the feature embeddings with the clues of other fields .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|▉         | 191/1992 [12:11<2:00:52,  4.03s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Modern neural networks are over-parametrized. In particular, each rectified linear hidden unit can be modified by a multiplicative factor by adjusting input and out- put weights, without changing the rest of the network. Inspired by the Sinkhorn-Knopp algorithm, we introduce a fast iterative method for minimizing the l2 norm of the weights, equivalently the weight decay regularizer. It provably converges to a unique solution. Interleaving our algorithm with SGD during training improves the test accuracy. For small batches, our approach offers an alternative to batch- and group- normalization on CIFAR-10 and ImageNet with a ResNet-18.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'each rectified linear hidden unit can be modified by a multiplicative factor by adjusting input and out- put weights, without changing the rest of the network . a fast iterative method for minimizing the l2 norm of the weights . it provably converges to a unique solution .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|▉         | 192/1992 [12:15<1:57:47,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software, websites, and mobile applications. In this paper, we show that deep learning methods can be leveraged to train a model end-to-end to automatically generate code from a single input image with over 77% of accuracy for three different platforms (i.e. iOS, Android and web-based technologies).<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this paper, we show that deep learning methods can be leveraged to train a model end-to-end to automatically generate code from a single input image with over 77% of accuracy for three different platforms . a designer created a screenshot into computer code is a typical task conducted by a developer in order to build customized software, websites, and mobile applications .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|▉         | 193/1992 [12:19<1:56:23,  3.88s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Identifying analogies across domains without supervision is a key task for artificial intelligence. Recent advances in cross domain image mapping have concentrated on translating images across domains. Although the progress made is impressive, the visual fidelity many times does not suffice for identifying the matching sample from the other domain. In this paper, we tackle this very task of finding exact analogies between datasets i.e. for every image from domain A find an analogous image in domain B. We present a matching-by-synthesis approach: AN-GAN, and show that it outperforms current techniques. We further show that the cross-domain mapping task can be broken into two parts: domain alignment and learning the mapping function. The tasks can be iteratively solved, and as the alignment is improved, the unsupervised translation function reaches quality comparable to full supervision.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'recent advances in cross domain image mapping have concentrated on translating images across domains . the progress made is impressive, but the visual fidelity many times does not suffice for identifying the matching sample from the other domain .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|▉         | 194/1992 [12:22<1:51:53,  3.73s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Representation learning is one of the foundations of Deep Learning and allowed important improvements on several Machine Learning tasks, such as Neural Machine Translation, Question Answering and Speech Recognition. Recent works have proposed new methods for learning representations for nodes and edges in graphs. Several of these methods are based on the SkipGram algorithm, and they usually process a large number of multi-hop neighbors in order to produce the context from which node representations are learned. In this paper, we propose an effective and also efficient method for generating node embeddings in graphs that employs a restricted number of permutations over the immediate neighborhood of a node as context to generate its representation, thus ego-centric representations. We present a thorough evaluation showing that our method outperforms state-of-the-art methods in six different datasets related to the problems of link prediction and node classification, being one to three orders of magnitude faster than baselines when generating node embeddings for very large graphs.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'recent works have proposed new methods for learning representations for nodes and edges in graphs . several of these methods are based on the SkipGram algorithm . they usually process a large number of multi-hop neighbors in order to produce the context from which node representations are learned .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|▉         | 195/1992 [12:26<1:54:09,  3.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose\n[{'summary_text': 'this paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster . we begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). this similarity is category-agnostic and can be learned from data in the source domain using a similarity network .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|▉         | 196/1992 [12:31<1:58:43,  3.97s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper, a deep boosting algorithm is developed to\n learn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs (base experts)\n with diverse capabilities, e.g., these base deep CNNs are\n sequentially trained to recognize a set of \n object classes in an easy-to-hard way according to their\n learning complexities. Our experimental results have demonstrated\n that our deep boosting algorithm can significantly improve the\n accuracy rates on large-scale visual recognition.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'a deep boosting algorithm is developed to learn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs (base experts) with diverse capabilities . our experimental results have demonstrated that our deep-boosting algorithm can significantly improve the accuracy rates on large-scale visual recognition .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|▉         | 197/1992 [12:34<1:55:10,  3.85s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Multivariate time series with missing values are common in areas such as healthcare and finance, and have grown in number and complexity over the years. This raises the question whether deep learning methodologies can outperform classical data imputation methods in this domain. However, naive applications of deep learning fall short in giving reliable confidence estimates and lack interpretability. We propose a new deep sequential latent variable model for dimensionality reduction and data imputation. Our modeling assumption is simple and interpretable: the high dimensional time series has a lower-dimensional representation which evolves smoothly in time according to a Gaussian process. The non-linear dimensionality reduction in the presence of missing data is achieved using a VAE approach with a novel structured variational approximation. We demonstrate that our approach outperforms several classical and deep learning-based data imputation methods on high-dimensional data from the domains of computer vision and healthcare, while additionally improving the smoothness of the imputations and providing interpretable uncertainty estimates.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'multivariate time series with missing values are common in areas such as healthcare and finance, and have grown in number and complexity over the years . this raises the question whether deep learning methodologies can outperform classical data imputation methods in this domain . however, naive applications of deep learning fall short in giving reliable confidence estimates and lack interpretability .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|▉         | 198/1992 [12:38<1:54:30,  3.83s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The key attribute that drives the unprecedented success of modern Recurrent Neural Networks (RNNs) on learning tasks which involve sequential data, is their ever-improving ability to model intricate long-term temporal dependencies. However, a well established measure of RNNs' long-term memory capacity is lacking, and thus formal understanding of their ability to correlate data throughout time is limited. Though depth efficiency in convolutional networks is well established by now, it does not suffice in order to account for the success of deep RNNs on inputs of varying lengths, and the need to address their 'time-series expressive power' arises. In this paper, we analyze the effect of depth on the ability of recurrent networks to express correlations ranging over long time-scales. To meet the above need, we introduce a measure of the information flow across time that can be supported by the network, referred to as the Start-End separation rank. Essentially, this measure reflects the distance of the function realized\n[{'summary_text': \"the key attribute that drives the unprecedented success of modern Recurrent Neural Networks (RNNs) on learning tasks which involve sequential data is their ever-improving ability to model intricate long-term temporal dependencies . however, a well established measure of RNNs' long -term memory capacity is lacking, and thus formal understanding of their ability to correlate data throughout time is limited . in this paper, we analyze the effect of depth on the ability of recurrent networks to express correlations ranging over long time-scale\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|▉         | 199/1992 [12:42<2:00:52,  4.04s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Stochastic gradient descent (SGD) with stochastic momentum is popular in nonconvex stochastic optimization and particularly for the training of deep neural networks. In standard SGD, parameters are updated by improving along the path of the gradient at the current iterate on a batch of examples, where the addition of a ``momentum'' term biases the update in the direction of the previous change in parameters. In non-stochastic convex optimization one can show that a momentum adjustment provably reduces convergence time in many settings, yet such results have been elusive in the stochastic and non-convex settings. At the same time, a widely-observed empirical phenomenon is that in training deep networks stochastic momentum appears to significantly improve convergence time, variants of it have flourished in the development of other popular update methods, e.g. ADAM, AMSGrad, etc. Yet theoretical justification for the use of stochastic momentum has remained a\n[{'summary_text': \"in standard SGD, parameters are updated by improving along the path of the gradient at the current iterate on a batch of examples . the addition of a momentum'' term biases the update in the direction of the previous change in parameters . in non-stochastic convex optimization one can show that a momentum adjustment provably reduces convergence time, variants of it have flourished in the development of other popular update methods .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|█         | 200/1992 [12:47<2:04:08,  4.16s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The use of imitation learning to learn a single policy for a complex task that has multiple modes or hierarchical structure can be challenging. In fact, previous work has shown that when the modes are known, learning separate policies for each mode or sub-task can greatly improve the performance of imitation learning. In this work, we discover the interaction between sub-tasks from their resulting state-action trajectory sequences using a directed graphical model. We propose a new algorithm based on the generative adversarial imitation learning framework which automatically learns sub-task policies from unsegmented demonstrations. Our approach maximizes the directed information flow in the graphical model between sub-task latent variables and their generated trajectories. We also show how our approach connects with the existing Options framework, which is commonly used to learn hierarchical policies.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the use of imitation learning to learn a single policy for a complex task that has multiple modes or hierarchical structure can be challenging . in this work, we discover the interaction between sub-tasks from their resulting state-action trajectory sequences using a directed graphical model . we propose a new algorithm based on the generative adversarial imitation learning framework .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|█         | 201/1992 [12:51<2:01:38,  4.07s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.\n[{'summary_text': 'many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs) this problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features . in experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|█         | 202/1992 [12:55<2:03:49,  4.15s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In spite of their great success, traditional factorization algorithms typically do not support features (e.g., Matrix Factorization), or their complexity scales quadratically with the number of features (e.g, Factorization Machine). On the other hand, neural methods allow large feature sets, but are often designed for a specific application. We propose novel deep factorization methods that allow efficient and flexible feature representation. For example, we enable describing items with natural language with complexity linear to the vocabulary size—this enables prediction for unseen items and avoids the cold start problem. We show that our architecture can generalize some previously published single-purpose neural architectures. Our experiments suggest improved training times and accuracy compared to shallow methods.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'traditional factorization algorithms typically do not support features (e.g., Matrix Factorization), or their complexity scales quadratically with the number of features . neural methods allow large feature sets, but are often designed for a specific application .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|█         | 203/1992 [12:59<1:57:58,  3.96s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"State-of-the-art performances on language comprehension tasks are achieved by huge language models pre-trained on massive unlabeled text corpora, with very light subsequent fine-tuning in a task-specific supervised manner. It seems the pre-training procedure learns a very good common initialization for further training on various natural language understanding tasks, such that only few steps need to be taken in the parameter space to learn each task. In this work, using Bidirectional Encoder Representations from Transformers (BERT) as an example, we verify this hypothesis by showing that task-specific fine-tuned language models are highly close in parameter space to the pre-trained one. Taking advantage of such observations, we further show that the fine-tuned versions of these huge models, having on the order of $10^8$ floating-point parameters, can be made very computationally efficient. First, fine-tuning only a fraction of critical layers suffices. Second, fine\n[{'summary_text': 'fineState-of-the-art performances on language comprehension tasks are achieved by huge language models pre-trained on unlabeled text corpora, with very light subsequent fine-tuning in a task-specific supervised manner . it seems the pre-training procedure learns a very good common initialization for further training on various natural language understanding tasks .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|█         | 204/1992 [13:03<1:59:38,  4.01s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Plan recognition aims to look for target plans to best explain the observed actions based on plan libraries and/or domain models. Despite the success of previous approaches on plan recognition, they mostly rely on correct action observations. \n Recent advances in visual activity recognition have the potential of enabling applications such as automated video surveillance. Effective approaches for such problems would require the ability to recognize the plans of agents from video information. Traditional plan recognition algorithms rely on access to detailed planning domain models. One recent promising direction involves learning approximate (or shallow) domain models directly from the observed activity sequences. Such plan recognition approaches expect observed action sequences as inputs. However, visual inference results are often noisy and uncertain, typically represented as a distribution over possible actions. In this work, we develop a visual plan recognition framework that recognizes plans with an approximate domain model learned from uncertain visual data.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'plan recognition aims to look for target plans to best explain the observed actions based on plan libraries and/or domain models . recent advances in visual activity recognition have the potential of enabling applications such as automated video surveillance . effective approaches for such problems would require the ability to recognize the plans of agents from video information.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|█         | 205/1992 [13:06<1:55:50,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Classification systems typically act in isolation, meaning they are required to implicitly memorize the characteristics of all candidate classes in order to classify. The cost of this is increased memory usage and poor sample efficiency. We propose a model which instead verifies using reference images during the classification process, reducing the burden of memorization. The model uses iterative non-differentiable queries in order to classify an image. We demonstrate that such a model is feasible to train and can match baseline accuracy while being more parameter efficient. However, we show that finding the correct balance between image recognition and verification is essential to pushing the model towards desired behavior, suggesting that a pipeline of recognition followed by verification is a more promising approach towards designing more powerful networks with simpler architectures.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'classification systems act in isolation, meaning they are required to implicitly memorize the characteristics of all candidate classes in order to classify . the cost of this is increased memory usage and poor sample efficiency . we propose a model which instead verifies using reference images during the classification process .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|█         | 206/1992 [13:10<1:52:38,  3.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the\n[{'summary_text': 'non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models . knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|█         | 207/1992 [13:14<1:55:26,  3.88s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Neural population responses to sensory stimuli can exhibit both nonlinear stimulus- dependence and richly structured shared variability. Here, we show how adversarial training can be used to optimize neural encoding models to capture both the deterministic and stochastic components of neural population data. To account for the discrete nature of neural spike trains, we use the REBAR method to estimate unbiased gradients for adversarial optimization of neural encoding models. We illustrate our approach on population recordings from primary visual cortex. We show that adding latent noise-sources to a convolutional neural network yields a model which captures both the stimulus-dependence and noise correlations of the population activity.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we illustrate our approach on population recordings from primary visual cortex . we show that adding latent noise-sources to a convolutional neural network yields a model which captures both the stimulus-dependence and noise correlations of the population activity .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|█         | 208/1992 [13:17<1:51:47,  3.76s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before.   We use simple feed-forward encoder and decoder networks, thus our model is an attractive candidate for applications where the encoding and decoding speed is critical. Additionally, this  allows us to only sample autoregressively in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of  VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known\n[{'summary_text': 'to this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before . we use simple feed-forward encoder and decoder networks, thus our model is an attractive candidate for applications where the encoding and decoding speed is critical . also, this allows us to only sample autoegressively in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|█         | 209/1992 [13:22<1:59:16,  4.01s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Knowledge graphs are structured representations of real world facts. However, they typically contain only a small subset of all possible facts. Link prediction is the task of inferring missing facts based on existing ones. We propose TuckER, a relatively simple yet powerful linear model based on Tucker decomposition of the binary tensor representation of knowledge graph triples. By using this particular decomposition, parameters are shared between relations, enabling multi-task learning. TuckER outperforms previous state-of-the-art models across several standard link prediction datasets.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we propose TuckER, a relatively simple yet powerful linear model based on Tucker decomposition of the binary tensor representation of knowledge graph triples . by using this particular dedecomposition, parameters are shared between relations, enabling multi-task learning . tuckER outperforms previous state-of-the-art models across several standard link prediction datasets.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 210/1992 [13:26<1:57:17,  3.95s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We outline the problem of concept drifts for time series data. In this work, we analyze the temporal inconsistency of streaming wireless signals in the context of device-free passive indoor localization. We show that data obtained from WiFi channel state information (CSI) can be used to train a robust system capable of performing room level localization. One of the most challenging issues for such a system is the movement of input data distribution to an unexplored space over time, which leads to an unwanted shift in the learned boundaries of the output space. In this work, we propose a phase and magnitude augmented feature space along with a standardization technique that is little affected by drifts. We show that this robust representation of the data yields better learning accuracy and requires less number of retraining.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we show that data obtained from WiFi channel state information can be used to train a robust system capable of performing room level localization . one of the most challenging issues for such a system is the movement of input data distribution to an unexplored space over time .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 211/1992 [13:29<1:53:28,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Despite a growing literature on explaining neural networks, no consensus has been reached on how to explain a neural network decision or how to evaluate an explanation.\n\t Our contributions in this paper are twofold. First, we investigate schemes to combine explanation methods and reduce model uncertainty to obtain a single aggregated explanation. The aggregation is more robust and aligns better with the neural network than any single explanation method..\n\t Second, we propose a new approach to evaluating explanation methods that circumvents the need for manual evaluation and is not reliant on the alignment of neural networks and humans decision processes.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'despite a growing literature on explaining neural networks, no consensus has been reached on how to explain a neural network decision or how to evaluate an explanation . our contributions in this paper are twofold: first, we investigate schemes to combine explanation methods and reduce model uncertainty . the aggregation is more robust and aligns better with the neural network than any single explanation method .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 212/1992 [13:33<1:52:27,  3.79s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"This paper studies the undesired phenomena of over-sensitivity of representations learned by deep networks to semantically-irrelevant changes in data. We identify a cause for this shortcoming in the classical Variational Auto-encoder (VAE) objective, the evidence lower bound (ELBO). We show that the ELBO fails to control the behaviour of the encoder out of the support of the empirical data distribution and this behaviour of the VAE can lead to extreme errors in the learned representation. This is a key hurdle in the effective use of representations for data-efficient learning and transfer. To address this problem, we propose to augment the data with specifications that enforce insensitivity of the representation with respect to families of transformations. To incorporate these specifications, we propose a regularization method that is based on a selection mechanism that creates a fictive data point by explicitly perturbing an observed true data point. For certain choices of parameters, our formulation naturally leads to the minimization of the entropy regular\n[{'summary_text': 'this paper studies the undesired phenomena of over-sensitivity of representations learned by deep networks to semantically-irrelevant changes in data . we identify a cause for this shortcoming in the classical Variational Auto-encoder (VAE) objective, the evidence lower bound (ELBO) we show that the ELBO fails to control the behaviour of the encoder out of the support of the empirical data distribution . this behavior of the VAE can lead to extreme errors in the learned representation .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 213/1992 [13:38<1:59:03,  4.02s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Learning theory tells us that more data is better when minimizing the generalization error of identically distributed training and test sets. However, when training and test distribution differ, this distribution shift can have a significant effect. With a novel perspective on function transfer learning, we are able to lower bound the change of performance when transferring from training to test set with the Wasserstein distance between the embedded training and test set distribution. We find that there is a trade-off affecting performance between how invariant a function is to changes in training and test distribution and how large this shift in distribution is. Empirically across several data domains, we substantiate this viewpoint by showing that test performance correlates strongly with the distance in data distributions between training and test set. Complementary to the popular belief that more data is always better, our results highlight the utility of also choosing a training data distribution that is close to the test data distribution when the learned function is not invariant to such changes.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'learning theory tells us that more data is better when minimizing the generalization error of identically distributed training and test sets . however, when training and testing distribution differ, this distribution shift can have a significant effect . we find that there is a trade-off affecting performance between how invariant a function is to changes .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 214/1992 [13:41<1:56:26,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'implication of this is that a filter may know what it is looking at, but not where it is positioned in the image . information concerning absolute position is inherently useful, and it is reasonable to assume deep CNNs may implicitly learn to encode this information if there is a means to do so .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 215/1992 [13:45<1:53:20,  3.83s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Natural Language Inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such architecture, Densely Interactive Inference Network (DIIN), demonstrates the state-of-the-art performance on large scale NLI copora and large-scale NLI alike corpus. It's noteworthy that DIIN achieve a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'natural language inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and an artificial language hypothesis . interactive inference network (IIN) is a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 216/1992 [13:49<1:52:22,  3.80s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recent pretrained sentence encoders achieve state of the art results on language understanding tasks, but does this mean they have implicit knowledge of syntactic structures? We introduce a grammatically annotated development set for the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018), which we use to investigate the grammatical knowledge of three pretrained encoders, including the popular OpenAI Transformer (Radford et al., 2018) and BERT (Devlin et al., 2018). We fine-tune these encoders to do acceptability classification over CoLA and compare the models’ performance on the annotated analysis set. Some phenomena, e.g. modification by adjuncts, are easy to learn for all models, while others, e.g. long-distance movement, are learned effectively only by models with strong overall performance, and others still, e.g. morphological agreement, are hardly learned by any model.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'pretrained encoders achieve state of the art results on language understanding tasks . does this mean they have implicit knowledge of syntactic structures? we introduce a grammatically annotated development set for the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018)'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 217/1992 [13:52<1:51:57,  3.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We investigate methods to efficiently learn diverse strategies in reinforcement learning for a generative structured prediction problem: query reformulation. In the proposed framework an agent consists of multiple specialized sub-agents and a meta-agent that learns to aggregate the answers from sub-agents to produce a final answer. Sub-agents are trained on disjoint partitions of the training data, while the meta-agent is trained on the full training set. Our method makes learning faster, because it is highly parallelizable, and has better generalization performance than strong baselines, such as\n an ensemble of agents trained on the full data. We evaluate on the tasks of document retrieval and question answering. The\n improved performance seems due to the increased diversity of reformulation strategies. This suggests that multi-agent, hierarchical approaches might play an important role in structured prediction tasks of this kind. However, we also find that it is not obvious how to characterize diversity in this context, and a first attempt based on clustering did not produce\n[{'summary_text': 'agents are trained on disjoint partitions of training data, while the meta-agent is trained on the full training set . our method makes learning faster, because it is highly parallelizable, and has better generalization performance than strong baselines .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 218/1992 [13:56<1:53:34,  3.84s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly ﬁnd the optimal network parameters and sparse network structure in a uniﬁed optimization process with trainable pruning thresholds. These thresholds can have ﬁne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and efﬁciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures . these thresholds can have fine-grained layer-wise adjustments dynamically via backpropagation . we have several surprising observations that provide strong evidence to the effectiveness and efficiency of our algorithm .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 219/1992 [14:00<1:51:35,  3.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The reparameterization trick has become one of the most useful tools in the field of variational inference. However, the reparameterization trick is based on the standardization transformation which restricts the scope of application of this method to distributions that have tractable inverse cumulative distribution functions or are expressible as deterministic transformations of such distributions. In this paper, we generalized the reparameterization trick by allowing a general transformation. We discover that the proposed model is a special case of control variate indicating that the proposed model can combine the advantages of CV and generalized reparameterization. Based on the proposed gradient model, we propose a new polynomial-based gradient estimator which has better theoretical performance than the reparameterization trick under certain condition and can be applied to a larger class of variational distributions. In studies of synthetic and real data, we show that our proposed gradient estimator has a significantly lower gradient variance than other state-of-the-art methods thus enabling a\n[{'summary_text': 'the reparameterization trick is based on the standardization transformation which restricts the scope of application of this method to distributions that have tractable inverse cumulative distribution functions . in studies of synthetic and real data, we show that our proposed gradient estimator has a significantly lower gradient variance than other state-of-the-art methods thus enabling a .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 220/1992 [14:04<1:54:48,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at https://attackingrl.github.io.<|endoftext|><|endoftext|>\n[{'summary_text': \"an attacker is not usually able to directly modify another agent's observations . this might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 221/1992 [14:08<1:51:30,  3.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora . there have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 222/1992 [14:11<1:49:10,  3.70s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The capability of reliably detecting out-of-distribution samples is one of the key factors in deploying a good classifier, as the test distribution always does not match with the training distribution in most real-world applications. In this work, we propose a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks. Unlike the discriminative (or softmax) classifier that only focuses on the decision boundary partitioning its latent space into multiple regions, our generative classifier aims to explicitly model class-conditional distributions as separable Gaussian distributions. Thereby, we can define the confidence score by the distance between a test sample and the center of each distribution. Our empirical evaluation on multi-class images and tabular data demonstrate that the generative classifier achieves the best performances in distinguishing out-of-distribution samples, and also\n[{'summary_text': 'the test distribution always does not match with the training distribution in most real-world applications . in this work, we propose a deep generative classifier which is effective to detect out-of-distribution samples . we can define the confidence score by the distance between a test sample and the center of each distribution . the generative classification aims to explicitly model class-conditional distributions as separable Gaussian distributions .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 223/1992 [14:16<1:54:39,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose RaPP, a new methodology for novelty detection by utilizing hidden space activation values obtained from a deep autoencoder.\n Precisely, RaPP compares input and its autoencoder reconstruction not only in the input space but also in the hidden spaces.\n We show that if we feed a reconstructed input to the same autoencoder again, its activated values in a hidden space are equivalent to the corresponding reconstruction in that hidden space given the original input.\n In order to aggregate the hidden space activation values, we propose two metrics, which enhance the novelty detection performance.\n Through extensive experiments using diverse datasets, we validate that RaPP improves novelty detection performances of autoencoder-based approaches.\n Besides, we show that RaPP outperforms recent novelty detection methods evaluated on popular benchmarks.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'RaPP compares input and its autoencoder reconstruction not only in the input space but also in the hidden spaces . in order to aggregate the hidden space activation values, we propose two metrics, which enhance the novelty detection performance .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 224/1992 [14:19<1:50:06,  3.74s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Disentangling underlying generative factors of a data distribution is important for interpretability and generalizable representations. In this paper,  we introduce two novel disentangling methods. Our first method, Unlabeled Disentangling GAN (UD-GAN, unsupervised), decomposes the latent noise by generating similar/dissimilar image pairs and it learns a distance metric on these pairs with siamese networks and a contrastive loss. This pairwise approach provides consistent representations for similar data points. Our second method (UD-GAN-G, weakly supervised) modifies the UD-GAN with user-defined guidance functions, which restrict the information that goes into the siamese networks. This constraint helps UD-GAN-G to focus on the desired semantic variations in the data. We  show  that  both  our  methods  outperform  existing  unsupervised approaches in quantitative metrics that measure semantic accuracy of the learned representations. In addition, we illustrate that\n[{'summary_text': 'unlabeled disentangling GAN decomposes latent noise by generating similar/dissimilar image pairs . it learns a distance metric on these pairs with siamese networks and a contrastive loss . this pairwise approach provides consistent representations for similar data points .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█▏        | 225/1992 [14:23<1:53:03,  3.84s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"What can we learn about the functional organization of cortical microcircuits from large-scale recordings of neural activity?   To obtain an explicit and interpretable model of time-dependent functional connections between neurons and to establish the dynamics of the cortical information flow, we develop 'dynamic neural relational inference' (dNRI). We study  both synthetic and real-world neural spiking data and demonstrate that the developed method is able to uncover the dynamic relations between neurons more reliably than existing baselines.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': \"to obtain an explicit and interpretable model of time-dependent functional connections between neurons and to establish the dynamics of the cortical information flow, we develop 'dynamic neural relational inference' (dNRI) the developed method is able to uncover the dynamic relations between neurons more reliably than existing baselines .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█▏        | 226/1992 [14:27<1:50:16,  3.75s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We consider two questions at the heart of machine learning; how can we predict if a minimum will generalize to the test set, and why does stochastic gradient descent find minima that generalize well? Our work responds to \\citet{zhang2016understanding}, who showed deep neural networks can easily memorize randomly labeled training data, despite generalizing well on real labels of the same inputs. We show that the same phenomenon occurs in small linear models. These observations are explained by the Bayesian evidence, which penalizes sharp minima but is invariant to model parameterization. We also demonstrate that, when one holds the learning rate fixed, there is an optimum batch size which maximizes the test set accuracy. We propose that the noise introduced by small mini-batches drives the parameters towards minima whose evidence is large. Interpreting stochastic gradient descent as a stochastic differential equation, we identify the ``noise scale\" $g = \\epsilon\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-06T18:16:40.392836Z","iopub.execute_input":"2024-10-06T18:16:40.393129Z","iopub.status.idle":"2024-10-06T18:16:41.549082Z","shell.execute_reply.started":"2024-10-06T18:16:40.393096Z","shell.execute_reply":"2024-10-06T18:16:41.548030Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nos.mkdir('/kaggle/working/output_dir')","metadata":{"execution":{"iopub.status.busy":"2024-10-06T18:16:41.553024Z","iopub.execute_input":"2024-10-06T18:16:41.553454Z","iopub.status.idle":"2024-10-06T18:16:41.561242Z","shell.execute_reply.started":"2024-10-06T18:16:41.553418Z","shell.execute_reply":"2024-10-06T18:16:41.560345Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install -U -q \"huggingface_hub[cli]\"","metadata":{"execution":{"iopub.status.busy":"2024-10-06T18:16:41.562303Z","iopub.execute_input":"2024-10-06T18:16:41.562594Z","iopub.status.idle":"2024-10-06T18:16:56.305397Z","shell.execute_reply.started":"2024-10-06T18:16:41.562562Z","shell.execute_reply":"2024-10-06T18:16:56.304087Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token [HF_Token]","metadata":{"execution":{"iopub.status.busy":"2024-10-06T17:37:29.837320Z","iopub.execute_input":"2024-10-06T17:37:29.837643Z","iopub.status.idle":"2024-10-06T17:37:31.534145Z","shell.execute_reply.started":"2024-10-06T17:37:29.837608Z","shell.execute_reply":"2024-10-06T17:37:31.533153Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"scitldr\")\n\ndataset['train'] = dataset['train'].remove_columns(['source_labels', 'rouge_scores', 'paper_id'])\n\ndataset['validation'] = dataset['validation'].remove_columns(['source_labels', 'rouge_scores', 'paper_id'])\n\ndataset['test'] = dataset['test'].remove_columns(['source_labels', 'rouge_scores', 'paper_id'])\n\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-10-06T17:24:27.044491Z","iopub.execute_input":"2024-10-06T17:24:27.044886Z","iopub.status.idle":"2024-10-06T17:24:43.141432Z","shell.execute_reply.started":"2024-10-06T17:24:27.044848Z","shell.execute_reply":"2024-10-06T17:24:43.140413Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"scitldr.py:   0%|          | 0.00/7.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f18048d9e0f4c958a290cd81f2a8c99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dataset_infos.json:   0%|          | 0.00/7.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da977ecf4a024d409beebd3c3534ed97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20f2e72638fe4c37ac7743fdc2486a07"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for scitldr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/scitldr.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ee6b6b05ea040679b406a83dcfd1366"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.12M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93029839a64343059b4613f627cc81ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.20M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e7405f703464ab891ee29ea4147354b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1992 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd4c6878e878419f9f2ca4d73c280a79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/618 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41a3f030ebb04faebbfe1e0b70e3d945"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/619 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e001b51aa363471aaf47214d0d95c8e4"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['source', 'target'],\n        num_rows: 1992\n    })\n    test: Dataset({\n        features: ['source', 'target'],\n        num_rows: 618\n    })\n    validation: Dataset({\n        features: ['source', 'target'],\n        num_rows: 619\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom transformers import DataCollatorForLanguageModeling\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\ndef preprocess_function(examples):\n    return tokenizer([\" \".join(x) for x in examples[\"source\"]])\n\n# Map this function to dataset\ntokenized_dataset = dataset.map(\n                      preprocess_function,\n                      batched= True,#Batch the data to increase speed\n                      num_proc = 4, #Number of Processes | Parallelization step\n                      remove_columns= dataset['train'].column_names\n                      )\n\ntokenizer.pad_token = tokenizer.eos_token\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T17:24:43.142608Z","iopub.execute_input":"2024-10-06T17:24:43.142923Z","iopub.status.idle":"2024-10-06T17:25:04.296194Z","shell.execute_reply.started":"2024-10-06T17:24:43.142890Z","shell.execute_reply":"2024-10-06T17:25:04.295143Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcb37376f85344a9a6aa97b96f4fc95a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98197c96f29a4279b03ca0ab3588207d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e955638fd76c4e439320a86e522930b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50f3676111064909b323053af7f0cb4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2e23f752fc04eefb08219840a6c3ab0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"933ffd8cab9a4d6eb32e0bfcb5907a24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1992 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef3a35ff015c442da933f062012c6a2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/618 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41299273977748b385ce4fd43fb450ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/619 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6568ef8bf6c4acf8050efbc57946b04"}},"metadata":{}}]},{"cell_type":"markdown","source":"# SFT on Complete Model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoConfig, AutoModelForCausalLM, TrainingArguments, Trainer\nimport math\n\n# Import the model:\n# Model_checkpoint = string containing model Id or Path to directory containing model weights save using save_pretrained()\n# Kwargs generally mention change to configuration to the model while loading like loading a tensorflow model in pytorch using from_tf=True\nmodel = AutoModelForCausalLM.from_pretrained('gpt2')\n\n# Training the model:\n# TrainingArguments can be found at https://huggingface.co/transformers/v4.7.0/main_classes/trainer.html?highlight=trainingarguments#transformers.TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/RawSFT\",\n    eval_strategy=\"epoch\",\n    run_name=\"gpt2-summerizer\",\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    # push_to_hub=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    data_collator=data_collator, # For padding with EOS as mentioned earlier\n    tokenizer=tokenizer,\n)\n\ntrainer.train()\n\n# Evaluation of results:\neval_results = trainer.evaluate()\nprint(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-06T17:25:53.598076Z","iopub.execute_input":"2024-10-06T17:25:53.598503Z","iopub.status.idle":"2024-10-06T17:32:56.888981Z","shell.execute_reply.started":"2024-10-06T17:25:53.598460Z","shell.execute_reply":"2024-10-06T17:32:56.888008Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [375/375 06:46, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>3.419472</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>3.375183</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>3.364841</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [39/39 00:14]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Perplexity: 28.93\n","output_type":"stream"}]},{"cell_type":"code","source":"# To push the model to HuggingFace we can use:\ntrainer.create_model_card()\ntrainer.push_to_hub(\"HF_username/RawSFT\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SFT LoRA","metadata":{}},{"cell_type":"code","source":"!pip install -q trl peft","metadata":{"execution":{"iopub.status.busy":"2024-10-06T18:16:56.307906Z","iopub.execute_input":"2024-10-06T18:16:56.308272Z","iopub.status.idle":"2024-10-06T18:17:10.475910Z","shell.execute_reply.started":"2024-10-06T18:16:56.308224Z","shell.execute_reply":"2024-10-06T18:17:10.474657Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom trl import SFTTrainer \nfrom peft import LoraConfig\nfrom random import randrange\nfrom transformers import AutoConfig, AutoModelForCausalLM, TrainingArguments, Trainer \nimport math \n\n\n# Create a data modification function like below if required to perform SFT.\n# Here we are considering case of text-summarization as shown in blog: LoRA for Fine-Tuning LLMs explained with codes and example\n# Change or remove this function based on your dataset.\ndef prompt_instruction_format(sample):\n    return f\"\"\"### Instruction:\n        Use the Task below and the Input given to write the Response:\n\n        ### Task:\n        Summarize the Input\n\n        ### Input:\n        {sample['source']}\n\n        ### Response:\n        {sample['target']}\n        \"\"\" \n\n# Load your data that you want to fine-tune the existing model with\ndataset = load_dataset(\"scitldr\")\n\ndataset['train'] = dataset['train'].remove_columns(['source_labels', 'rouge_scores', 'paper_id'])\n\ndataset['validation'] = dataset['validation'].remove_columns(['source_labels', 'rouge_scores', 'paper_id'])\n\ndataset['test'] = dataset['test'].remove_columns(['source_labels', 'rouge_scores', 'paper_id'])\n\n# Load your pretrained model, either from huggingface or from your local\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\n# Set new training arguments for fine-tuning \ntrainingArgs = TrainingArguments(\n    output_dir=\"/kaggle/working/SFT_with_LoRA\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    save_strategy=\"epoch\",\n    learning_rate=2e-4\n)\n\n# Create configuration for LoRA \npeft_config = LoraConfig(\n      lora_alpha=16, # The alpha parameter for Lora scaling, adjusts the influence of LoRA adaptations, generally set 16, 32, or 64.\n      lora_dropout=0.1, # Dropout Rate.\n      r=64, #  Lora attention dimension (the “rank”).\n      bias=\"none\", # Bias type for LoRA. Can be ‘none’, ‘all’ or ‘lora_only’.\n      task_type=\"CAUSAL_LM\", # Type of task\n      # target_modules = [\"q_proj\", \"v_proj\"] # Specify which layers of the model should be adapted by LoRA.\n      # layers_to_transform: [0,1,2,4] # Control which layers are adapted by specifying their indices.\n)\n\n# Create a SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset['train'],\n    eval_dataset = dataset['test'],\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    packing=True,\n    formatting_func=prompt_instruction_format,\n    args=trainingArgs,\n)\n\n# Evaluation of results:\neval_results = trainer.evaluate()\nprint(f\"Perplexity Before Training: {math.exp(eval_results['eval_loss']):.2f}\")\n\n# Fine-tune the final model\ntrainer.train()\n\n# Evaluation of results:\neval_results = trainer.evaluate()\nprint(f\"Perplexity After Training: {math.exp(eval_results['eval_loss']):.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-06T17:33:31.409137Z","iopub.execute_input":"2024-10-06T17:33:31.409511Z","iopub.status.idle":"2024-10-06T17:36:23.876383Z","shell.execute_reply.started":"2024-10-06T17:33:31.409462Z","shell.execute_reply":"2024-10-06T17:36:23.875443Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d75f047b2f748cd88ad75545bf827c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0217057d9d9c4386a6c3cd1d4675de5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='32' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [16/16 02:45]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"Perplexity Before Training: 27.88\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='85' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [85/85 02:11, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Perplexity After Training: 11.57\n","output_type":"stream"}]},{"cell_type":"code","source":"# To push the new model to HuggingFace we can use:\ntrainer.create_model_card()\ntrainer.push_to_hub(\"HF_username/SFT_with_LoRA\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RLHF","metadata":{}},{"cell_type":"markdown","source":"### IMP NOTE: Restart the notebook, run the code before Data Preprocessing and then directly the cells below. This is for not getting any dependency or CUDA error","metadata":{}},{"cell_type":"code","source":"!pip install -q trl peft","metadata":{"execution":{"iopub.status.busy":"2024-10-06T17:39:34.074929Z","iopub.execute_input":"2024-10-06T17:39:34.075705Z","iopub.status.idle":"2024-10-06T17:39:47.254667Z","shell.execute_reply.started":"2024-10-06T17:39:34.075661Z","shell.execute_reply":"2024-10-06T17:39:47.253603Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSequenceClassification, \n    AutoTokenizer, \n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, TaskType, get_peft_model\nfrom trl import RewardTrainer, RewardConfig\n\n# Load the dataset and tokenizer\ndataset = load_dataset(\"davanstrien/dataset-tldr-preference-dpo\")\ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n\n# Remove unnecessary columns from the dataset\ndataset = dataset.remove_columns([\n    'datasetId', 'card', 'generation_models', \n    'generations', 'model_name', 'ratings', \n    'rationales', 'prompt'\n])\n\n# Preprocessing function for tokenizing the \"chosen\" and \"rejected\" texts\ndef preprocess_function(examples):\n    chosen = tokenizer([x for x in examples[\"chosen\"]], padding=\"max_length\", truncation=True)\n    rejected = tokenizer([x for x in examples[\"rejected\"]], padding=\"max_length\", truncation=True)\n    \n    return {\n        \"input_ids_chosen\": chosen[\"input_ids\"],\n        \"attention_mask_chosen\": chosen[\"attention_mask\"],\n        \"input_ids_rejected\": rejected[\"input_ids\"],\n        \"attention_mask_rejected\": rejected[\"attention_mask\"]\n    }\n\n# Apply preprocessing to the dataset\ntokenized_dataset = dataset.map(\n    preprocess_function,\n    batched=True,  # Process in batches to increase speed\n    num_proc=4,    # Use parallel processing\n    remove_columns=dataset['train'].column_names  # Remove unused columns after preprocessing\n)\n\n# Split the tokenized dataset into training and testing sets\ntokenized_dataset = tokenized_dataset['train'].train_test_split(test_size=0.2)\n\n# Ensure the padding token is set correctly\ntokenizer.pad_token = tokenizer.eos_token\n\n# Data collator to handle padding\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n# Set model ID\nmodel_id = \"t5-small\"\n\n# Load the model and move it to the correct device (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModelForSequenceClassification.from_pretrained(model_id)\nmodel.to(device)\n\n# Align padding tokens between tokenizer and model\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n# Define PEFT (LoRA) configuration for efficient fine-tuning\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n)\n\n# Apply PEFT to the model\nmodel = get_peft_model(model, peft_config)\n\n# Define reward-specific training arguments\ntraining_args = RewardConfig(\n    output_dir=\"./output_dir/my_reward_model\",  # Adjust the path for local storage\n    eval_strategy=\"steps\",\n    center_rewards_coefficient=0.01,\n    eval_steps=50,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    per_device_train_batch_size=8,  # Adjusted batch size\n    max_length=50,\n    do_eval=True,\n)\n\n# Define the RewardTrainer with TRL\ntrainer = RewardTrainer(\n    model=model,\n    args=training_args,\n    tokenizer=tokenizer,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['test'],\n    peft_config=peft_config\n)\n\n# Train the model\ntrainer.train()\n\n# Save the model\ntrainer.save_model(training_args.output_dir)\n\n# Evaluate and log metrics\nmetrics = trainer.evaluate()\ntrainer.log_metrics(\"eval\", metrics)\ntrainer.save_metrics(\"eval\", metrics)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T17:40:39.592031Z","iopub.execute_input":"2024-10-06T17:40:39.592433Z","iopub.status.idle":"2024-10-06T17:42:12.795148Z","shell.execute_reply.started":"2024-10-06T17:40:39.592387Z","shell.execute_reply":"2024-10-06T17:42:12.794222Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nCould not estimate the number of tokens of the input, floating-point operations will not be computed\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [81/81 01:24, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>No log</td>\n      <td>0.702054</td>\n      <td>0.333333</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mchosen_text                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrejected_text                               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlogits          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n│ This dataset contains persona descriptions    │ The tl;dr summary of the dataset based on    │ [0.5153, 0.4847] │\n│ and traits, which can be used to develop      │ the provided card is unavailable, as no      │                  │\n│ chatbots or virtual assistants that can       │ dataset card has been supplied. Please       │                  │\n│ engage in more personalized and human-like    │ provide a dataset card with information      │                  │\n│ conversations.<pad><pad><pad><pad><pad><pad>… │ about a specific dataset to generate an      │                  │\n│                                               │ accurate tl;dr                               │                  │\n│                                               │ summary.<pad><pad><pad><pad><pad><pad><pad>… │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ The HelpSteer dataset contains 37,120 samples │ HelpSteer is a dataset containing 37,120     │ [0.5259, 0.4741] │\n│ with prompts and responses, along with        │ samples of prompts, responses, and           │                  │\n│ human-annotated attributes for helpfulness,   │ human-annotated attributes, enabling the     │                  │\n│ correctness, coherence, complexity, and       │ training of models to become more helpful,   │                  │\n│ verbosity, designed to aid in aligning models │ factually correct, coherent, and adjustable  │                  │\n│ to provide more helpful, factually correct,   │ in terms of complexity and                   │                  │\n│ and coherent                                  │ verbosity.<pad><pad><pad><pad><pad><pad><pa… │                  │\n│ responses.<pad><pad><pad><pad><pad><pad><pad… │                                              │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ The Roboflow Website Screenshots dataset is a │ This dataset contains annotated screenshots  │ [0.5507, 0.4493] │\n│ synthetic dataset containing over 1000        │ from over 1000 top websites, labeled with    │                  │\n│ annotated screenshots from popular websites,  │ various HTML elements such as buttons,       │                  │\n│ useful for Robotic Process Automation and     │ headings, links, and images, which can be    │                  │\n│ offering a cost-effective alternative to      │ used for Robotic Process Automation and      │                  │\n│ human                                         │ related                                      │                  │\n│ labeling.<pad><pad><pad><pad><pad><pad><pad>… │ applications.<pad><pad><pad><pad><pad><pad>… │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ MultiCoNER is a fine-grained multilingual     │ The MultiCoNER dataset, available in 12      │ [0.5328, 0.4672] │\n│ named entity recognition dataset covering 12  │ languages, is a fine-grained multilingual    │                  │\n│ languages, including Bangla, Chinese,         │ named entity recognition (NER) resource      │                  │\n│ English, and others, annotated with 11 entity │ designed for use in the SemEval-2023 Task    │                  │\n│ types such as Location, Person, and Medical,  │ 2.<pad><pad><pad><pad><pad><pad><pad><pad><… │                  │\n│ suitable for training and evaluating models   │                                              │                  │\n│ for named entity recognition                  │                                              │                  │\n│ tasks.<pad><pad><pad><pad><pad><pad><pad><pa… │                                              │                  │\n└───────────────────────────────────────────────┴──────────────────────────────────────────────┴──────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> chosen_text                                   </span>┃<span style=\"font-weight: bold\"> rejected_text                                </span>┃<span style=\"font-weight: bold\"> logits           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n│ This dataset contains persona descriptions    │ The tl;dr summary of the dataset based on    │ [0.5153, 0.4847] │\n│ and traits, which can be used to develop      │ the provided card is unavailable, as no      │                  │\n│ chatbots or virtual assistants that can       │ dataset card has been supplied. Please       │                  │\n│ engage in more personalized and human-like    │ provide a dataset card with information      │                  │\n│ conversations.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;… │ about a specific dataset to generate an      │                  │\n│                                               │ accurate tl;dr                               │                  │\n│                                               │ summary.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;… │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ The HelpSteer dataset contains 37,120 samples │ HelpSteer is a dataset containing 37,120     │ [0.5259, 0.4741] │\n│ with prompts and responses, along with        │ samples of prompts, responses, and           │                  │\n│ human-annotated attributes for helpfulness,   │ human-annotated attributes, enabling the     │                  │\n│ correctness, coherence, complexity, and       │ training of models to become more helpful,   │                  │\n│ verbosity, designed to aid in aligning models │ factually correct, coherent, and adjustable  │                  │\n│ to provide more helpful, factually correct,   │ in terms of complexity and                   │                  │\n│ and coherent                                  │ verbosity.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa… │                  │\n│ responses.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad… │                                              │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ The Roboflow Website Screenshots dataset is a │ This dataset contains annotated screenshots  │ [0.5507, 0.4493] │\n│ synthetic dataset containing over 1000        │ from over 1000 top websites, labeled with    │                  │\n│ annotated screenshots from popular websites,  │ various HTML elements such as buttons,       │                  │\n│ useful for Robotic Process Automation and     │ headings, links, and images, which can be    │                  │\n│ offering a cost-effective alternative to      │ used for Robotic Process Automation and      │                  │\n│ human                                         │ related                                      │                  │\n│ labeling.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;… │ applications.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;… │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ MultiCoNER is a fine-grained multilingual     │ The MultiCoNER dataset, available in 12      │ [0.5328, 0.4672] │\n│ named entity recognition dataset covering 12  │ languages, is a fine-grained multilingual    │                  │\n│ languages, including Bangla, Chinese,         │ named entity recognition (NER) resource      │                  │\n│ English, and others, annotated with 11 entity │ designed for use in the SemEval-2023 Task    │                  │\n│ types such as Location, Person, and Medical,  │ 2.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;… │                  │\n│ suitable for training and evaluating models   │                                              │                  │\n│ for named entity recognition                  │                                              │                  │\n│ tasks.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa… │                                              │                  │\n└───────────────────────────────────────────────┴──────────────────────────────────────────────┴──────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mchosen_text                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrejected_text                               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlogits          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n│ This dataset contains persona descriptions    │ The tl;dr summary of the dataset based on    │ [0.4909, 0.5091] │\n│ and traits, which can be used to develop      │ the provided card is unavailable, as no      │                  │\n│ chatbots or virtual assistants that can       │ dataset card has been supplied. Please       │                  │\n│ engage in more personalized and human-like    │ provide a dataset card with information      │                  │\n│ conversations.<pad><pad><pad><pad><pad><pad>… │ about a specific dataset to generate an      │                  │\n│                                               │ accurate tl;dr                               │                  │\n│                                               │ summary.<pad><pad><pad><pad><pad><pad><pad>… │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ The HelpSteer dataset contains 37,120 samples │ HelpSteer is a dataset containing 37,120     │ [0.5879, 0.4121] │\n│ with prompts and responses, along with        │ samples of prompts, responses, and           │                  │\n│ human-annotated attributes for helpfulness,   │ human-annotated attributes, enabling the     │                  │\n│ correctness, coherence, complexity, and       │ training of models to become more helpful,   │                  │\n│ verbosity, designed to aid in aligning models │ factually correct, coherent, and adjustable  │                  │\n│ to provide more helpful, factually correct,   │ in terms of complexity and                   │                  │\n│ and coherent                                  │ verbosity.<pad><pad><pad><pad><pad><pad><pa… │                  │\n│ responses.<pad><pad><pad><pad><pad><pad><pad… │                                              │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ The Roboflow Website Screenshots dataset is a │ This dataset contains annotated screenshots  │ [0.5049, 0.4951] │\n│ synthetic dataset containing over 1000        │ from over 1000 top websites, labeled with    │                  │\n│ annotated screenshots from popular websites,  │ various HTML elements such as buttons,       │                  │\n│ useful for Robotic Process Automation and     │ headings, links, and images, which can be    │                  │\n│ offering a cost-effective alternative to      │ used for Robotic Process Automation and      │                  │\n│ human                                         │ related                                      │                  │\n│ labeling.<pad><pad><pad><pad><pad><pad><pad>… │ applications.<pad><pad><pad><pad><pad><pad>… │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ MultiCoNER is a fine-grained multilingual     │ The MultiCoNER dataset, available in 12      │ [0.5486, 0.4514] │\n│ named entity recognition dataset covering 12  │ languages, is a fine-grained multilingual    │                  │\n│ languages, including Bangla, Chinese,         │ named entity recognition (NER) resource      │                  │\n│ English, and others, annotated with 11 entity │ designed for use in the SemEval-2023 Task    │                  │\n│ types such as Location, Person, and Medical,  │ 2.<pad><pad><pad><pad><pad><pad><pad><pad><… │                  │\n│ suitable for training and evaluating models   │                                              │                  │\n│ for named entity recognition                  │                                              │                  │\n│ tasks.<pad><pad><pad><pad><pad><pad><pad><pa… │                                              │                  │\n└───────────────────────────────────────────────┴──────────────────────────────────────────────┴──────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> chosen_text                                   </span>┃<span style=\"font-weight: bold\"> rejected_text                                </span>┃<span style=\"font-weight: bold\"> logits           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n│ This dataset contains persona descriptions    │ The tl;dr summary of the dataset based on    │ [0.4909, 0.5091] │\n│ and traits, which can be used to develop      │ the provided card is unavailable, as no      │                  │\n│ chatbots or virtual assistants that can       │ dataset card has been supplied. Please       │                  │\n│ engage in more personalized and human-like    │ provide a dataset card with information      │                  │\n│ conversations.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;… │ about a specific dataset to generate an      │                  │\n│                                               │ accurate tl;dr                               │                  │\n│                                               │ summary.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;… │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ The HelpSteer dataset contains 37,120 samples │ HelpSteer is a dataset containing 37,120     │ [0.5879, 0.4121] │\n│ with prompts and responses, along with        │ samples of prompts, responses, and           │                  │\n│ human-annotated attributes for helpfulness,   │ human-annotated attributes, enabling the     │                  │\n│ correctness, coherence, complexity, and       │ training of models to become more helpful,   │                  │\n│ verbosity, designed to aid in aligning models │ factually correct, coherent, and adjustable  │                  │\n│ to provide more helpful, factually correct,   │ in terms of complexity and                   │                  │\n│ and coherent                                  │ verbosity.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa… │                  │\n│ responses.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad… │                                              │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ The Roboflow Website Screenshots dataset is a │ This dataset contains annotated screenshots  │ [0.5049, 0.4951] │\n│ synthetic dataset containing over 1000        │ from over 1000 top websites, labeled with    │                  │\n│ annotated screenshots from popular websites,  │ various HTML elements such as buttons,       │                  │\n│ useful for Robotic Process Automation and     │ headings, links, and images, which can be    │                  │\n│ offering a cost-effective alternative to      │ used for Robotic Process Automation and      │                  │\n│ human                                         │ related                                      │                  │\n│ labeling.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;… │ applications.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;… │                  │\n├───────────────────────────────────────────────┼──────────────────────────────────────────────┼──────────────────┤\n│ MultiCoNER is a fine-grained multilingual     │ The MultiCoNER dataset, available in 12      │ [0.5486, 0.4514] │\n│ named entity recognition dataset covering 12  │ languages, is a fine-grained multilingual    │                  │\n│ languages, including Bangla, Chinese,         │ named entity recognition (NER) resource      │                  │\n│ English, and others, annotated with 11 entity │ designed for use in the SemEval-2023 Task    │                  │\n│ types such as Location, Person, and Medical,  │ 2.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;… │                  │\n│ suitable for training and evaluating models   │                                              │                  │\n│ for named entity recognition                  │                                              │                  │\n│ tasks.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa… │                                              │                  │\n└───────────────────────────────────────────────┴──────────────────────────────────────────────┴──────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7/7 00:02]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"***** eval metrics *****\n  epoch                   =        3.0\n  eval_accuracy           =     0.3333\n  eval_loss               =      0.702\n  eval_runtime            = 0:00:03.57\n  eval_samples_per_second =     29.389\n  eval_steps_per_second   =      1.959\n","output_type":"stream"}]},{"cell_type":"code","source":"# To push the new model to HuggingFace:\ntrainer.save_model(\"/kaggle/working/output_dir/my_reward_model\")","metadata":{"execution":{"iopub.status.busy":"2024-10-06T17:42:12.796686Z","iopub.execute_input":"2024-10-06T17:42:12.797008Z","iopub.status.idle":"2024-10-06T17:42:12.948767Z","shell.execute_reply.started":"2024-10-06T17:42:12.796974Z","shell.execute_reply":"2024-10-06T17:42:12.948003Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, pipeline, DataCollatorForLanguageModeling\nfrom transformers import LogitsProcessorList, MinLengthLogitsProcessor\nfrom trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\nfrom tqdm import tqdm\n\n# Load the dataset\ndataset = load_dataset(\"scitldr\")\n\n# Remove unwanted columns from all dataset splits\ncolumns_to_remove = ['source_labels', 'rouge_scores', 'paper_id']\nfor split in ['train', 'validation', 'test']:\n    dataset[split] = dataset[split].remove_columns(columns_to_remove)\n\n# Set up PPO config with a GPT-like model (causal language model)\nconfig = PPOConfig(\n    model_name=\"gpt2\",  # Use GPT-2 or another causal language model\n    learning_rate=1.41e-5,\n    mini_batch_size=1,\n    batch_size=1,\n    gradient_accumulation_steps=1\n)\n\n# Union function to concatenate the source field\ndef union(sample):\n    return {'union': \" \".join(sample['source']).strip()}\n\n# Apply the union function to all dataset splits\nunion_dataset = dataset.map(\n    union,\n    batched=False,  # Process samples one at a time\n    num_proc=4,     # Parallel processing for speed\n    remove_columns=dataset['train'].column_names\n)\n\n# Load the GPT-2 model and tokenizer\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\ntokenizer = AutoTokenizer.from_pretrained(config.model_name)\ntokenizer.pad_token = tokenizer.eos_token  # Ensure padding token is set correctly\n\n# Set up reward model pipeline (use the model trained for rewards)\nreward_model = pipeline(\n    \"summarization\",\n    model=\"/kaggle/working/output_dir/my_reward_model\",  # Path to your reward model\n    device=0 if torch.cuda.is_available() else -1\n)\n\n# Tokenization function\ndef tokenize(sample):\n    return tokenizer(sample['union'], padding='max_length', max_length=200, truncation=True)\n\n# Tokenize the dataset\ntokenized_dataset = union_dataset.map(\n    tokenize,\n    batched=False,\n    num_proc=4,\n    remove_columns=union_dataset['train'].column_names\n)\n\n# Data collator to handle padding\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n\neos_token_id = torch.tensor(tokenizer.eos_token_id).to(device)\n\nlogits_processor = LogitsProcessorList([\n    MinLengthLogitsProcessor(min_length=10, eos_token_id=eos_token_id),  # Ensure min length\n])\n\n# Initialize PPOTrainer\nppo_trainer = PPOTrainer(\n    model=model,\n    config=config,\n    dataset=tokenized_dataset['train'],\n    tokenizer=tokenizer,\n    data_collator=data_collator\n)\n\n# Keywords to pass to model.generate at every SFT step\ngeneration_kwargs = {\n    \"min_length\": -1,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.eos_token_id,\n    \"max_length\": 400,\n    \"logits_processor\":logits_processor\n}\n\n# Training loop\nepochs = 1\nfor epoch in tqdm(range(epochs), desc=\"epoch\"):\n    for batch in tqdm(ppo_trainer.dataloader, desc=\"batch\"): \n        query_tensors = batch[\"input_ids\"]\n\n        # Loop through the batch, generating responses for each sample individually\n        for query_tensor in query_tensors:\n            # Ensure query tensors are correctly shaped (1D tensors)\n            query_tensor = query_tensor.squeeze()\n\n            print(tokenizer.decode(query_tensor))\n\n            # Generate responses using the model (wrap query_tensor in a list)\n            response_tensors = ppo_trainer.generate([query_tensor], **generation_kwargs)\n            batch[\"target\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n\n            # Combine queries and responses for reward calculation\n            texts = [tokenizer.decode(query_tensor) + tokenizer.decode(r.squeeze()) for r in response_tensors]\n            \n            # Get the summaries from the reward model (summarization)\n            summaries = reward_model(texts)\n            print(summaries)\n\n            # Calculate a simple reward based on length difference (convert rewards to float)\n            rewards = []\n            for query, summary in zip(tokenizer.decode(query_tensor), summaries):\n                # A simple reward calculation: length difference between original and summary\n                reward = torch.tensor(len(summary[\"summary_text\"]) - len(query)).float()  # Cast to float\n                rewards.append(reward)\n\n            # Perform PPO step (pass query_tensor and response_tensors as lists)\n            stats = ppo_trainer.step([query_tensor], response_tensors, rewards)\n            ppo_trainer.log_stats(stats, batch, rewards)\n\n# Save the PPO-trained model\nppo_trainer.save_model(\"/kaggle/working/output_dir/my_ppo_model\")\n\n# Evaluate the results\neval_results = ppo_trainer.evaluate()\nprint(f\"Perplexity: {torch.exp(torch.tensor(eval_results['eval_loss'])):.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T17:43:07.447407Z","iopub.execute_input":"2024-10-06T17:43:07.448379Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"epoch:   0%|          | 0/1 [00:00<?, ?it/s]\nbatch:   0%|          | 0/1992 [00:00<?, ?it/s]\u001b[AYou're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"name":"stdout","text":"In this paper, we explore new approaches to combining information encoded within the learned representations of autoencoders. We explore models that are capable of combining the attributes of multiple inputs such that a resynthesised output is trained to fool an adversarial discriminator for real versus synthesised data. Furthermore, we explore the use of such an architecture in the context of semi-supervised learning, where we learn a mixing function whose objective is to produce interpolations of hidden states, or masked combinations of latent representations that are consistent with a conditioned class label. We show quantitative and qualitative evidence that such a formulation is an interesting avenue of research.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n","output_type":"stream"},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (812 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"[{'summary_text': 'in this paper, we explore new approaches to combining information encoded within the learned representations of autoencoders . we explore models that are capable of combining attributes of multiple inputs such that a resynthesised output is trained to fool an adversarial discriminator . also explore the use of such an architecture in the context of semi-supervised learning .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   0%|          | 1/1992 [00:03<2:12:28,  3.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recently, there has been a surge in interest in safe and robust techniques within reinforcement learning (RL). \n Current notions of risk in RL fail to capture the potential for systemic failures such as abrupt stoppages from system failures or surpassing of safety thresholds and the appropriate responsive controls in such instances. We propose a novel approach to fault-tolerance within RL in which the controller learns a policy can cope with adversarial attacks and random stoppages that lead to failures of the system subcomponents. The results of the paper also cover fault-tolerant (FT) control so that the controller learns to avoid states that carry risk of system failures. By demonstrating that the class of problems is represented by a variant of SGs, we prove the existence of a solution which is a unique fixed point equilibrium of the game and characterise the optimal controller behaviour. We then introduce a value function approximation algorithm that converges to the solution through simulation in unknown environments.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'current notions of risk in RL fail to capture potential for systemic failures such as abrupt stoppages from system failures or surpassing of safety thresholds . we propose a novel approach to fault-tolerance within RL in which the controller learns a policy can cope with adversarial attacks . by demonstrating that the class of problems is represented by a variant of SGs, we prove the existence of a solution which is unique fixed point equilibrium of the game .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   0%|          | 2/1992 [00:08<2:15:28,  4.08s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The verification of planning domain models is crucial to ensure the safety, integrity and correctness of planning-based automated systems. This task is usually performed using model checking techniques.   However, directly applying model checkers to verify planning domain models can result in false positives, i.e. counterexamples that are unreachable by a sound planner when using the domain under verification during a planning task. In this paper, we discuss the downside of unconstrained planning domain model verification. We then propose a fail-safe practice for designing planning domain models that can inherently guarantee the safety of the produced plans in case of undetected errors in domain models.   In addition, we demonstrate how model checkers, as well as state trajectory constraints planning techniques, should be used to verify planning domain models so that unreachable counterexamples are not returned.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'directly applying model checkers to verify planning domain models can result in false positives, i.e. counterexamples that are unreachable by a sound planner when using the domain under verification during a planning task . in this paper, we discuss the downside of unconstrained planning domain model verification .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   0%|          | 3/1992 [00:12<2:15:28,  4.09s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generating complex discrete distributions remains as one of the challenging problems in machine learning. Existing techniques for generating complex distributions with high degrees of freedom depend on standard generative models like Generative Adversarial Networks (GAN), Wasserstein GAN, and associated variations. Such models are based on an optimization involving the distance between two continuous distributions. We introduce a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions. We derive a novel training algorithm and corresponding network architecture based on the formulation. Experimental results are provided for both synthetic discrete data, and real discretized data from MNIST handwritten digits.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'existing techniques for generating complex distributions depend on standard generative models like Generative Adversarial Networks (GAN), Wasserstein GAN, and associated variations . such models are based on an optimization involving the distance between two continuous distributions . experimental results are provided for both synthetic discrete data, and real discretized data from MNIST handwritten digits.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   0%|          | 4/1992 [00:16<2:11:57,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The large memory requirements of deep neural networks strain the capabilities of many devices, limiting their deployment and adoption. Model compression methods effectively reduce the memory requirements of these models, usually through applying transformations such as weight pruning or quantization. In this paper, we present a novel scheme for lossy weight encoding which complements conventional compression techniques. The encoding is based on the Bloomier filter, a probabilistic data structure that can save space at the cost of introducing random errors. Leveraging the ability of neural networks to tolerate these imperfections and by re-training around the errors, the proposed technique, Weightless, can compress DNN weights by up to 496x; with the same model accuracy, this results in up to a 1.51x improvement over the state-of-the-art.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the encoding is based on the Bloomier filter, a probabilistic data structure that can save space at the cost of introducing random errors . the proposed technique, Weightless, can compress DNN weights by up to 496x . this results in up to a 1.51x improvement over the state-of-the-art .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   0%|          | 5/1992 [00:18<1:55:37,  3.49s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Answering complex logical queries on large-scale incomplete knowledge graphs (KGs) is a fundamental yet challenging task. Recently, a promising approach to this problem has been to embed KG entities as well as the query into a vector space such that entities that answer the query are embedded close to the query. However, prior work models queries as single points in the vector space, which is problematic because a complex query represents a potentially large set of its answer entities, but it is unclear how such a set can be represented as a single point. Furthermore, prior work can only handle queries that use conjunctions ($\\wedge$) and existential quantifiers ($\\exists$). Handling queries with logical disjunctions ($\\vee$) remains an open problem. Here we propose query2box, an embedding-based framework for reasoning over arbitrary queries with $\\wedge$, $\\vee$, and $\\exists$ operators in massive and incomplete KGs. Our main insight is that\n[{'summary_text': 'query2box is an embedding-based framework for reasoning over arbitrary queries with $wedge$, $vee$, and $exists$ operators in massive and incomplete KGs . a complex query represents a potentially large set of its answer entities, but it is unclear how such a set can be represented as a single point . prior work models queries as single points in the vector space .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   0%|          | 6/1992 [00:22<1:56:50,  3.53s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"A framework for efficient Bayesian inference in probabilistic programs is introduced by embedding a sampler inside a variational posterior approximation. Its strength lies in both ease of implementation and automatically tuning sampler parameters to speed up mixing time. Several strategies to approximate the evidence lower bound (ELBO) computation are introduced, including a rewriting of the ELBO objective. Experimental evidence is shown by performing experiments on an unconditional VAE on density estimation tasks; solving an influence diagram in a high-dimensional space with a conditional variational autoencoder (cVAE) as a deep Bayes classifier; and state-space models for time-series data.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'a framework for efficient Bayesian inference in probabilistic programs is introduced by embedding a sampler inside a variational posterior approximation . its strength lies in both ease of implementation and automatically tuning sampler parameters to speed up mixing time . experimental evidence is shown by performing experiments on an unconditional VAE on density estimation tasks .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   0%|          | 7/1992 [00:26<1:59:00,  3.60s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose a neural clustering model that jointly learns both latent features and how they cluster. Unlike similar methods our model does not require a predefined number of clusters. Using a supervised approach, we agglomerate latent features towards randomly sampled targets within the same space whilst progressively removing the targets until we are left with only targets which represent cluster centroids. To show the behavior of our model across different modalities we apply our model on both text and image data and very competitive results on MNIST. Finally, we also provide results against baseline models for fashion-MNIST, the 20 newsgroups dataset, and a Twitter dataset we ourselves create.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we propose a neural clustering model that jointly learns both latent features and how they cluster . Unlike similar methods our model does not require a predefined number of clusters . we apply our model on both text and image data and very competitive results on MNIST .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   0%|          | 8/1992 [00:29<1:53:50,  3.44s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"While real brain networks exhibit functional modularity, we investigate whether functional mod- ularity also exists in Deep Neural Networks (DNN) trained through back-propagation. Under the hypothesis that DNN are also organized in task-specific modules, in this paper we seek to dissect a hidden layer into disjoint groups of task-specific hidden neurons with the help of relatively well- studied neuron attribution methods. By saying task-specific, we mean the hidden neurons in the same group are functionally related for predicting a set of similar data samples, i.e. samples with similar feature patterns.\n We argue that such groups of neurons which we call Functional Modules can serve as the basic functional unit in DNN. We propose a preliminary method to identify Functional Modules via bi- clustering attribution scores of hidden neurons.\n We find that first, unsurprisingly, the functional neurons are highly sparse, i.e., only a small sub- set of neurons are important for predicting a small subset\n[{'summary_text': 'functional mod- ularity also exists in Deep Neural Networks (DNN) trained through back-propagation . by saying task-specific, we mean the hidden neurons in the same group are functionally related for predicting a set of similar data samples, i.e. samples with similar feature patterns .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   0%|          | 9/1992 [00:33<1:59:09,  3.61s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep neural networks, in particular convolutional neural networks, have become highly effective tools for compressing images and solving inverse problems including denoising, inpainting, and reconstruction from few and noisy measurements. This success can be attributed in part to their ability to represent and generate natural images well. Contrary to classical tools such as wavelets, image-generating deep neural networks have a large number of parameters---typically a multiple of their output dimension---and need to be trained on large datasets. \n In this paper, we propose an untrained simple image model, called the deep decoder, which is a deep neural network that can generate natural images from very few weight parameters.\n The deep decoder has a simple architecture with no convolutions and fewer weight parameters than the output dimensionality. This underparameterization enables the deep decoder to compress images into a concise set of network weights, which we show is on par with wavelet-based thresholding. Further, underparam\n[{'summary_text': 'deep neural networks have become highly effective tools for compressing images . this success can be attributed in part to their ability to represent and generate natural images well . the deep decoder has a simple architecture with no convolutions and fewer weight parameters than the output dimensionality .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 10/1992 [00:37<2:02:17,  3.70s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Convolutional networks are not aware of an object's geometric variations, which leads to inefficient utilization of model and data capacity. To overcome this issue, recent works on deformation modeling seek to spatially reconfigure the data towards a common arrangement such that semantic recognition suffers less from deformation. This is typically done by augmenting static operators with learned free-form sampling grids in the image space, dynamically tuned to the data and task for adapting the receptive field. Yet adapting the receptive field does not quite reach the actual goal -- what really matters to the network is the *effective* receptive field (ERF), which reflects how much each pixel contributes. It is thus natural to design other approaches to adapt the ERF directly during runtime. In this work, we instantiate one possible solution as Deformable Kernels (DKs), a family of novel and generic convolutional operators for handling object deformations by directly adapting the ERF while leaving the receptive field untouched. At the heart of\n","output_type":"stream"},{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"},{"name":"stdout","text":"[{'summary_text': 'recent works on deformation modeling seek to spatially reconfigure data towards a common arrangement such that semantic recognition suffers less from deformation . this is typically done by augmenting static operators with learned free-form sampling grids in the image space . but adapting the receptive field does not quite reach the actual goal .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 11/1992 [00:39<1:45:44,  3.20s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Multi-hop question answering requires models to gather information from different parts of a text to answer a question. Most current approaches learn to address this task in an end-to-end way with neural networks, without maintaining an explicit representation of the reasoning process. We propose a method to extract a discrete reasoning chain over the text, which consists of a series of sentences leading to the answer. We then feed the extracted chains to a BERT-based QA model to do final answer prediction. Critically, we do not rely on gold annotated chains or ``supporting facts:'' at training time, we derive pseudogold reasoning chains using heuristics based on named entity recognition and coreference resolution. Nor do we rely on these annotations at test time, as our model learns to extract chains from raw text alone.   We test our approach on two recently proposed large multi-hop question answering datasets: WikiHop and HotpotQA, and achieve state-of-art performance on\n[{'summary_text': 'multi-hop question answering requires models to gather information from different parts of a text to answer a question . most current approaches learn to address this task in an end-to-end way with neural networks, without maintaining an explicit representation of the reasoning process . at training time, we derive pseudogold reasoning chains using heuristics based on named entity recognition and coreference resolution .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 12/1992 [00:41<1:40:13,  3.04s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"While it has not yet been proven, empirical evidence suggests that model generalization is related to local properties of the optima which can be described via the Hessian. We connect model generalization with the local property of a solution under the PAC-Bayes paradigm. In particular, we prove that model generalization ability is related to the Hessian, the higher-order \"smoothness\" terms characterized by the Lipschitz constant of the Hessian, and the scales of the parameters. Guided by the proof, we propose a metric to score the generalization capability of the model, as well as an algorithm that optimizes the perturbed model accordingly.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'empirical evidence suggests that model generalization is related to local properties of the optima which can be described via the Hessian . guided by the proof, we propose a metric to score the generalization capability of the model, as well as an algorithm that optimizes the perturbed model accordingly .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 13/1992 [00:44<1:41:45,  3.09s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Predictive coding theories suggest that the brain learns by predicting observations at various levels of abstraction. One of the most basic prediction tasks is view prediction: how would a given scene look from an alternative viewpoint? Humans excel at this task. Our ability to imagine and fill in missing visual information is tightly coupled with perception: we feel as if we see  the world in 3 dimensions, while in fact, information from only the front surface of the world hits our (2D) retinas. This paper explores the connection between view-predictive representation learning and its role in the development of 3D visual recognition. We propose inverse graphics networks, which take as input 2.5D video streams captured by a moving camera, and map to stable 3D feature maps of the scene, by disentangling the scene content from the motion of the camera. The model can also project its 3D feature maps to novel viewpoints, to predict and match against target views. We propose contrastive prediction losses that\n[{'summary_text': 'predictive coding theories suggest that the brain learns by predicting observations at various levels of abstraction . one of the most basic prediction tasks is view prediction: how would a given scene look from an alternative viewpoint? we feel as if we see the world in 3 dimensions, while information from only the front surface of the world hits our (2D) retinas .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 14/1992 [00:49<1:51:46,  3.39s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36 hours down to 1 hour.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts . we also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36 hours down to 1 hour .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 15/1992 [00:52<1:55:00,  3.49s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Graph neural networks have recently achieved great successes in predicting quantum mechanical properties of molecules. These models represent a molecule as a graph using only the distance between atoms (nodes) and not the spatial direction from one atom to another. However, directional information plays a central role in empirical potentials for molecules, e.g. in angular potentials. To alleviate this limitation we propose directional message passing, in which we embed the messages passed between atoms instead of the atoms themselves. Each message is associated with a direction in coordinate space. These directional message embeddings are rotationally equivariant since the associated directions rotate with the molecule. We propose a message passing scheme analogous to belief propagation, which uses the directional information by transforming messages based on the angle between them. Additionally, we use spherical Bessel functions to construct a theoretically well-founded, orthogonal radial basis that achieves better performance than the currently prevalent Gaussian radial basis functions while using more than 4x fewer parameters. We leverage these innovations to\n[{'summary_text': 'these models represent a molecule as a graph using only the distance between atoms (nodes) and not the spatial direction from one atom to another . however, directional information plays a central role in empirical potentials for molecules, e.g. in angular potentials .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 16/1992 [00:56<1:59:42,  3.63s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The gradient of a deep neural network (DNN) w.r.t. the input provides information that can be used to explain the output prediction in terms of the input features and has been widely studied to assist in interpreting DNNs.  In a linear model (i.e., $g(x)=wx+b$), the gradient corresponds solely to the weights $w$. Such a model can reasonably locally linearly approximate a smooth nonlinear DNN, and hence the weights of this local model are the gradient. The other part, however, of a local linear model, i.e., the bias $b$, is usually overlooked in attribution methods since it is not part of the gradient. In this paper, we observe that since the bias in a DNN also has a non-negligible contribution to the correctness of predictions, it can also play a significant role in understanding DNN behaviors. In particular, we study how to attribute a DNN's bias to its\n[{'summary_text': 'the gradient of a deep neural network (DNN) w.r.t. the input provides information that can be used to explain the output prediction in terms of the input features . in a linear model (i.e., $g(x)=wx+b$), the gradient corresponds solely to the weights $w$ . the other part, however, is usually overlooked in attribution methods since it is not part of the gradient .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 17/1992 [01:01<2:06:52,  3.85s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Inverse problems are ubiquitous in natural sciences and refer to the challenging task of inferring complex and potentially multi-modal posterior distributions over hidden parameters given a set of observations. Typically, a model of the physical process in the form of differential equations is available but leads to intractable inference over its parameters. While the forward propagation of parameters through the model simulates the evolution of the system, the inverse problem of finding the parameters given the sequence of states is not unique. In this work, we propose a generalisation of the Bayesian optimisation framework to approximate inference. The resulting method learns approximations to the posterior distribution by applying Stein variational gradient descent on top of estimates from a Gaussian process model. Preliminary results demonstrate the method's performance on likelihood-free inference for reinforcement learning environments.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'inverse problems are ubiquitous in natural sciences and refer to the challenging task of inferring complex and potentially multi-modal posterior distributions over hidden parameters given a set of observations . Typically, a model of the physical process in the form of differential equations is available but leads to intractable inference over its parameters .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 18/1992 [01:03<1:52:56,  3.43s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Spectral embedding is a popular technique for the representation of graph data. Several regularization techniques have been proposed to improve the quality of the embedding with respect to downstream tasks like clustering. In this paper, we explain on a simple block model the impact of the complete graph regularization, whereby a constant is added to all entries of the adjacency matrix. Specifically, we show that the regularization forces the spectral embedding  to focus on  the  largest blocks, making the representation less sensitive to noise or outliers. We illustrate these results on both  on both synthetic and real data, showing how regularization improves standard clustering scores.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Spectral embedding is a popular technique for the representation of graph data . in this paper, we explain on a simple block model the impact of the complete graph regularization, whereby a constant is added to all entries of the adjacency matrix . this paper shows how regularization improves standard clustering scores .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 19/1992 [01:07<1:57:42,  3.58s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recently, researchers have discovered that the state-of-the-art object classifiers can be fooled easily by small perturbations in the input unnoticeable to human eyes.  It is known that an attacker can generate strong adversarial examples if she knows the classifier parameters . Conversely, a defender can robustify the classifier by retraining if she has the adversarial examples . \nThe cat-and-mouse game nature of attacks and defenses raises the question of the presence of equilibria in the dynamics .\nIn this paper, we present a neural-network based attack class to approximate a larger but intractable class of attacks, and \n formulate the attacker-defender interaction as a zero-sum leader-follower game. We present sensitivity-penalized optimization algorithms to find minimax solutions, which are the best worst-case defenses against whitebox attacks. Advantages of the learning-based attacks and defenses compared to gradient-based attacks and defenses\n[{'summary_text': 'the state-of-the-art object classifiers can be fooled easily by small perturbations in the input unnoticeable to human eyes . it is known that an attacker can generate strong adversarial examples if she knows the classifier parameters . in this paper, we present a neural-network based attack class to approximate a larger but intractable class of attacks .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 20/1992 [01:11<2:04:01,  3.77s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recent efforts to combine Representation Learning with Formal Methods, commonly known as the Neuro-Symbolic Methods, have given rise to a new trend of applying rich neural architectures to solve classical combinatorial optimization problems. In this paper, we propose a neural framework that can learn to solve the Circuit Satisfiability problem. Our framework is built upon two fundamental contributions: a rich embedding architecture that encodes the problem structure and an end-to-end differentiable training procedure that mimics Reinforcement Learning and trains the model directly toward solving the SAT problem. The experimental results show the superior out-of-sample generalization performance of our framework compared to the recently developed NeuroSAT method.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this paper, we propose a neural framework that can learn to solve the Circuit Satisfiability problem . our framework is built upon two fundamental contributions: a rich embedding architecture that encodes the problem structure and an end-to-end differentiable training procedure that mimics Reinforcement Learning . the experimental results show the superior out-of-sample generalization performance of our framework compared to the recently developed NeuroSAT method .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 21/1992 [01:13<1:46:48,  3.25s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Intuitively, image classification should profit from using spatial information. Recent work, however, suggests that this might be overrated in standard CNNs. In this paper, we are pushing the envelope and aim to further investigate the reliance on and necessity of spatial information. We propose and analyze three methods, namely Shuffle Conv, GAP+FC and 1x1 Conv, that destroy spatial information during both training and testing phases. We extensively evaluate these methods on several object recognition datasets (CIFAR100, Small-ImageNet, ImageNet) with a wide range of CNN architectures (VGG16, ResNet50, ResNet152, MobileNet, SqueezeNet). Interestingly, we consistently observe that spatial information can be completely deleted from a significant number of layers with no or only small performance drops.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this paper, we are pushing the envelope and aim to further investigate the reliance on and necessity of spatial information . we propose and analyze three methods, namely Shuffle Conv, GAP+FC and 1x1 Conv that destroy spatial information during both training and testing phases .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 22/1992 [01:16<1:41:06,  3.08s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper, we propose two methods, namely Trace-norm regression (TNR) and Stable Trace-norm Analysis (StaTNA), to improve performances of recommender systems with side information. Our trace-norm regression approach extracts low-rank latent factors underlying the side information that drives user preference under different context. Furthermore, our novel recommender framework StaTNA not only captures latent low-rank common drivers for user preferences, but also considers idiosyncratic taste for individual users. We compare performances of TNR and StaTNA on the MovieLens datasets against state-of-the-art models, and demonstrate that StaTNA and TNR in general outperforms these methods.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'our trace-norm regression approach extracts low-rank latent factors . underlying the side information that drives user preference under different context . our novel recommender framework StaTNA also considers idiosyncratic taste .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 23/1992 [01:18<1:32:42,  2.83s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Answering questions that require multi-hop reasoning at web-scale necessitates retrieving multiple evidence documents, one of which often has little lexical or semantic relationship to the question. This paper introduces a new graph-based recurrent retrieval approach that learns to retrieve reasoning paths over the Wikipedia graph to answer multi-hop open-domain questions. Our retriever model trains a recurrent neural network that learns to sequentially retrieve evidence paragraphs in the reasoning path by conditioning on the previously retrieved documents. \n Our reader model ranks the reasoning paths and extracts the answer span included in the best reasoning path.\n Experimental results show state-of-the-art results in three open-domain QA datasets, showcasing the effectiveness and robustness of our method. Notably, our method achieves significant improvement in HotpotQA, outperforming the previous best model by more than 14 points.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'our retriever model trains a recurrent neural network that learns to sequentially retrieve evidence paragraphs in the reasoning path . our reader model ranks the reasoning paths and extracts the answer span included in the best reasoning path, notably, our method achieves significant improvement in HotpotQA .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|          | 24/1992 [01:20<1:22:10,  2.51s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.<|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). we show that one cause for such failures is the exponential moving average used in the algorithms .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|▏         | 25/1992 [01:22<1:20:28,  2.45s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Prefrontal cortex (PFC) is a part of the brain which is responsible for behavior repertoire. Inspired by PFC functionality and connectivity,  as well as human behavior formation process, we propose a novel modular architecture of neural networks with a Behavioral Module (BM) and corresponding end-to-end training strategy.   This approach allows the efficient learning of behaviors and preferences representation. This property is particularly useful for user modeling (as for dialog agents) and recommendation tasks, as allows learning personalized representations of different user states.   In the experiment with video games playing, the resultsshow that the proposed method allows separation of main task’s objectives andbehaviors between different BMs. The experiments also show network extendability through independent learning of new behavior patterns. Moreover, we demonstrate a strategy for an efficient transfer of newly learned BMs to unseen tasks.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'prefrontal cortex (PFC) is a part of the brain which is responsible for behavior repertoire . inspired by PFC functionality and connectivity, as well as human behavior formation process, we propose a novel modular architecture of neural networks with a Behavioral Module (BM) and corresponding training strategy . this approach allows the efficient learning of behaviors and preferences representation .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|▏         | 26/1992 [01:25<1:21:29,  2.49s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We introduce a systematic framework for quantifying the robustness of classifiers to naturally occurring perturbations of images found in videos. As part of this framework, we construct ImageNet-Vid-Robust, a human-expert--reviewed dataset of 22,668 images grouped into 1,145 sets of perceptually similar images derived from frames in the ImageNet Video Object Detection dataset. We evaluate a diverse array of classifiers trained on ImageNet, including models trained for robustness, and show a median classification accuracy drop of 16\\%. Additionally, we evaluate the Faster R-CNN and R-FCN models for detection, and show that natural perturbations induce both classification as well as localization errors, leading to a median drop in detection mAP of 14 points. Our analysis shows that natural perturbations in the real world are heavily problematic for current CNNs, posing a significant challenge to their deployment in safety-critical environments that require reliable, low-latency predictions.<|endoftext|>\n[{'summary_text': 'we construct ImageNet-Vid-Robust, a human-expert--reviewed dataset of 22,668 images grouped into 1,145 sets of perceptually similar images derived from frames in the ImageNet Video Object Detection dataset . we evaluate a diverse array of classifiers trained on ImageNet, including models trained for robustness, and show a median classification accuracy drop of 16% .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|▏         | 27/1992 [01:29<1:36:36,  2.95s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present a novel black-box adversarial attack algorithm with state-of-the-art model evasion rates for query efficiency under $\\ell_\\infty$ and $\\ell_2$ metrics. It exploits a \\textit{sign-based}, rather than magnitude-based, gradient estimation approach that shifts the gradient estimation from continuous to binary black-box optimization. It adaptively constructs queries to estimate the gradient, one query relying upon the previous, rather than re-estimating the gradient each step with random query construction. Its reliance on sign bits yields  a smaller memory footprint and it requires neither hyperparameter tuning or dimensionality reduction. Further, its theoretical performance is guaranteed and it can characterize  adversarial subspaces better than white-box gradient-aligned subspaces. On two public black-box attack challenges and a model robustly trained against transfer attacks, the algorithm's evasion rates surpass all submitted attacks. For a suite of published models,  the algorithm is $\n[{'summary_text': 'algorithm exploits a textitsign-based, rather than magnitude-based, gradient estimation approach that shifts the gradient estimation from continuous to binary black-box optimization . it adaptively constructs queries to estimate the gradient, one query relying upon the previous, instead of re-estimating the gradient each step with random query construction . its theoretical performance is guaranteed and it can characterize adversarial subspaces better than white-box gradient .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|▏         | 28/1992 [01:33<1:50:23,  3.37s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Although there are more than 65,000 languages in the world, the pronunciations of many phonemes sound similar across the languages. When people learn a foreign language, their pronunciation often reflect their native language's characteristics. That motivates us to investigate how the speech synthesis network learns the pronunciation when multi-lingual dataset is given. In this study, we train the speech synthesis network bilingually in English and Korean, and analyze how the network learns the relations of phoneme pronunciation between the languages. Our experimental result shows that the learned phoneme embedding vectors are located closer if their pronunciations are similar across the languages. Based on the result, we also show that it is possible to train networks that synthesize English speaker's Korean speech and vice versa. In another experiment, we train the network with limited amount of English dataset and large Korean dataset, and analyze the required amount of dataset to train a resource-poor language with the help of resource-rich languages.<|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': \"the pronunciations of many phonemes sound similar across the languages . when people learn a foreign language, their pronunciation often reflect their native language's characteristics . this motivates us to investigate how the speech synthesis network learns the relations of phoneme pronunciation between the languages.\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   1%|▏         | 29/1992 [01:36<1:47:55,  3.30s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recent progress on physics-based character animation has shown impressive breakthroughs on human motion synthesis, through imitating motion capture data via deep reinforcement learning. However, results have mostly been demonstrated on imitating a single distinct motion pattern, and do not generalize to interactive tasks that require flexible motion patterns due to varying human-object spatial configurations. To bridge this gap, we focus on one class of interactive tasks---sitting onto a chair. We propose a hierarchical reinforcement learning framework which relies on a collection of subtask controllers trained to imitate simple, reusable mocap motions, and a meta controller trained to execute the subtasks properly to complete the main task. We experimentally demonstrate the strength of our approach over different single level and hierarchical baselines. We also show that our approach can be applied to motion prediction given an image input. A video highlight can be found at https://youtu.be/XWU3wzz1ip8/.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'recent progress on physics-based character animation has shown impressive breakthroughs on human motion synthesis . but results have mostly been demonstrated on imitating motion capture data via deep reinforcement learning . to bridge this gap, we focus on one class of interactive tasks---sitting onto a chair .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 30/1992 [01:40<1:50:50,  3.39s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Challenges in natural sciences can often be phrased as optimization problems. Machine learning techniques have recently been applied to solve such problems. One example in chemistry is the design of tailor-made organic materials and molecules, which requires efficient methods to explore the chemical space. We present a genetic algorithm (GA) that is enhanced with a neural network (DNN) based discriminator model to improve the diversity of generated molecules and at the same time steer the GA. We show that our algorithm outperforms other generative models in optimization tasks. We furthermore present a way to increase interpretability of genetic algorithms, which helped us to derive design principles<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'machine learning techniques have recently been applied to solve such problems . one example in chemistry is the design of tailor-made organic materials and molecules, which requires efficient methods to explore the chemical space . we present a genetic algorithm (GA) that is enhanced with a neural network (DNN) based discriminator model .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 31/1992 [01:44<1:53:11,  3.46s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"It is difficult for the beginners of etching latte art to make well-balanced patterns by using two fluids with different viscosities such as foamed milk and syrup. Even though making etching latte art while watching making videos which show the procedure, it is difficult to keep balance. Thus well-balanced etching latte art cannot be made easily. \n In this paper, we propose a system which supports the beginners to make well-balanced etching latte art by projecting a making procedure of etching latte art directly onto a cappuccino. \n The experiment results show the progress by using our system.   We also discuss about the similarity of the etching latte art and the design templates by using background subtraction.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'it is difficult for the beginners of etching latte art to make well-balanced patterns by using two fluids with different viscosities such as foamed milk and syrup . even though making etchers while watching making videos which show the procedure, it is hard to keep balance . the experiment results show the progress by using our system.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 32/1992 [01:47<1:55:39,  3.54s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We improve previous end-to-end differentiable neural networks (NNs) with fast\n weight memories. A gate mechanism updates fast weights at every time step of\n a sequence through two separate outer-product-based matrices generated by slow\n parts of the net. The system is trained on a complex sequence to sequence variation\n of the Associative Retrieval Problem with roughly 70 times more temporal\n memory (i.e. time-varying variables) than similar-sized standard recurrent NNs\n (RNNs). In terms of accuracy and number of parameters, our architecture outperforms\n a variety of RNNs, including Long Short-Term Memory, Hypernetworks,\n and related fast weight architectures.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'gate mechanism updates fast weights at every time step of a sequence through two separate outer-product-based matrices generated by slow parts of the net . system is trained on a complex sequence to sequence variation of the Associative Retrieval Problem with roughly 70 times more temporal memory (i.e. time-varying variables) .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 33/1992 [01:50<1:46:11,  3.25s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Auto-encoding and generative models have made tremendous successes in image and signal representation learning and generation. These models, however, generally employ the full Euclidean space or a bounded subset (such as $[0,1]^l$) as the latent space, whose trivial geometry is often too simplistic to meaningfully reflect the structure of the data. This paper aims at exploring a nontrivial geometric structure of the latent space for better data representation. Inspired by differential geometry, we propose \\textbf{Chart Auto-Encoder (CAE)}, which captures the manifold structure of the data with multiple charts and transition functions among them. CAE translates the mathematical definition of manifold through parameterizing the entire data set as a collection of overlapping charts, creating local latent representations. These representations are an enhancement of the single-charted latent space commonly employed in auto-encoding models, as they reflect the intrinsic structure of the manifold.   Therefore, CAE achieves a more accurate\n[{'summary_text': 'auto-encoding and generative models have made tremendous successes in image and signal representation learning and generation . these models, however, generally employ the full Euclidean space or a bounded subset (such as $[0,1]l$) as the latent space, whose trivial geometry is often too simplistic to meaningfully reflect the structure of the data .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 34/1992 [01:54<1:55:36,  3.54s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Presence of bias and confounding effects is inarguably one of the most critical challenges in machine learning applications that has alluded to pivotal debates in the recent years. Such challenges range from spurious associations of confounding variables in medical studies to the bias of race in gender or face recognition systems. One solution is to enhance datasets and organize them such that they do not reflect biases, which is a cumbersome and intensive task. The alternative is to make use of available data and build models considering these biases. Traditional statistical methods apply straightforward techniques such as residualization or stratification to precomputed features to account for confounding variables. However, these techniques are not in general applicable to end-to-end deep learning methods. In this paper, we propose a method based on the adversarial training strategy to learn discriminative features unbiased and invariant to the confounder(s). This is enabled by incorporating a new adversarial loss function that encourages a vanished correlation between the bias and learned features. We apply our method\n[{'summary_text': 'the alternative is to make use of available data and build models considering these biases . traditional statistical methods apply straightforward techniques such as residualization or stratification to precomputed features to account for confounding variables . this is enabled by incorporating a new adversarial loss function that encourages a vanished correlation between the bias and learned features .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 35/1992 [01:58<2:03:06,  3.77s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep learning natural language processing models often use vector word embeddings, such as word2vec or GloVe, to represent words. A discrete sequence of words can be much more easily integrated with downstream neural layers if it is represented as a  sequence of continuous vectors. Also, semantic relationships between words, learned from a text corpus, can be encoded in the relative configurations of the embedding vectors. However, storing and accessing embedding vectors for all words in a dictionary requires large amount of space, and may stain systems with limited GPU memory. Here, we used approaches inspired by quantum computing to propose two related methods, word2ket and word2ketXS, for storing word embedding matrix during training and inference in a highly efficient way. Our approach achieves a hundred-fold or more reduction in the space required to store the embeddings with almost no relative drop in accuracy in practical natural language processing tasks.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'deep learning natural language processing models often use vector word embeddings to represent words . a discrete sequence of words can be much more easily integrated with downstream neural layers if it is represented as a sequence of continuous vectors . also semantic relationships between words, learned from a text corpus, can be encoded in the relative configurations of the embedded vectors, . however, storing and accessing embeddeding vectors for all words in a dictionary requires large amount of space .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 36/1992 [02:03<2:07:14,  3.90s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Adam is shown not being able to converge to the optimal solution in certain cases. Researchers recently propose several algorithms to avoid the issue of non-convergence of Adam, but their efficiency turns out to be unsatisfactory in practice. In this paper, we provide a new insight into the non-convergence issue of Adam as well as other adaptive learning rate methods. We argue that there exists an inappropriate correlation between gradient $g_t$ and the second moment term $v_t$ in Adam ($t$ is the timestep), which results in that a large gradient is likely to have small step size while a small gradient may have a large step size. We demonstrate that such unbalanced step sizes are the fundamental cause of non-convergence of Adam, and we further prove that decorrelating $v_t$ and $g_t$ will lead to unbiased step size for each gradient, thus solving the non-convergence problem of Adam. Finally, we\n[{'summary_text': 'researchers recently propose several algorithms to avoid the issue of non-convergence of Adam . but their efficiency turns out to be unsatisfactory in practice . weAdam is shown not being able to converge to the optimal solution in certain cases .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 37/1992 [02:07<2:08:34,  3.95s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In some important computer vision domains, such as medical or hyperspectral imaging, we care about the classification of tiny objects in large images. However, most Convolutional Neural Networks (CNNs) for image classification were developed using biased datasets that contain large objects, in mostly central image positions. To assess whether classical CNN architectures work well for tiny object classification we build a comprehensive testbed containing two datasets: one derived from MNIST digits and one from histopathology images. This testbed allows controlled experiments to stress-test CNN architectures with a broad spectrum of signal-to-noise ratios. Our observations indicate that: (1) There exists a limit to signal-to-noise below which CNNs fail to generalize and that this limit is affected by dataset size - more data leading to better performances; however, the amount of training data required for the model to generalize scales rapidly with the inverse of the object-to-image ratio (2) in general, higher capacity models\n[{'summary_text': 'some important computer vision domains, such as medical or hyperspectral imaging, we care about the classification of tiny objects in large images . but most Convolutional Neural Networks (CNNs) for image classification were developed using biased datasets that contain large objects, in mostly central image positions . this testbed allows controlled experiments to stress-test CNN architectures with a broad spectrum of signal-to-noise ratios .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 38/1992 [02:11<2:12:05,  4.06s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We study the robust one-bit compressed sensing problem whose goal is to design an algorithm that faithfully recovers any sparse target vector\n $\\theta_0\\in\\mathbb{R}^d$ \\emph{uniformly} from $m$ quantized noisy measurements. Under the assumption that the measurements are sub-Gaussian,  to recover any $k$-sparse $\\theta_0$ ($k\\ll d$) \\emph{uniformly} up to an error $\\varepsilon$ with high probability, the best known computationally tractable algorithm requires\\footnote{Here, an algorithm is ``computationally tractable'' if it has provable convergence guarantees. The notation $\\tilde{\\mathcal{O}}(\\cdot)$ omits a logarithm factor of $\\varepsilon^{-1}$. } $m\\geq\\tilde{\\mathcal{O}}(k\n[{'summary_text': \"the notation $tildemathcalO(cdot)$ omits a logarithm factor of $varepsilon-1$ . the best known computationally tractable algorithm requiresfootnoteHere, an algorithm is computationally tractable'' if it has provable convergence guarantees.\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 39/1992 [02:15<2:14:12,  4.12s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In multiagent systems (MASs), each agent makes individual decisions but all of them contribute globally to the system evolution. Learning in MASs is difficult since each agent's selection of actions must take place in the presence of other co-learning agents. Moreover, the environmental stochasticity and uncertainties increase exponentially with the increase in the number of agents. Previous works borrow various multiagent coordination mechanisms into deep learning architecture to facilitate multiagent coordination. However, none of them explicitly consider action semantics between agents that different actions have different influences on other agents. In this paper, we propose a novel network architecture, named Action Semantics Network (ASN), that explicitly represents such action semantics between agents. ASN characterizes different actions' influence on other agents using neural networks based on the action semantics between them. ASN can be easily combined with existing deep reinforcement learning (DRL) algorithms to boost their performance. Experimental results on StarCraft II micromanagement and Neural MMO show ASN significantly improves the performance\n[{'summary_text': \"learning in MASs is difficult since each agent's selection of actions must take place in the presence of other co-learning agents . the environmental stochasticity and uncertainties increase exponentially with the increase in the number of agents. in this paper, we propose a novel network architecture, named Action Semantics Network (ASN), that explicitly represents such action semantics between agents.\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 40/1992 [02:19<2:14:58,  4.15s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Adversarial examples have somewhat disrupted the enormous success of machine learning (ML) and are causing concern with regards to its trustworthiness: A small perturbation of an input results in an arbitrary failure of an otherwise seemingly well-trained ML system. While studies are being conducted to discover the intrinsic properties of adversarial examples, such as their transferability and universality, there is insufficient theoretic analysis to help understand the phenomenon in a way that can influence the design process of ML experiments. In this paper, we deduce an information-theoretic model which explains adversarial attacks universally as the abuse of feature redundancies in ML algorithms. We prove that feature redundancy is a necessary condition for the existence of adversarial examples. Our model helps to explain the major questions raised in many anecdotal studies on adversarial examples. Our theory is backed up by empirical measurements of the information content of benign and adversarial examples on both image and text datasets. Our measurements show that typical adversarial examples introduce just enough\n[{'summary_text': 'a small perturbation of an input results in an arbitrary failure of an otherwise seemingly well-trained ML system . studies are being conducted to discover the intrinsic properties of adversarial examples, such as their transferability and universality . there is insufficient theoretic analysis to help understand the phenomenon in a way that can influence the design process of ML experiments .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 41/1992 [02:24<2:15:26,  4.17s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose an approach for sequence modeling based on autoregressive normalizing flows. Each autoregressive transform, acting across time, serves as a moving reference frame for modeling higher-level dynamics. This technique provides a simple, general-purpose method for improving sequence modeling, with connections to existing and classical techniques. We demonstrate the proposed approach both with standalone models, as well as a part of larger sequential latent variable models. Results are presented on three benchmark video datasets, where flow-based dynamics improve log-likelihood performance over baseline models.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'each autoregressive transform serves as a moving reference frame for modeling higher-level dynamics . results are presented on three benchmark video datasets, where flow-based dynamics improve log-likelihood performance over baseline models .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 42/1992 [02:27<2:07:42,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Beyond understanding what is being discussed, human communication requires an awareness of what someone is feeling. One challenge for dialogue agents is recognizing feelings in the conversation partner and replying accordingly, a key communicative skill that is trivial for humans. Research in this area is made difficult by the paucity of suitable publicly available datasets both for emotion and dialogues. This work proposes a new task for empathetic dialogue generation and EmpatheticDialogues, a dataset of 25k conversations grounded in emotional situations to facilitate training and evaluating dialogue systems. Our experiments indicate that dialogue models that use our dataset are perceived to be more empathetic by human evaluators, while improving on other metrics as well (e.g. perceived relevance of responses, BLEU scores), compared to models merely trained on large-scale Internet conversation data. We also present empirical comparisons of several ways to improve the performance of a given model by leveraging existing models or datasets without requiring lengthy re-training of the full model.<|endoftext|>\n[{'summary_text': 'research in this area is made difficult by the paucity of suitable publicly available datasets both for emotion and dialogues . this work proposes a new task for empathetic dialogue generation and EmpatheticDialogues, a dataset of 25k conversations grounded in emotional situations .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 43/1992 [02:29<1:48:13,  3.33s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks---as function approximators, and logic programming---as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers.   After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the Neural Logic Machine (NLM) is a neural-symbolic architecture for inductive learning and logic reasoning . NLMs exploit the power of both neural networks---as function approximators, and logic programming --as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers . most of these tasks are hard to accomplish for neural networks or inductive logic programming alone .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 44/1992 [02:33<1:54:32,  3.53s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"When an image classifier makes a prediction, which parts of the image are relevant and why? We can rephrase this question to ask: which parts of the image, if they were not seen by the classifier, would most change its decision? Producing an answer requires marginalizing over images that could have been seen but weren't. We can sample plausible image in-fills by conditioning a generative model on the rest of the image. We then optimize to find the image regions that most change the classifier's decision after in-fill. Our approach contrasts with ad-hoc in-filling approaches, such as blurring or injecting noise, which generate inputs far from the data distribution, and ignore informative relationships between different parts of the image. Our method produces more compact and relevant saliency maps, with fewer artifacts compared to previous methods.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': \"we can sample plausible image in-fills by conditioning a generative model on the rest of the image . we then optimize to find the image regions that most change the classifier's decision after infill . our approach contrasts with ad-hoc infilling approaches, such as blurring or injecting noise .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 45/1992 [02:37<1:55:46,  3.57s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The inference of models, prediction of future symbols, and entropy rate estimation of discrete-time, discrete-event processes is well-worn ground. However, many time series are better conceptualized as continuous-time, discrete-event processes. Here, we provide new methods for inferring models, predicting future symbols, and estimating the entropy rate of continuous-time, discrete-event processes. The methods rely on an extension of Bayesian structural inference that takes advantage of neural network’s universal approximation power. Based on experiments with simple synthetic data, these new methods seem to be competitive with state-of- the-art methods for prediction and entropy rate estimation as long as the correct model is inferred.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the methods rely on an extension of Bayesian structural inference that takes advantage of neural network’s universal approximation power . these new methods seem to be competitive with state-of-the-art methods for prediction and entropy rate estimation as long as the correct model is inferred .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 46/1992 [02:40<1:56:58,  3.61s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The fields of artificial intelligence and neuroscience have a long history of fertile bi-directional interactions. On the one hand, important inspiration for the development of artificial intelligence systems has come from the study of natural systems of intelligence, the mammalian neocortex in particular. On the other, important inspiration for models and theories of the brain have emerged from artificial intelligence research. A central question at the intersection of these two areas is concerned with the processes by which neocortex learns, and the extent to which they are analogous to the back-propagation training algorithm of deep networks. Matching the data efficiency, transfer and generalisation properties of neocortical learning remains an area of active research in the field of deep learning. Recent advances in our understanding of neuronal, synaptic and dendritic physiology of the neocortex suggest new approaches for unsupervised representation learning, perhaps through a new class of objective functions, which could act alongside or in lieu of back-propagation. Such local learning rules have implicit rather\n[{'summary_text': 'important inspiration for the development of artificial intelligence systems has come from the study of natural systems of intelligence . a central question at the intersection of these two areas is concerned with the processes by which neocortex learns, and the extent to which they are analogous to the back-propagation training algorithm .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 47/1992 [02:44<2:01:13,  3.74s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The prohibitive energy cost of running high-performance Convolutional Neural Networks (CNNs) has been limiting their deployment on resource-constrained platforms including mobile and wearable devices. We propose a CNN for energy-aware dynamic routing, called the EnergyNet, that achieves adaptive-complexity inference based on the inputs, leading to an overall reduction of run time energy cost without noticeably losing (or even improving) accuracy. That is achieved by proposing an energy loss that captures both computational and data movement costs. We combine it with the accuracy-oriented loss, and learn a dynamic routing policy for skipping certain layers in the networks, that optimizes the hybrid loss.   Our empirical results demonstrate that, compared to the baseline CNNs, EnergyNetcan trim down the energy cost up to 40% and 65%, during inference on the CIFAR10 and Tiny ImageNet testing sets, respectively, while maintaining the same testing accuracies.   It is further encouraging to observe that the energy awareness\n[{'summary_text': 'prohibitive energy cost of running high-performance Convolutional Neural Networks (CNNs) has been limiting their deployment on resource-constrained platforms including mobile and wearable devices . we propose a CNN for energy-aware dynamic routing, called the EnergyNet, that achieves adaptive-complexity inference based on the inputs . that is achieved by proposing an energy loss that captures both computational and data movement costs .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 48/1992 [02:49<2:06:42,  3.91s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Partially observable Markov decision processes (POMDPs) are a natural model for scenarios where one has to deal with incomplete knowledge and random events.\n Applications include, but are not limited to, robotics and motion planning.\n However, many relevant properties of POMDPs are either undecidable or very expensive to compute in terms of both runtime and memory consumption.\n In our work, we develop a game-based abstraction method that is able to deliver safe bounds and tight\n  approximations for important sub-classes of such properties.\n We discuss the theoretical implications and showcase the applicability of our results on a broad spectrum of benchmarks.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'applications include, but are not limited to, robotics and motion planning . many relevant properties of POMDPs are either undecidable or very expensive to compute in terms of both runtime and memory consumption . we discuss the theoretical implications and showcase the applicability of our results on a broad spectrum of benchmarks .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   2%|▏         | 49/1992 [02:52<2:03:16,  3.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present Line-Storm, an interactive computer system for creative performance. The context we investigated was writing on paper using Line-Storm. We used self-report questionnaires as part of research involving human participants, to evaluate Line-Storm. Line-Storm consisted of a writing stylus and writing pad, augmented with electronics. The writing pad was connected to a contact microphone, and the writing stylus had a small micro-controller board and peripherals attached to it. The signals from these electronic augmentations were fed into the audio-synthesis environment Max/MSP to produce an interactive soundscape. We attempted to discover whether Line-Storm enhanced a self-reported sense of being present and engaged during a writing task, and we compared Line-Storm to a non-interactive control condition. After performing statistical analysis in SPSS, we were unable to support our research hypothesis, that presence and engagement were enhanced by Line-Storm. Participants reported they were, on average, no more\n[{'summary_text': 'line-Storm consisted of a writing stylus and writing pad, augmented with electronics . the writing pad was connected to a contact microphone, and the pen stylus had a small micro-controller board and peripherals attached to it . participants reported they were, on average, no more than sexualienced .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 50/1992 [02:56<2:06:14,  3.90s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We study the precise mechanisms which allow autoencoders to encode and decode a simple geometric shape, the disk. In this carefully controlled setting, we are able to describe the specific form of the optimal solution to the minimisation problem of the training step. We show that the autoencoder indeed approximates this solution during training. Secondly, we identify a clear failure in the generalisation capacity of the autoencoder, namely its inability to interpolate data. Finally, we explore several regularisation schemes to resolve the generalisation problem. Given the great attention that has been recently given to the generative capacity of neural networks, we believe that studying in depth simple geometric cases sheds some light on the generation process and can provide a minimal requirement experimental setup for more complex architectures.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the autoencoder indeed approximates this solution during training . in this carefully controlled setting, we are able to describe the specific form of the optimal solution to the minimisation problem of the training step.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 51/1992 [02:58<1:48:44,  3.36s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Bayesian optimization (BO) is a popular methodology to tune the hyperparameters of expensive black-box functions. Despite its success, standard BO focuses on a single task at a time and is not designed to leverage information from related functions, such as tuning performance metrics of the same algorithm across multiple datasets. In this work, we introduce a novel approach to achieve transfer learning across different datasets as well as different metrics. The main idea is to regress the mapping from hyperparameter to metric quantiles with a semi-parametric Gaussian Copula distribution, which provides robustness against different scales or outliers that can occur in different tasks. We introduce two methods to leverage this estimation: a Thompson sampling strategy as well as a Gaussian Copula process using such quantile estimate as a prior. We show that these strategies can combine the estimation of multiple metrics such as runtime and accuracy, steering the optimization toward cheaper hyperparameters for the same level of accuracy. Experiments on an extensive set of hyper\n[{'summary_text': 'despite its success, standard BO focuses on a single task at a time . it is not designed to leverage information from related functions, such as tuning performance metrics of the same algorithm across multiple datasets . the main idea is to regress the mapping from hyperparameter to metric quantiles with a semi-parametric Gaussian Copula distribution .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 52/1992 [03:03<1:56:37,  3.61s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Unsupervised and semi-supervised learning are important problems that are especially challenging with complex data like natural images. Progress on these problems would accelerate if we had access to appropriate generative models under which to pose the associated inference tasks. Inspired by the success of Convolutional Neural Networks (CNNs) for supervised prediction in images, we design the Neural Rendering Model (NRM), a new hierarchical probabilistic generative model whose inference calculations correspond to those in a CNN. The NRM introduces a small set of latent variables at each level of the model and enforces dependencies among all the latent variables via a conjugate prior distribution. The conjugate prior yields a new regularizer for learning based on the paths rendered in the generative model for training CNNs–the Rendering Path Normalization (RPN). We demonstrate that this regularizer improves generalization both in theory and in practice. Likelihood estimation in the NRM yields the new Max-Min cross entropy training loss\n[{'summary_text': 'unsupervised and semi-supervised learning are important problems that are especially challenging with complex data like natural images . progress on these problems would accelerate if we had access to appropriate generative models under which to pose the associated inference tasks . the NRM introduces a small set of latent variables at each level of the model .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 53/1992 [03:07<2:00:09,  3.72s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others . we show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 54/1992 [03:10<1:59:30,  3.70s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Sentiment classification is an active research area with several applications including analysis of political opinions, classifying comments, movie reviews, news reviews and product reviews. To employ rule based sentiment classification, we require sentiment lexicons. However, manual construction of sentiment lexicon is time consuming and costly for resource-limited languages. To bypass manual development time and costs, we tried to build Amharic Sentiment Lexicons relying on corpus based approach. The intention of this approach is to handle sentiment terms specific to Amharic language from Amharic Corpus. Small set of seed terms are manually prepared from three parts of speech such as noun, adjective and verb. We developed algorithms for constructing Amharic sentiment lexicons automatically from Amharic news corpus. Corpus based approach is proposed relying on the word co-occurrence distributional embedding including frequency based embedding (i.e. Positive Point-wise Mutual Information PPMI). First we build word-context unigram frequency count matrix and transform\n[{'summary_text': 'Sentiment classification is an active research area with several applications including analysis of political opinions, classifying comments, movie reviews, news reviews and product reviews . to employ rule based sentiment classification, we require sentiment lexicons .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 55/1992 [03:14<2:02:05,  3.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The information bottleneck method provides an information-theoretic method for representation learning, by training an encoder to retain all information which is relevant for predicting the label, while minimizing the amount of other, superfluous information in the representation. The original formulation, however, requires labeled data in order to identify which information is superfluous.   In this work, we extend this ability to the multi-view unsupervised setting, in which two views of the same underlying entity are provided but the label is unknown. This enables us to identify superfluous information as that which is not shared by both views. A theoretical analysis leads to the definition of a new multi-view model that produces state-of-the-art results on the Sketchy dataset and on label-limited versions of the MIR-Flickr dataset.   We also extend our theory to the single-view setting by taking advantage of standard data augmentation techniques, empirically showing better generalization capabilities when compared to traditional unsuper\n[{'summary_text': 'the information bottleneck method provides an information-theoretic method for representation learning . the original formulation requires labeled data in order to identify which information is superfluous . a theoretical analysis leads to the definition of a new multi-view model that produces state-of-the-art results on the Sketchy dataset and on label-limited versions of the MIR-Flickr dataset .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 56/1992 [03:18<2:05:31,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep generative models such as Variational AutoEncoder (VAE) and Generative Adversarial Network (GAN) play an increasingly important role in machine learning and computer vision. However, there are two fundamental issues hindering their real-world applications: the difficulty of conducting variational inference in VAE and the functional absence of encoding real-world samples in GAN. In this paper, we propose a novel algorithm named Latently Invertible Autoencoder (LIA) to address the above two issues in one framework. An invertible network and its inverse mapping are symmetrically embedded in the latent space of VAE. Thus the partial encoder first transforms the input into feature vectors and then the distribution of these feature vectors is reshaped to fit a prior by the invertible network. The decoder proceeds in the reverse order of the encoder's composite mappings. A two-stage stochasticity-free training scheme is designed to train LIA via\n[{'summary_text': 'the difficulty of conducting variational inference in VAE and the functional absence of encoding real-world samples in GAN . in this paper, we propose a novel algorithm named Latently Invertible Autoencoder (LIA) to address the above two issues in one framework . an invertible network and its inverse mapping are symmetrically embedded in the latent space of VAE .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 57/1992 [03:23<2:08:52,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM, which uses DropConnect on hidden-to-hidden weights, as a form of recurrent regularization. Further, we introduce NT-ASGD, a non-monotonically triggered  (NT) variant of the averaged stochastic gradient method (ASGD), wherein the averaging trigger is determined using a NT condition as opposed to being tuned by the user. Using these and other regularization strategies, our ASGD Weight-Dropped LSTM (AWD-LSTM) achieves state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art\n[{'summary_text': 'we propose the weight-dropped LSTM, which uses DropConnect on hidden-to-hidden weights, as a form of recurrent regularization . we introduce NT-ASGD, a non-monotonically triggered (NT) variant of the averaged stochastic gradient method (ASGD), wherein the averaging trigger is determined using a NT condition as opposed to being tuned by the user.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 58/1992 [03:27<2:12:25,  4.11s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We show implicit filter level sparsity manifests in convolutional neural networks (CNNs) which employ Batch Normalization and ReLU activation, and are trained using adaptive gradient descent techniques with L2 regularization or weight decay. Through an extensive empirical study (Anonymous, 2019) we hypothesize the mechanism be hind the sparsification process. We find that the interplay  of  various  phenomena  influences  the strength of L2 and weight decay regularizers, leading the supposedly non sparsity inducing regularizers to induce filter sparsity.   In this workshop article we summarize some of our key findings and experiments, and present additional results on modern network architectures such as ResNet-50.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'implicit filter level sparsity manifests in convolutional neural networks (CNNs) which employ Batch Normalization and ReLU activation . we hypothesize the mechanism be hind the sparsification process . the interplay of various phenomena influences the strength of L2 and weight decay regularizers .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 59/1992 [03:31<2:07:35,  3.96s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Inferring the most likely configuration for a subset of variables of a joint distribution given the remaining ones – which we refer to as co-generation – is an important challenge that is computationally demanding for all but the simplest settings. This task has received a considerable amount of attention, particularly for classical ways of modeling distributions like structured prediction. In contrast, almost nothing is known about this task when considering recently proposed techniques for modeling high-dimensional distributions, particularly generative adversarial nets (GANs). Therefore, in this paper, we study the occurring challenges for co-generation with GANs. To address those challenges we develop an annealed importance sampling (AIS) based Hamiltonian Monte Carlo (HMC) co-generation algorithm. The presented approach significantly outperforms classical gradient-based methods on synthetic data and on CelebA.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'this task has received considerable amount of attention, particularly for classical ways of modeling distributions like structured prediction . almost nothing is known about this task when considering recently proposed techniques for modeling high-dimensional distributions, particularly generative adversarial nets (GANs)'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 60/1992 [03:34<2:03:30,  3.84s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We introduce the open-ended, modular, self-improving Omega AI unification architecture which is a refinement of Solomonoff's Alpha architecture, as considered from first principles. The architecture embodies several crucial principles of general intelligence including diversity of representations, diversity of data types, integrated memory, modularity, and higher-order cognition. We retain the basic design of a fundamental algorithmic substrate called an ``AI kernel'' for problem solving and basic cognitive functions like memory, and a larger, modular architecture that re-uses the kernel in many ways. Omega includes eight representation languages and six classes of neural networks, which are briefly introduced. The architecture is intended to initially address data science automation, hence it includes many problem solving methods for statistical tasks. We review the broad software architecture, higher-order cognition, self-improvement, modular neural architectures, intelligent agents, the process and memory hierarchy, hardware abstraction, peer-to-peer computing, and data abstraction facility.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we introduce the open-ended, modular, self-improving Omega AI unification architecture . the architecture embodies several crucial principles of general intelligence including diversity of representations, diversity of data types, integrated memory, modularity, and higher-order cognition . Omega includes eight representation languages and six classes of neural networks, which are briefly introduced .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 61/1992 [03:38<2:02:35,  3.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Which generative model is the most suitable for Continual Learning? This paper aims at evaluating and comparing generative models on disjoint sequential image generation tasks. We investigate how several models learn and forget, considering various strategies: rehearsal, regularization, generative replay and fine-tuning. We used two quantitative metrics to estimate the generation quality and memory ability. We experiment with sequential tasks on three commonly used benchmarks for Continual Learning (MNIST, Fashion MNIST and CIFAR10). We found that among all models, the original GAN performs best and among Continual Learning strategies, generative replay outperforms all other methods. Even if we found satisfactory combinations on MNIST and Fashion MNIST, training generative models sequentially on CIFAR10 is particularly instable, and remains a challenge.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'this paper aims at evaluating and comparing generative models on disjoint sequential image generation tasks . we investigate how several models learn and forget, considering various strategies: rehearsal, regularization, generative replay and fine-tuning .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 62/1992 [03:42<2:04:22,  3.87s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU). We give an algorithm to train a ReLU DNN with one hidden layer to {\\em global optimality} with runtime polynomial in the data size albeit exponential in the input dimension. Further, we improve on the known lower bounds on size (from exponential to super exponential) for approximating a ReLU deep net function by a shallower ReLU net. Our gap theorems hold for smoothly parametrized families of ``hard'' functions, contrary to countable, discrete families known in the literature.   An example consequence of our gap theorems is the following: for every natural number $k$ there exists a function representable by a ReLU DNN with $k^2$ hidden layers and total size $k^3$, such that any ReLU DNN with at most $k$ hidden layers will require at\n[{'summary_text': 'in this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU) we give an algorithm to train a ReLU DNN with one hidden layer to em global optimality with runtime polynomial in the data size albeit exponential in the input dimension .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 63/1992 [03:46<2:08:47,  4.01s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, this places on environment designers the onus of designing language-conditional reward functions which may not be easily or tractably implemented as the complexity of the environment and the language scales. To overcome this limitation, we present a framework within which instruction-conditional RL agents are trained using rewards obtained not from the environment, but from reward models which are jointly trained from expert examples.   As reward models improve, they learn to accurately reward agents for completing tasks for environment configurations---and for instructions---not present amongst the expert data. This framework effectively separates the representation of what instructions require from how they can be executed.\n In a simple grid world, it enables an agent to learn a range of commands requiring interaction with blocks and understanding of spatial relations and underspecified abstract arrangements. We further show the method allows our agent to adapt to changes in the environment without requiring new\n[{'summary_text': 'deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards . however, this places on environment designers the onus of designing language-conditional reward functions which may not be easily or tractably implemented as the complexity of the environment and the language scales . as reward models improve, they learn to accurately reward agents for completing tasks for environment configurations---and for instructions---not present amongst the expert data.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 64/1992 [03:51<2:11:23,  4.09s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy.   Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes large step updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent . achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work . our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 65/1992 [03:54<2:09:24,  4.03s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Implementing correct method invocation is an important task for software developers. However, this is challenging work, since the structure of method invocation can be complicated. In this paper, we propose InvocMap, a code completion tool allows developers to obtain an implementation of multiple method invocations from a list of method names inside code context. InvocMap is able to predict the nested method invocations which their names didn’t appear in the list of input method names given by developers. To achieve this, we analyze the Method Invocations by four levels of abstraction. We build a Machine Translation engine to learn the mapping from the first level to the third level of abstraction of multiple method invocations, which only requires developers to manually add local variables from generated expression to get the final code. We evaluate our proposed approach on six popular libraries: JDK, Android, GWT, Joda-Time, Hibernate, and Xstream. With the training corpus of 2.86 million method inv\n[{'summary_text': 'invocMap is able to predict the nested method invocations which their names didn’t appear in the list of input method names given by developers . to achieve this, we analyze the Method Invocations by four levels of abstraction . we evaluate our proposed approach on six popular libraries: JDK, Android, GWT, Joda-Time, Hibernate, and Xstream.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 66/1992 [03:59<2:11:51,  4.11s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"This work investigates unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality in the input into the objective can significantly improve a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and compares favorably with fully-supervised learning on several classification tasks in with some standard architectures. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation learning objectives for specific end-goals.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': \"this work investigates unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder . incorporating knowledge about locality in the input into the objective can significantly improve a representation's suitability for downstream tasks .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 67/1992 [04:02<2:05:55,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present a simple nearest-neighbor (NN) approach that synthesizes high-frequency photorealistic images from an ``incomplete'' signal such as a low-resolution image, a surface normal map, or edges. Current state-of-the-art deep generative models designed for such conditional image synthesis lack two important things: (1) they are unable to generate a large set of diverse outputs, due to the mode collapse problem. (2) they are not interpretable, making it difficult to control the synthesized output. We demonstrate that NN approaches potentially address such limitations, but suffer in accuracy on small datasets. We design a simple pipeline that combines the best of both worlds:  the first stage uses a convolutional neural network (CNN) to map the input to a (overly-smoothed) image, and the second stage uses a pixel-wise nearest neighbor method to map the smoothed output to multiple high-quality, high-frequency outputs\n[{'summary_text': 'current state-of-the-art deep generative models designed for such conditional image synthesis lack two important things: (1) they are unable to generate a large set of diverse outputs, due to the mode collapse problem . (2) they are not interpretable, making it difficult to control the synthesized output . we design a simple pipeline that combines the best of both worlds .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 68/1992 [04:06<2:08:05,  3.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We argue that symmetry is an important consideration in addressing the problem\n of systematicity and investigate two forms of symmetry relevant to symbolic processes. \n We implement this approach in terms of convolution and show that it can\n be used to achieve effective generalisation in three toy problems: rule learning,\n composition and grammar learning.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we argue that symmetry is an important consideration in addressing the problem of systematicity . we implement this approach in terms of convolution and show that it can be used to achieve effective generalisation in three toy problems .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   3%|▎         | 69/1992 [04:10<2:01:47,  3.80s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In multi-agent systems, complex interacting behaviors arise due to the high correlations among agents. However, previous work on modeling multi-agent interactions from demonstrations is primarily constrained by assuming the independence among policies and their reward structures. \n In this paper, we cast the multi-agent interactions modeling problem into a multi-agent imitation learning framework with explicit modeling of correlated policies by approximating opponents’ policies, which can recover agents' policies that can regenerate similar interactions. Consequently, we develop a Decentralized Adversarial Imitation Learning algorithm with Correlated policies (CoDAIL), which allows for decentralized training and execution. Various experiments demonstrate that CoDAIL can better regenerate complex interactions close to the demonstrators and outperforms state-of-the-art multi-agent imitation learning methods. Our code is available at \\url{https://github.com/apexrl/CoDAIL}.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'previous work on modeling multi-agent interactions from demonstrations is primarily constrained by assuming the independence among policies and their reward structures . in this paper, we cast the modeling problem into an imitation learning framework with explicit modeling of correlated policies by approximating opponents’ policies . we develop a Decentralized Adversarial Imitation Learning algorithm with Correlated policies (CoDAIL), which allows for decentralized training and execution .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▎         | 70/1992 [04:14<2:03:57,  3.87s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Meta-learning algorithms learn to acquire new tasks more quickly from past experience. In the context of reinforcement learning, meta-learning algorithms can acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta-learning methods generalize best to tasks from the same distribution as the meta-training tasks. In effect, meta-reinforcement learning offloads the design burden from algorithm design to task design. If we can automate the process of task design as well, we can devise a meta-learning algorithm that is truly automated. In this work, we take a step in this direction, proposing a family of unsupervised meta-learning algorithms for reinforcement learning. We motivate and describe a general recipe for unsupervised meta-reinforcement learning, and present an instantiation\n[{'summary_text': 'meta-learning algorithms can acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks . supervised learning generalizes best to test points drawn from the same distribution as the training points . in effect, meta-reinforcement learning offloads the design burden from algorithm design to task design .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▎         | 71/1992 [04:18<2:05:15,  3.91s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Our work offers a new method for domain translation from semantic label maps\n and Computer Graphic (CG) simulation edge map images to photo-realistic im-\n ages. We train a Generative Adversarial Network (GAN) in a conditional way to\n generate a photo-realistic version of a given CG scene. Existing architectures of\n GANs still lack the photo-realism capabilities needed to train DNNs for computer\n vision tasks, we address this issue by embedding edge maps, and training it in an\n adversarial mode. We also offer an extension to our model that uses our GAN\n architecture to create visually appealing and temporally coherent videos.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'our work offers a new method for domain translation from semantic label maps and Computer Graphic (CG) simulation edge map images to photo-realistic im- ages . existing architectures of GANs still lack the photo-realism capabilities needed to train DNNs for computer vision tasks .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▎         | 72/1992 [04:21<2:01:48,  3.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Capsule Networks have shown encouraging results on defacto benchmark computer vision datasets such as MNIST, CIFAR and smallNORB. Although, they are yet to be tested on tasks where (1) the entities detected inherently have more complex internal representations and (2) there are very few instances per class to learn from and (3) where point-wise classification is not suitable. Hence, this paper carries out experiments on face verification in both controlled and uncontrolled settings that together address these points. In doing so we introduce Siamese Capsule Networks, a new variant that can be used for pairwise learning tasks. The model is trained using contrastive loss with l2-normalized capsule encoded pose features. We find that Siamese Capsule Networks perform well against strong baselines on both pairwise learning datasets, yielding best results in the few-shot learning setting where image pairs in the test set contain unseen subjects.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Capsule Networks have shown encouraging results on defacto benchmark computer vision datasets such as MNIST, CIFAR and smallNORB . although they are yet to be tested on tasks where (1) the entities detected inherently have more complex internal representations and (2) there are very few instances per class to learn from .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▎         | 73/1992 [04:25<2:01:00,  3.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We apply canonical forms of gradient complexes (barcodes) to explore neural networks loss surfaces. We present an algorithm for calculations of the objective function's barcodes of minima.   Our experiments confirm two principal observations: (1) the barcodes of minima are located in a small lower part of the range of values of objective function and (2) increase of the neural network's depth brings down the minima's barcodes. This has natural implications for the neural network learning and the ability to generalize.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the barcodes of minima are located in a small lower part of the range of values of objective function . this has natural implications for the neural network learning and the ability to generalize .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▎         | 74/1992 [04:29<1:58:44,  3.71s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Learning an efficient update rule from data that promotes rapid learning of new tasks from the same distribution remains an open problem in meta-learning. Typically, previous works have approached this issue either by attempting to train a neural network that directly produces updates or by attempting to learn better initialisations or scaling factors for a gradient-based update rule. Both these approaches pose challenges. On one hand, directly producing an update forgoes a useful inductive bias and can easily lead to non-converging behaviour. On the other hand, approaches that try to control a gradient-based update rule typically resort to computing gradients through the learning process to obtain their meta-gradients, leading to methods that can not scale beyond few-shot task adaptation. In this work we propose Warped Gradient Descent (WarpGrad), a method that intersects these approaches to mitigate their limitations. WarpGrad meta-learns an efficiently parameterised preconditioning matrix that facilitates gradient descent across the task distribution.\n[{'summary_text': 'previous works have approached this issue either by attempting to train a neural network that directly produces updates . directly producing an update forgoes a useful inductive bias and can easily lead to non-converging behaviour . in this work we propose Warped Gradient Descent, a method that intersects these approaches to mitigate their limitations .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 75/1992 [04:33<2:02:31,  3.83s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Search space is a key consideration for neural architecture search. Recently, Xie et al. (2019a) found that randomly generated networks from the same distribution perform similarly, which suggest we should search for random graph distributions instead of graphs. We propose graphon as a new search space. A graphon is the limit of Cauchy sequence of graphs and a scale-free probabilistic distribution, from which graphs of different number of vertices can be drawn. This property enables us to perform NAS using fast, low-capacity models and scale the found models up when necessary. We develop an algorithm for NAS in the space of graphons and empirically demonstrate that it can find stage-wise graphs that outperform DenseNet and other baselines on ImageNet.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Xie et al. (2019a) found that randomly generated networks from the same distribution perform similarly . this suggests we should search for random graph distributions instead of graphs . graphon is the limit of Cauchy sequences and a scale-free probabilistic distribution, from which graphs of different number of vertices can be drawn .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 76/1992 [04:36<2:01:47,  3.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Positive-unlabeled (PU) learning addresses the problem of learning a binary classifier from positive (P) and unlabeled (U) data. It is often applied to situations where negative (N) data are difficult to be fully labeled. However, collecting a non-representative N set that contains only a small portion of all possible N data can be much easier in many practical situations. This paper studies a novel classification framework which incorporates such biased N (bN) data in PU learning. The fact that the training N data are biased also makes our work very different from those of standard semi-supervised learning. We provide an empirical risk minimization-based method to address this PUbN classification problem. Our approach can be regarded as a variant of traditional example-reweighting algorithms, with the weight of each example computed through a preliminary step that draws inspiration from PU learning. We also derive an estimation error bound for the proposed method. Experimental results demonstrate the effectiveness of\n[{'summary_text': 'positive-unlabeled (PU) learning addresses the problem of learning a binary classifier from positive (P) and unlabed (U) data . it is often applied to situations where negative (N) data are difficult to be fully labeled . however, collecting a non-representative N set that contains only a small portion of all possible N data can be much easier in many practical situations .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 77/1992 [04:41<2:06:25,  3.96s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"LSTM-based language models exhibit compositionality in their representations, but how this behavior emerges over the course of training has not been explored. Analyzing synthetic data experiments with contextual decomposition, we find that LSTMs learn long-range dependencies compositionally by building them from shorter constituents during training.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'LSTM-based language models exhibit compositionality in their representations . how this behavior emerges over the course of training has not been explored . analysis of synthetic data experiments with contextual decomposition .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 78/1992 [04:44<2:02:46,  3.85s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We outline new approaches to incorporate ideas from deep learning into wave-based least-squares imaging. The aim, and main contribution of this work, is the combination of handcrafted constraints with deep convolutional neural networks, as a way to harness their remarkable ease of generating natural images. The mathematical basis underlying our method is the expectation-maximization framework, where data are divided in batches and coupled to additional \"latent\" unknowns. These unknowns are pairs of elements from the original unknown space (but now coupled to a specific data batch) and network inputs. In this setting, the neural network controls the similarity between these additional parameters, acting as a \"center\" variable. The resulting problem amounts to a maximum-likelihood estimation of the network parameters when the augmented data model is marginalized over the latent variables.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we outline new approaches to incorporate ideas from deep learning into wave-based least-squares imaging . the aim, and main contribution of this work, is the combination of handcrafted constraints with deep convolutional neural networks, as a way to harness their remarkable ease of generating natural images .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 79/1992 [04:48<2:00:13,  3.77s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"While deep neural networks are a highly successful model class, their large memory footprint puts considerable strain on energy consumption, communication bandwidth, and storage requirements. Consequently, model size reduction has become an utmost goal in deep learning. A typical approach is to train a set of deterministic weights, while applying certain techniques such as pruning and quantization, in order that the empirical weight distribution becomes amenable to Shannon-style coding schemes. However, as shown in this paper, relaxing weight determinism and using a full variational distribution over weights allows for more efficient coding schemes and consequently higher compression rates. In particular, following the classical bits-back argument, we encode the network weights using a random sample, requiring only a number of bits corresponding to the Kullback-Leibler divergence between the sampled variational distribution and the encoding distribution. By imposing a constraint on the Kullback-Leibler divergence, we are able to explicitly control the compression rate, while optimizing the expected loss on the training\n[{'summary_text': 'deep neural networks are a highly successful model class, but their large memory footprint puts considerable strain on energy consumption, communication bandwidth, and storage requirements. Consequently, model size reduction has become an utmost goal in deep learning . a typical approach is to train a set of deterministic weights, while applying certain techniques such as pruning and quantization, in order that the empirical weight distribution becomes amenable to Shannon-style coding schemes .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 80/1992 [04:52<2:05:42,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pre-trained ResNet. An attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations. In particular, the proposed method enables the visual information to be integrated into large-scale text-only NMT in addition to the multimodel NMT. Experiments on four widely used translation datasets,\n[{'summary_text': 'a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs . an attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 81/1992 [04:56<2:06:20,  3.97s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Board games often rely on visual information such as the location of the game pieces and textual information on cards. Due to this reliance on visual feedback, blind players are at a disadvantage because they cannot read the cards or see the location of the game pieces and may be unable to play a game without sighted help. We present Game Changer, an augmented workspace that provides both audio descriptions and tactile additions to make the state of the board game accessible to blind and visually impaired players. In this paper, we describe the design of Game Changer and present findings from a user study in which 7 blind participants used Game Changer to play against a sighted partner. Most players stated the game was more accessible with the additions from Game Changer and felt that Game Changer could be used to augment other games.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'blind players are at a disadvantage because they cannot read the cards or see the location of the game pieces and may be unable to play a game without sighted help . we present Game Changer, an augmented workspace that provides both audio descriptions and tactile additions to make the state of the board game accessible to blind and visually impaired players .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 82/1992 [05:00<2:06:44,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"This work presents the Poincaré Wasserstein Autoencoder, a reformulation of\n the recently proposed Wasserstein autoencoder framework on a non-Euclidean\n manifold, the Poincaré ball model of the hyperbolic space H n . By assuming the\n latent space to be hyperbolic, we can use its intrinsic hierarchy to impose structure\n on the learned latent space representations. We show that for datasets with latent\n hierarchies, we can recover the structure in a low-dimensional latent space. We\n also demonstrate the model in the visual domain to analyze some of its properties\n and show competitive results on a graph link prediction task.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'this work presents the Poincaré Wasserstein Autoencoder, a reformulation of the recently proposed Wasserstein framework on a non-euclidean manifold . we show that for datasets with latent hierarchies, we can recover the structure in a low-dimensional latent space . also demonstrate the model in the visual domain to analyze some of its properties .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 83/1992 [05:04<2:05:42,  3.95s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Data parallelism has become a dominant method to scale Deep Neural Network (DNN) training across multiple nodes.   Since the synchronization of the local models or gradients can be a bottleneck for large-scale distributed training, compressing communication traffic has gained widespread attention recently.   Among several recent proposed compression algorithms, \nResidual Gradient Compression (RGC) is one of the most successful approaches---it can significantly compress the transmitting message size (0.1% of the gradient size) of each node and still preserve accuracy. However, the literature on compressing deep networks focuses almost exclusively on achieving good compression rate, while the efficiency of RGC in real implementation has been less investigated. In this paper, we develop an RGC method that achieves significant training time improvement in real-world multi-GPU systems. Our proposed RGC system design called RedSync, introduces a set of optimizations to reduce communication bandwidth while introducing limited overhead. We examine the performance of RedSync on\n[{'summary_text': 'synchronization of the local models or gradients can be a bottleneck for large-scale distributed training, compressing communication traffic has gained widespread attention recently . a proposed RGC system design called RedSync introduces a set of optimizations to reduce communication bandwidth while introducing limited overhead .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 84/1992 [05:08<2:05:18,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Understanding object motion is one of the core problems in computer vision. It requires segmenting and tracking objects over time. Significant progress has been made in instance segmentation, but such models cannot track objects, and more crucially, they are unable to reason in both 3D space and time.\n We propose a new spatio-temporal embedding loss on videos that generates temporally consistent video instance segmentation. Our model includes a temporal network that learns to model temporal context and motion, which is essential to produce smooth embeddings over time. Further, our model also estimates monocular depth, with a self-supervised loss, as the relative distance to an object effectively constrains where it can be next, ensuring a time-consistent embedding. Finally, we show that our model can accurately track and segment instances, even with occlusions and missed detections, advancing the state-of-the-art on the KITTI Multi-Object and Tracking Dataset\n[{'summary_text': 'significant progress has been made in instance segmentation . but such models cannot track objects, and more crucially, they are unable to reason in both 3D space and time . we propose a new spatio-temporal embedding loss on videos .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 85/1992 [05:12<2:05:19,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generative neural networks map a standard, possibly distribution to a complex high-dimensional distribution, which represents the real world data set. However, a determinate input distribution as well as a specific architecture of neural networks may impose limitations on capturing the diversity in the high dimensional target space. To resolve this difficulty, we propose a training framework that greedily produce a series of generative adversarial networks that incrementally capture the diversity of the target space. We show theoretically and empirically that our training algorithm converges to the theoretically optimal distribution, the projection of the real distribution onto the convex hull of the network's distribution space.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'a determinate input distribution as well as a specific architecture of neural networks may impose limitations on capturing the diversity in the high dimensional target space . to resolve this difficulty, we propose a training framework that greedily produce a series of generative adversarial networks that incrementally capture the diversity of the target space.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 86/1992 [05:16<2:02:59,  3.87s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': \"our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications . then we propose a new dataset called imageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations . Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations.\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 87/1992 [05:20<2:04:22,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep neural networks trained on large supervised datasets have led to impressive results in recent years. However, since well-annotated datasets can be prohibitively expensive and time-consuming to collect, recent work has explored the use of larger but noisy datasets that can be more easily obtained. In this paper, we investigate the behavior of deep neural networks on training sets with massively noisy labels. We show on multiple datasets such as MINST, CIFAR-10 and ImageNet that successful learning is possible even with an essentially arbitrary amount of noise. For example, on MNIST we find that accuracy of above 90 percent is still attainable even when the dataset has been diluted with 100 noisy examples for each clean example. Such behavior holds across multiple patterns of label noise, even when noisy labels are biased towards confusing classes. Further, we show how the required dataset size for successful training increases with higher label noise. Finally, we present simple actionable techniques for improving learning in the regime of high label noise.<|endoftext|>\n[{'summary_text': 'deep neural networks trained on large supervised datasets have led to impressive results in recent years . but since well-annotated datasets can be prohibitively expensive and time-consuming to collect, recent work has explored the use of larger but noisy datasets that can be more easily obtained .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 88/1992 [05:23<2:01:21,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Self-training is one of the earliest and simplest semi-supervised methods. The key idea is to augment the original labeled dataset with unlabeled data paired with the model’s prediction. Self-training has mostly been well-studied to classification problems. However, in complex sequence generation tasks such as machine translation, it is still not clear how self-training woks due to the compositionality of the target space. In this work, we first show that it is not only possible but recommended to apply self-training in sequence generation. Through careful examination of the performance gains, we find that the noise added on the hidden states (e.g. dropout) is critical to the success of self-training, as this acts like a regularizer which forces the model to yield similar predictions for similar inputs from unlabeled data. To further encourage this mechanism, we propose to inject noise to the input space, resulting in a “noisy” version of self-\n[{'summary_text': 'self-training is one of the earliest and simplest semi-supervised methods . the key idea is to augment the original labeled dataset with unlabeled data paired with the model’s prediction . however, in complex sequence generation tasks such as machine translation, it is still not clear how self -training woks due to the compositionality of the target space . to further encourage this mechanism, we propose to inject noise to the input space, resulting in a “noisy” version of self- ('}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   4%|▍         | 89/1992 [05:28<2:08:28,  4.05s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Exploration while learning representations is one of the main challenges Deep\n Reinforcement Learning (DRL) faces today. As the learned representation is dependant in the observed data, the exploration strategy has a crucial role. The popular DQN algorithm has improved significantly the capabilities of Reinforcement\n Learning (RL) algorithms to learn state representations from raw data, yet, it uses\n a naive exploration strategy which is statistically inefficient. The Randomized\n Least Squares Value Iteration (RLSVI) algorithm (Osband et al., 2016), on the\n other hand, explores and generalizes efficiently via linearly parameterized value\n functions. However, it is based on hand-designed state representation that requires\n prior engineering work for every environment. In this paper, we propose a Deep\n Learning adaptation for RLSVI. Rather than using hand-design state representation, we use a state representation that is being learned directly from the data by a\n DQN agent. As the representation\n[{'summary_text': 'the popular DQN algorithm has improved significantly the capabilities of Reinforcement Learning (RL) algorithms to learn state representations from raw data . however, it is based on hand-designed state representation that requires prior engineering work for every environment . in this paper, we propose a Deep Learning adaptation for RLSVI .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▍         | 90/1992 [05:32<2:08:06,  4.04s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The complexity of large-scale neural networks can lead to poor understanding of their internal  details. We show that this opaqueness provides an opportunity for adversaries to embed unintended functionalities into the network in the form of Trojan horse attacks. Our novel framework hides the existence of a malicious network within a benign transport network. Our attack is flexible, easy to execute, and difficult to detect. We prove theoretically that the malicious network's detection is computationally infeasible and demonstrate empirically that the transport network does not compromise its disguise. Our attack exposes an important, previously unknown loophole that unveils a new direction in machine learning security.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': \"complex neural networks can lead to poor understanding of their internal details . our attack is flexible, easy to execute, and difficult to detect . we prove theoretically that the malicious network's detection is computationally infeasible .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▍         | 91/1992 [05:35<2:02:20,  3.86s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present a new approach to assessing the robustness of neural networks based on estimating the proportion of inputs for which a property is violated. Specifically, we estimate the probability of the event that the property is violated under an input model. Our approach critically varies from the formal verification framework in that when the property can be violated, it provides an informative notion of how robust the network is, rather than just the conventional assertion that the network is not verifiable. Furthermore, it provides an ability to scale to larger networks than formal verification approaches. Though the framework still provides a formal guarantee of satisfiability whenever it successfully finds one or more violations, these advantages do come at the cost of only providing a statistical estimate of unsatisfiability whenever no violation is found. Key to the practical success of our approach is an adaptation of multi-level splitting, a Monte Carlo approach for estimating the probability of rare events, to our statistical robustness framework. We demonstrate that our approach is able to emulate formal verification procedures on benchmark problems,\n[{'summary_text': 'we present a new approach to assessing the robustness of neural networks . based on estimating the proportion of inputs for which a property is violated . our approach critically varies from the formal verification framework in that when the property can be violated, it provides an informative notion of how robust the network is .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▍         | 92/1992 [05:39<2:03:42,  3.91s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Existing public face image datasets are strongly biased toward Caucasian faces, and other races (e.g., Latino) are significantly underrepresented. The models trained from such datasets suffer from inconsistent classification accuracy, which limits the applicability of face analytic systems to non-White race groups. To mitigate the race bias problem in these datasets, we constructed a novel face image dataset containing 108,501 images which is balanced on race. We define 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups. Evaluations were performed on existing face attribute datasets as well as novel image datasets to measure the generalization performance. We find that the model trained from our dataset is substantially more accurate on novel datasets and the accuracy is consistent across race and gender groups. We also compare several commercial computer vision APIs and report their balanced accuracy across gender, race, and\n[{'summary_text': 'models trained from such datasets suffer from inconsistent classification accuracy, which limits the applicability of face analytic systems to non-White race groups . images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups. evaluations were performed on existing face attribute datasets as well as novel image datasets to measure the generalization performance.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▍         | 93/1992 [05:44<2:06:52,  4.01s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Standard image captioning tasks such as COCO and Flickr30k are factual, neutral in tone and (to a human) state the obvious (e.g., “a man playing a guitar”). While such tasks are useful to verify that a machine understands the content of an image,  they are not engaging to humans as captions.    With this in mind we define a new task, Personality-Captions, where the goal is to be as engaging to humans as possible by incorporating controllable style and personality traits. We collect and release a large dataset of 201,858 of such captions conditioned over 215 possible traits.   We build models that combine existing work from (i) sentence representations (Mazaré et al., 2018) with Transformers trained on 1.7 billion dialogue examples; and (ii) image representations (Mahajan et al., 2018) with ResNets trained on 3.5 billion social media images.  \n[{'summary_text': 'standard image captioning tasks such as COCO and Flickr30k are factual, neutral in tone and (to a human) state the obvious (e.g., “a man playing a guitar”). with this in mind we define a new task, Personality-Captions, where the goal is to be as engaging to humans as possible by incorporating controllable style and personality traits .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▍         | 94/1992 [05:48<2:08:20,  4.06s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We introduce two approaches for conducting efficient Bayesian inference in stochastic simulators containing nested stochastic sub-procedures, i.e., internal procedures for which the density cannot be calculated directly such as rejection sampling loops. The resulting class of simulators are used extensively throughout the sciences and can be interpreted as probabilistic generative models. However, drawing inferences from them poses a substantial challenge due to the inability to evaluate even their unnormalised density, preventing the use of many standard inference procedures like Markov Chain Monte Carlo (MCMC). To address this, we introduce inference algorithms based on a two-step approach that first approximates the conditional densities of the individual sub-procedures, before using these approximations to run MCMC methods on the full program. Because the sub-procedures can be dealt with separately and are lower-dimensional than that of the overall problem, this two-step process allows them to be isolated and thus be tractably\n[{'summary_text': 'the resulting class of simulators are used extensively throughout the sciences . they can be interpreted as probabilistic generative models . drawing inferences from them poses a substantial challenge due to the inability to evaluate even their unnormalised density .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▍         | 95/1992 [05:52<2:06:56,  4.01s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Batch normalization (batch norm) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and noise by double-digit percentages, as we show on five standard datasets. Furthermore, substituting weight decay for batch norm is sufficient to nullify the relationship between adversarial vulnerability and the input dimension. Our work is consistent with a mean-field analysis that found that batch norm causes exploding gradients.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'batch normalization (batch norm) is often used in an attempt to stabilize and accelerate training in deep neural networks . in many cases it decreases the number of parameter updates required to achieve low training error . however, it also reduces robustness to small adversarial input perturbations and noise by double-digit percentages .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▍         | 96/1992 [05:55<2:03:21,  3.90s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Meta learning has been making impressive progress for fast model adaptation. However, limited work has been done on learning fast uncertainty adaption for Bayesian modeling. In this paper, we propose to achieve the goal by placing meta learning on the space of probability measures, inducing the concept of meta sampling for fast uncertainty adaption. Specifically, we propose a Bayesian meta sampling framework consisting of two main components: a meta sampler and a sample adapter. The meta sampler is constructed by adopting a neural-inverse-autoregressive-flow (NIAF) structure, a variant of the recently proposed neural autoregressive flows, to efficiently generate meta samples to be adapted. The sample adapter moves meta samples to task-specific samples, based on a newly proposed and general Bayesian sampling technique, called optimal-transport Bayesian sampling. The combination of the two components allows a simple learning procedure for the\n meta sampler to be developed, which can be efficiently optimized via standard back-\n[{'summary_text': 'in this paper, we propose to achieve the goal by placing meta learning on the space of probability measures . the meta sampler is constructed by adopting a neural-inverse-autoregressive-flow (NIAF) structure, a variant of the recently proposed neural autoregressive flows, to efficiently generate meta samples to be adapted .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▍         | 97/1992 [06:00<2:05:27,  3.97s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We study the Cross-Entropy Method (CEM) for the non-convex optimization of a continuous and parameterized objective function and introduce a differentiable variant (DCEM) that enables us to differentiate the output of CEM with respect to the objective function's parameters. In the machine learning setting this brings CEM inside of the end-to-end learning pipeline in cases this has otherwise been impossible. We show applications in a synthetic energy-based structured prediction task and in non-convex continuous control. In the control setting we show on the simulated cheetah and walker tasks that we can embed their optimal action sequences with DCEM and then use policy optimization to fine-tune components of the controller as a step towards combining model-based and model-free RL.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we show applications in a synthetic energy-based structured prediction task . in the control setting this brings CEM inside of the end-to-end learning pipeline in cases this has otherwise been impossible .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▍         | 98/1992 [06:03<2:00:22,  3.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"One of the main challenges in applying graph convolutional neural networks on gene-interaction data is the lack of understanding of the vector space  to which they belong and also the inherent difficulties involved in representing those interactions on a significantly lower dimension, viz Euclidean spaces. The challenge becomes more prevalent when dealing with various types of heterogeneous data. We introduce a systematic, generalized method, called iSOM-GSN, used to transform ``multi-omic'' data with higher dimensions onto a two-dimensional grid. Afterwards, we apply a convolutional neural network to predict disease states of various types. Based on the idea of Kohonen's self-organizing map, we generate a two-dimensional grid for each sample for a given set of genes that represent a gene similarity network.   We have tested the model to predict breast and prostate cancer using gene expression, DNA methylation and copy number alteration, yielding prediction accuracies in the 94-98% range for tumor stages of\n[{'summary_text': \"the challenge becomes more prevalent when dealing with heterogeneous data . we introduce a systematic, generalized method, called iSOM-GSN, used to transform multi-omic'' data with higher dimensions onto a two-dimensional grid . subsequently, we apply a convolutional neural network to predict disease states of various types .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▍         | 99/1992 [06:07<2:02:54,  3.90s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Keyword spotting—or wakeword detection—is an essential feature for hands-free operation of modern voice-controlled devices. With such devices becoming ubiquitous, users might want to choose a personalized custom wakeword. In this work, we present DONUT, a CTC-based algorithm for online query-by-example keyword spotting that enables custom wakeword detection. The algorithm works by recording a small number of training examples from the user, generating a set of label sequence hypotheses from these training examples, and detecting the wakeword by aggregating the scores of all the hypotheses given a new audio recording. Our method combines the generalization and interpretability of CTC-based keyword spotting with the user-adaptation and convenience of a conventional query-by-example system. DONUT has low computational requirements and is well-suited for both learning and inference on embedded systems without requiring private user data to be uploaded to the cloud.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'keyword spotting is an essential feature for hands-free operation of modern voice-controlled devices . the algorithm works by recording a small number of training examples from the user, generating a set of label sequence hypotheses from these training examples, and detecting the wakeword .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▌         | 100/1992 [06:11<1:59:54,  3.80s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Human reasoning involves recognising common underlying principles across many examples by utilising variables. The by-products of such reasoning are invariants that capture patterns across examples such as \"if someone went somewhere then they are there\" without mentioning specific people or places. Humans learn what variables are and how to use them at a young age, and the question this paper addresses is whether machines can also learn and use variables solely from examples without requiring human pre-engineering. We propose Unification Networks that incorporate soft unification into neural networks to learn variables and by doing so lift examples into invariants that can then be used to solve a given task. We evaluate our approach on four datasets to demonstrate that learning invariants captures patterns in the data and can improve performance over baselines.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'by-products of such reasoning are invariants that capture patterns across examples . humans learn what variables are and how to use them at a young age . we propose Unification Networks that incorporate soft unification into neural networks .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▌         | 101/1992 [06:14<1:56:22,  3.69s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Learning rate decay (lrDecay) is a \\emph{de facto} technique for training modern neural networks. It starts with a large learning rate and then decays it multiple times. It is empirically observed to help both optimization and generalization. Common beliefs in how lrDecay works come from the optimization analysis of (Stochastic) Gradient Descent: 1) an initially large learning rate accelerates training or helps the network escape spurious local minima; 2) decaying the learning rate helps the network converge to a local minimum and avoid oscillation. Despite the popularity of these common beliefs, experiments suggest that they are insufficient in explaining the general effectiveness of lrDecay in training modern neural networks that are deep, wide, and nonconvex. We provide another novel explanation: an initially large learning rate suppresses the network from memorizing noisy data while decaying the learning rate improves the learning of complex patterns. The proposed explanation is validated on a carefully-constructed\n[{'summary_text': 'learning rate decay (lrDecay) is a emphde facto technique for training modern neural networks . it starts with a large learning rate and then decays it multiple times . common beliefs come from the optimization analysis of (Stochastic) Gradient Descent .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▌         | 102/1992 [06:18<2:02:22,  3.88s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recent studies show that widely used Deep neural networks (DNNs) are vulnerable to the carefully crafted adversarial examples.\n Many advanced algorithms have been proposed to generate adversarial examples by leveraging the L_p distance for penalizing perturbations.\n Different defense methods have also been explored to defend against such adversarial attacks. \n While the effectiveness of L_p distance as a metric of perceptual quality remains an active research area, in this paper we will instead focus on a different type of perturbation, namely spatial transformation, as opposed to manipulating the pixel values directly as in prior works.\n Perturbations generated through spatial transformation could result in large L_p distance measures, but our extensive experiments show that such spatially transformed adversarial examples are perceptually realistic and more difficult to defend against with existing defense systems. This potentially provides a new direction in adversarial example generation and the design of corresponding defenses.\n We visualize the spatial transformation based perturbation for different examples and\n[{'summary_text': 'many advanced algorithms have been proposed to generate adversarial examples by leveraging the L_p distance for penalizing perturbations . different defense methods have also been explored to defend against such adversarials . a different type of perturbation could result in large pixel values as in prior works .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▌         | 103/1992 [06:22<2:02:33,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Among multiple ways of interpreting a machine learning model, measuring the importance of a set of features tied to a prediction is probably one of the most intuitive way to explain a model. In this paper, we establish the link between a set of features to a prediction with a new evaluation criteria, robustness analysis, which measures the minimum tolerance of adversarial perturbation. By measuring the tolerance level for an adversarial attack, we can extract a set of features that provides most robust support for a current prediction, and also can extract a set of features that contrasts the current prediction to a target class by setting a targeted adversarial attack. By applying this methodology to various prediction tasks across multiple domains, we observed the derived explanations are indeed capturing the significant feature set qualitatively and quantitatively.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'by measuring the tolerance level for an adversarial attack, we can extract a set of features that contrasts the current prediction to a target class . by applying this methodology to various prediction tasks across multiple domains, we observed the derived explanations are indeed capturing the significant feature set qualitatively and quantitatively .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▌         | 104/1992 [06:26<2:00:30,  3.83s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The practical usage of reinforcement learning agents is often bottlenecked by the duration of training time. To accelerate training, practitioners often turn to distributed reinforcement learning architectures to parallelize and accelerate the training process. However, modern methods for scalable reinforcement learning (RL) often tradeoff between the throughput of samples that an RL agent can learn from (sample throughput) and the quality of learning from each sample (sample efficiency). In these scalable RL architectures, as one increases sample throughput (i.e. increasing parallelization in IMPALA (Espeholt et al., 2018)), sample efficiency drops significantly. To address this, we propose a new distributed reinforcement learning algorithm, IMPACT. IMPACT extends PPO with three changes: a target network for stabilizing the surrogate objective, a circular buffer, and truncated importance sampling. In discrete action-space environments, we show that IMPACT attains higher reward and, simultaneously, achieves up to 30% decrease in training wall-time than that of IMPAL\n[{'summary_text': 'practitioners often turn to distributed reinforcement learning architectures to parallelize and accelerate training process . modern methods for scalable reinforcement learning (RL) often tradeoff between throughput of samples that an RL agent can learn from (sample throughput) and the quality of learning from each sample . in discrete action-space environments, we show that IMPACT attains higher reward and achieves up to 30% decrease in training wall-time than that of IMPAL .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▌         | 105/1992 [06:30<2:05:19,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Emoji suggestion systems based on typed text have been proposed to encourage emoji usage and enrich text messaging; however, such systems’ actual effects on the chat experience remain unknown. We built an Android keyboard with both lexical (word-based) and semantic (meaning-based) emoji suggestion capabilities and compared these in two different studies. To investigate the effect of emoji suggestion in online conversations, we conducted a laboratory text-messaging study with 24 participants, and also a 15-day longitudinal field deployment with 18 participants. We found that lexical emoji suggestions increased emoji usage by 31.5% over a keyboard without suggestions, while semantic suggestions increased emoji usage by 125.1%. However, suggestion mechanisms did not affect the chatting experience significantly. From these studies, we formulate a set of design guidelines for future emoji suggestion systems that better support users’ needs.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we built an Android keyboard with both lexical (word-based) and semantic emoji suggestion capabilities and compared these in two different studies . to investigate the effect of emeoji suggestion in online conversations, we conducted a laboratory text-messaging study with 24 participants . we found that lexicals increased emmoji usage by 31.5% over a keyboard without suggestions, while semantic suggestions increased a 125.1% . however, suggestion mechanisms did not affect the chatting experience significantly .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▌         | 106/1992 [06:35<2:08:11,  4.08s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recent advances in Neural Variational Inference allowed for a renaissance in latent variable models in a variety of domains involving high-dimensional data. In this paper, we introduce two generic Variational Inference frameworks for generative models of Knowledge Graphs; Latent Fact Model and Latent Information Model.   While traditional variational methods derive an analytical approximation for the intractable distribution over the latent variables, here we construct an inference network conditioned on the symbolic representation of entities and relation types in the Knowledge Graph, to provide the variational distributions. The new framework can create models able to discover underlying probabilistic semantics for the symbolic representation by utilising parameterisable distributions which permit training by back-propagation in the context of neural variational inference, resulting in a highly-scalable method. Under a Bernoulli sampling framework, we provide an alternative justification for commonly used techniques in large-scale stochastic variational inference, which drastically reduces training time at a cost of an additional\n[{'summary_text': 'in this paper, we introduce two generic Variational Inference frameworks for generative models of Knowledge Graphs . the new framework can create models able to discover underlying probabilistic semantics for the symbolic representation by using parameterisable distributions which permit training by back-propagation in the context of neural variational inference .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▌         | 107/1992 [06:39<2:07:55,  4.07s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Conversational question answering (CQA) is a novel QA task that requires the understanding of dialogue context. Different from traditional single-turn machine reading comprehension (MRC), CQA is a comprehensive task comprised of passage reading, coreference resolution, and contextual understanding. In this paper, we propose an innovative contextualized attention-based deep neural network, SDNet, to fuse context into traditional MRC models. Our model leverages both inter-attention and self-attention to comprehend the conversation and passage. Furthermore, we demonstrate a novel method to integrate the BERT contextual model as a sub-module in our network. Empirical results show the effectiveness of SDNet. On the CoQA leaderboard, it outperforms the previous best model's F1 score by 1.6%. Our ensemble model further improves the F1 score by 2.7%.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': \"Conversational question answering (CQA) is a novel QA task that requires the understanding of dialogue context . on the CoQA leaderboard, it outperforms the previous best model's F1 score by 1.6% .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▌         | 108/1992 [06:42<2:03:29,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"This work presents a scalable solution to continuous visual speech recognition. To achieve this, we constructed the largest existing visual speech recognition dataset, consisting of pairs of text and video clips of faces speaking (3,886 hours of video). In tandem, we designed and trained an integrated lipreading system, consisting of a video processing pipeline that maps raw video to stable videos of lips and sequences of phonemes, a scalable deep neural network that maps the lip videos to sequences of phoneme distributions, and a production-level speech decoder that outputs sequences of words. The proposed system achieves a word error rate (WER) of 40.9% as measured on a held-out set. In comparison, professional lipreaders achieve either 86.4% or 92.9% WER on the same dataset when having access to additional types of contextual information. Our approach significantly improves on previous lipreading approaches, including variants of LipNet and of Watch, Attend, and Spell (WAS), which are only\n[{'summary_text': 'proposed system achieves a word error rate (WER) of 40.9% as measured on a held-out set . professional lipreaders achieve either 86.4% or 92.9% WER on the same dataset when having access to additional types of contextual information .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   5%|▌         | 109/1992 [06:46<2:02:59,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper we study generative modeling via autoencoders while using the elegant geometric properties of the optimal transport (OT) problem and the Wasserstein distances. We introduce Sliced-Wasserstein Autoencoders (SWAE), which are generative models that enable one to shape the distribution of the latent space into any samplable probability distribution without the need for training an adversarial network or defining a closed-form for the distribution. In short, we regularize the autoencoder loss with the sliced-Wasserstein distance between the distribution of the encoded training samples and a predefined samplable distribution. We show that the proposed formulation has an efficient numerical solution that provides similar capabilities to Wasserstein Autoencoders (WAE) and Variational Autoencoders (VAE), while benefiting from an embarrassingly simple implementation.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we introduce Sliced-Wasserstein Autoencoders (SWAE) which are generative models that enable one to shape the distribution of the latent space into any samplable probability distribution without the need for training an adversarial network or defining a closed-form for the distribution .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 110/1992 [06:50<2:02:59,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The purpose of an encoding model is to predict brain activity given a stimulus. In this contribution, we attempt at estimating a whole brain encoding model of auditory perception in a naturalistic stimulation setting. We analyze data from an open dataset, in which 16 subjects watched a short movie while their brain activity was being measured using functional MRI. We extracted feature vectors aligned with the timing of the audio from the movie, at different layers of a Deep Neural Network pretrained on the classification of auditory scenes. fMRI data was parcellated using hierarchical clustering on 500 parcels, and encoding models were estimated using a fully connected neural network with one hidden layer, trained to predict the signals for each parcel from the DNN features. Individual encoding models were successfully trained and predicted brain activity on unseen data, in parcels located in the superior temporal lobe, as well as dorsolateral prefrontal regions, which are usually considered as areas involved in auditory and language processing. Taken together, this contribution extends previous attempts on estimating encoding models,\n[{'summary_text': 'the purpose of an encoding model is to predict brain activity given a stimulus . we analyze data from an open dataset, in which 16 subjects watched a short movie while their brain activity was being measured using functional MRI . fMRI data was parcellated using hierarchical clustering on 500 parcels .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 111/1992 [06:54<2:04:39,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features . more specifically, we explore three aspects of CNN design: depth of the network, use of residual blocks along with the use of grouped convolution, and global aggregation over time .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 112/1992 [06:58<2:02:47,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Data augmentation is one of the most effective approaches for improving the accuracy of modern machine learning models, and it is also indispensable to train a deep model for meta-learning. However, most current data augmentation implementations applied in meta-learning are the same as those used in the conventional image classification. In this paper, we introduce a new data augmentation method for meta-learning, which is named as ``Task Level Data Augmentation'' (referred to Task Aug). The basic idea of Task Aug is to increase the number of image classes rather than the number of images in each class. In contrast, with a larger amount of classes, we can sample more diverse task instances during training. This allows us to train a deep network by meta-learning methods with little over-fitting. Experimental results show that our approach achieves state-of-the-art performance on miniImageNet, CIFAR-FS, and FC100 few-shot learning benchmarks. Once paper is accepted, we will\n[{'summary_text': 'most current data augmentation implementations applied in meta-learning are the same as those used in the conventional image classification . the basic idea of Task Aug is to increase the number of image classes rather than the numbers of images in each class . a larger amount of classes, we can sample more diverse task instances during training . this allows us to train a deep network .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 113/1992 [07:02<2:04:32,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Can the success of reinforcement learning methods for simple combinatorial optimization problems be extended to multi-robot sequential assignment planning? In addition to the challenge of achieving near-optimal performance in large problems, transferability to an unseen number of robots and tasks is another key challenge for real-world applications. In this paper, we suggest a method that achieves the first success in both challenges for robot/machine scheduling problems.\n  \n Our method comprises of three components. First, we show any robot scheduling problem can be expressed as a random probabilistic graphical model (PGM). We develop a mean-field inference method for random PGM and use it for Q-function inference. Second, we show that transferability can be achieved by carefully designing two-step sequential encoding of problem state. Third, we resolve the computational scalability issue of fitted Q-iteration by suggesting a heuristic auction-based Q-iteration fitting method enabled by transferability we achieved.\n  \n[{'summary_text': 'can the success of reinforcement learning methods for simple combinatorial optimization problems be extended to multi-robot sequential assignment planning? in this paper, we suggest a method that achieves the first success in both challenges for robot/machine scheduling problems . our method comprises of three components: first, we show any robot scheduling problem can be expressed as a random probabilistic graphical model (PGM)'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 114/1992 [07:06<2:06:50,  4.05s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In many partially observable scenarios, Reinforcement Learning (RL) agents must rely on long-term memory in order to learn an optimal policy. We demonstrate that using techniques from NLP and supervised learning fails at RL tasks due to stochasticity from the environment and from exploration. Utilizing our insights on the limitations of traditional memory methods in RL, we propose AMRL, a class of models that can learn better policies with greater sample efficiency and are resilient to noisy inputs. Specifically, our models use a standard memory module to summarize short-term context, and then aggregate all prior states from the standard model without respect to order. We show that this provides advantages both in terms of gradient decay and signal-to-noise ratio over time. Evaluating in Minecraft and maze environments that test long-term memory, we find that our model improves average return by 19% over a baseline that has the same number of parameters and by 9% over a stronger baseline that has far more parameters.<|endoftext|>\n[{'summary_text': 'in many partially observable scenarios, Reinforcement Learning agents must rely on long-term memory in order to learn an optimal policy . using techniques from NLP and supervised learning fails at RL tasks due to stochasticity from the environment and from exploration . we propose AMRL, a class of models that can learn better policies with greater sample efficiency and are resilient to noisy inputs .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 115/1992 [07:10<2:06:02,  4.03s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present a new approach and a novel architecture, termed WSNet, for learning compact and efficient deep neural networks. Existing approaches conventionally learn full model parameters independently and then compress them via \\emph{ad hoc} processing such as model pruning or filter factorization. Alternatively, WSNet proposes learning model parameters by sampling from a compact set of learnable parameters, which naturally enforces {parameter sharing} throughout the learning process. We demonstrate that such a novel weight sampling approach (and induced WSNet) promotes both weights and computation sharing favorably. By employing this method, we can more efficiently learn much smaller networks with competitive performance compared to baseline networks with equal numbers of convolution filters. Specifically, we consider learning compact and efficient 1D convolutional neural networks for audio classification. Extensive experiments on multiple audio classification datasets verify the effectiveness of WSNet. Combined with weight quantization, the resulted models are up to \\textbf{180$\\times$} smaller and theoretically up\n[{'summary_text': 'existing approaches conventionally learn full model parameters independently . then compress them via emphad hoc processing . WSNet proposes learning model parameters by sampling from a compact set of learnable parameters, which naturally enforces parameter sharing throughout the learning process .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 116/1992 [07:14<2:05:04,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The use of deep learning for a wide range of data problems has increased the need for understanding and diagnosing these models, and deep learning interpretation techniques have become an essential tool for data analysts. Although numerous model interpretation methods have been proposed in recent years, most of these procedures are based on heuristics with little or no theoretical guarantees. In this work, we propose a statistical framework for saliency estimation for black box computer vision models. We build a model-agnostic estimation procedure that is statistically consistent and passes the saliency checks of Adebayo et al. (2018). Our method requires solving a linear program, whose solution can be efficiently computed in polynomial time. Through our theoretical analysis, we establish an upper bound on the number of model evaluations needed to recover the region of importance with high probability, and build a new perturbation scheme for estimation of local gradients that is shown to be more efficient than the commonly used random perturbation schemes. Validity of the new\n[{'summary_text': 'the use of deep learning for a wide range of data problems has increased the need for understanding and diagnosing these models . many model interpretation methods have been proposed in recent years, but most of these procedures are based on heuristics with little or no theoretical guarantees . in this work, we propose a statistical framework for saliency estimation for black box computer vision models.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 117/1992 [07:18<2:06:16,  4.04s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Convolutional neural networks (CNNs) in recent years have made a dramatic impact in science, technology and industry, yet the theoretical mechanism of CNN architecture design remains surprisingly vague. The CNN neurons, including its distinctive element, convolutional filters, are known to be learnable features, yet their individual role in producing the output is rather unclear. The thesis of this work is that not all neurons are equally important and some of them contain more useful information to perform a given task. Hence, we propose to quantify and rank neuron importance, and directly incorporate neuron importance in the objective function under two formulations: (1) a game theoretical approach based on Shapley value which computes the marginal contribution of each filter; and (2) a probabilistic approach based on what-we-call, the importance switch using variational inference. Using these two methods we confirm the general theory that some of the neurons are inherently more important than the others. Various experiments illustrate that learned ranks can be readily\n[{'summary_text': 'Various experiments illustrate that learned ranks can be readilyConvolutional neural networks (CNNs) in recent years have made a dramatic impact in science, technology and industry . but the theoretical mechanism of CNN architecture design remains surprisingly vague . a game theoretical approach based on Shapley value computes the marginal contribution of each filter .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 118/1992 [07:23<2:06:41,  4.06s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The standard variational lower bounds used to train latent variable models produce biased estimates of most quantities of interest. We introduce an unbiased estimator of the log marginal likelihood and its gradients for latent variable models based on randomized truncation of infinite series. If parameterized by an encoder-decoder architecture, the parameters of the encoder can be optimized to minimize its variance of this estimator. We show that models trained using our estimator give better test-set likelihoods than a standard importance-sampling based approach for the same average computational cost. This estimator also allows use of latent variable models for tasks where unbiased estimators, rather than marginal likelihood lower bounds, are preferred, such as minimizing reverse KL divergences and estimating score functions.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we introduce an unbiased estimator of the log marginal likelihood and its gradients for latent variable models based on randomized truncation of infinite series . models trained using our estimator give better test-set likelihoods than a standard importance-sampling based approach for the same average computational cost .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 119/1992 [07:26<2:05:23,  4.02s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Adaptive regularization methods pre-multiply a descent direction by a preconditioning matrix. Due to the large number of parameters of machine learning problems, full-matrix preconditioning methods are prohibitively expensive. We show how to modify full-matrix adaptive regularization in order to make it practical and effective. We also provide novel theoretical analysis\n for adaptive regularization in non-convex optimization settings. The core of our algorithm, termed GGT, consists of efficient inverse computation of square roots of low-rank matrices. Our preliminary experiments underscore improved convergence rate of GGT across a variety of synthetic tasks and standard deep learning benchmarks.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'full-matrix preconditioning methods are prohibitively expensive . the core of our algorithm, termed GGT, consists of efficient inverse computation of square roots of low-rank matrices .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 120/1992 [07:30<1:59:06,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"State-of-the-art Unsupervised Domain Adaptation (UDA) methods learn transferable features by minimizing the feature distribution discrepancy between the source and target domains. Different from these methods which do not model the feature distributions explicitly, in this paper, we explore explicit feature distribution modeling for UDA. In particular, we propose Distribution Matching Prototypical Network (DMPN) to model the deep features from each domain as Gaussian mixture distributions. With explicit feature distribution modeling, we can easily measure the discrepancy between the two domains. In DMPN, we propose two new domain discrepancy losses with probabilistic interpretations. The first one minimizes the distances between the corresponding Gaussian component means of the source and target data. The second one minimizes the pseudo negative log likelihood of generating the target features from source feature distribution. To learn both discriminative and domain invariant features, DMPN is trained by minimizing the classification loss on the labeled source data and the domain discrepancy\n[{'summary_text': 'state-of-the-art unsupervised domain Adaptation (UDA) methods learn transferable features by minimizing the feature distribution discrepancy between the source and target domains . in this paper, we explore explicit feature distribution modeling for UDA . we propose Distribution Matching Prototypical Network (DMPN) to model the deep features from each domain as Gaussian mixture distributions.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 121/1992 [07:34<2:02:55,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Typical amortized inference in variational autoencoders is specialized for a single probabilistic query. Here we propose an inference network architecture that generalizes to unseen probabilistic queries. Instead of an encoder-decoder pair, we can train a single inference network directly from data, using a cost function that is stochastic not only over samples, but also over queries. We can use this network to perform the same inference tasks as we would in an undirected graphical model with hidden variables, without having to deal with the intractable partition function. The results can be mapped to the learning of an actual undirected model, which is a notoriously hard problem. Our network also marginalizes nuisance variables as required.   We show that our approach generalizes to unseen probabilistic queries on also unseen test data, providing fast and flexible inference. Experiments show that this approach outperforms or matches PCD and AdVIL on 9 benchmark datasets.<|endoftext|><|endoftext|>\n[{'summary_text': 'Typical amortized inference in variational autoencoders is specialized for a single probabilistic query . instead of an encoder-decoder pair, we can train an inference network directly from data, using a cost function that is stochastic not only over samples, but also over queries . results can be mapped to learning of an actual undirected model, which is a notoriously hard problem .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 122/1992 [07:38<2:03:57,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"A variety of cooperative multi-agent control problems require agents to achieve individual goals while contributing to collective success. This multi-goal multi-agent setting poses difficulties for recent algorithms, which primarily target settings with a single global reward, due to two new challenges: efficient exploration for learning both individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents. To address both challenges, we restructure the problem into a novel two-stage curriculum, in which single-agent goal attainment is learned prior to learning multi-agent cooperation, and we derive a new multi-goal multi-agent policy gradient with a credit function for localized credit assignment. We use a function augmentation scheme to bridge value and policy functions across the curriculum. The complete architecture, called CM3, learns significantly faster than direct adaptations of existing algorithms on three challenging multi-goal multi-agent problems: cooperative navigation in difficult formations, negotiating multi-vehicle lane changes in the SUMO\n[{'summary_text': \"the complete architecture, called CM3, learns significantly faster than direct adaptations of existing algorithms on three challenging multi-goal multi-agent problems . efficient exploration for learning individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents . to address both challenges, we restructure the problem into a novel two-stage curriculum .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 123/1992 [07:42<2:06:27,  4.06s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We show that there exists an inherent tension between the goal of adversarial robustness and that of standard generalization. \n Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists even in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed in practice. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than  standard classifiers. These differences, in particular, seem to result in unexpected benefits: the features learned by robust models tend to align better with salient data characteristics and human perception.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy . this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists even in a fairly simple and natural setting . findings also corroborate a similar phenomenon observed in practice .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▌         | 124/1992 [07:46<2:03:10,  3.96s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper, we first identify \\textit{angle bias}, a simple but remarkable phenomenon that causes the vanishing gradient problem in a multilayer perceptron (MLP) with sigmoid activation functions. We then propose \\textit{linearly constrained weights (LCW)} to reduce the angle bias in a neural network, so as to train the network under the constraints that the sum of the elements of each weight vector is zero. A reparameterization technique is presented to efficiently train a model with LCW by embedding the constraints on weight vectors into the structure of the network. Interestingly, batch normalization (Ioffe & Szegedy, 2015) can be viewed as a mechanism to correct angle bias. Preliminary experiments show that LCW helps train a 100-layered MLP more efficiently than does batch normalization.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this paper, we first identify textitangle bias, a simple but remarkable phenomenon that causes the vanishing gradient problem in a multilayer perceptron (MLP) with sigmoid activation functions . a reparameterization technique is presented to efficiently train a model with LCW by embedding the constraints on weight vectors into the structure of the network .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▋         | 125/1992 [07:50<2:04:40,  4.01s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"While neural networks can be trained to map from one specific dataset to another, they usually do not learn a generalized transformation that can extrapolate accurately outside the space of training. For instance, a generative adversarial network (GAN) exclusively trained to transform images of cars from light to dark might not have the same effect on images of horses. This is because neural networks are good at generation within the manifold of the data that they are trained on. However, generating new samples outside of the manifold or extrapolating \"out-of-sample\" is a much harder problem that has been less well studied. To address this, we introduce a technique called neuron editing that learns how neurons encode an edit for a particular transformation in a latent space. We use an autoencoder to decompose the variation within the dataset into activations of different neurons and generate transformed data by defining an editing transformation on those neurons. By performing the transformation in a latent trained space, we encode fairly complex and non-linear\n[{'summary_text': 'neuron editing learns how neurons encode an edit for a particular transformation in a latent space . a generative adversarial network (GAN) exclusively trained to transform images of cars might not have the same effect on images of horses . this is because neural networks are good at generation within the manifold of the data they are trained on .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▋         | 126/1992 [07:54<2:05:51,  4.05s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Long-term video prediction is highly challenging since it entails simultaneously capturing spatial and temporal information across a long range of image frames.Standard recurrent models are ineffective since they are prone to error propagation and cannot effectively capture higher-order correlations. A potential solution is to extend to higher-order spatio-temporal recurrent models. However, such a model requires  a  large number of parameters and operations, making it intractable  to learn in practice and is prone to overfitting. In this work, we propose convolutional tensor-train LSTM (Conv-TT-LSTM), which  learns higher-orderConvolutional LSTM (ConvLSTM) efficiently using convolutional  tensor-train decomposition (CTTD). Our proposed model naturally incorporates higher-order spatio-temporal information at a small cost of memory and computation by using efficient low-rank tensor representations. We evaluate our model on Moving-MNIST\n[{'summary_text': 'long-term video prediction is highly challenging because it entails simultaneously capturing spatial and temporal information across a long range of image frames . a potential solution is to extend to higher-order spatio-temporal recurrent models . however, such a model requires a large number of parameters and operations, making it intractable to learn in practice .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▋         | 127/1992 [07:58<2:06:34,  4.07s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We introduce a novel method that enables parameter-efficient transfer and multi-task learning with deep neural networks. The basic approach is to learn a model patch - a small set of parameters - that will specialize to each task, instead of fine-tuning the last layer or the entire network. For instance, we show that learning a set of scales and biases is sufficient to convert a pretrained network to perform well on qualitatively different problems (e.g. converting a Single Shot MultiBox Detection (SSD) model into a 1000-class image classification model while reusing 98% of parameters of the SSD feature extractor). Similarly, we show that re-learning existing low-parameter layers (such as depth-wise convolutions) while keeping the rest of the network frozen also improves transfer-learning accuracy significantly. Our approach allows both simultaneous (multi-task) as well as sequential transfer learning. In several multi-task learning problems, despite using much fewer parameters than traditional logits-\n[{'summary_text': 'basic approach is to learn a model patch - a small set of parameters . learning a set of scales and biases is sufficient to convert a pretrained network to perform well on qualitatively different problems . re-learning existing low-parameter layers while keeping rest of the network frozen also improves transfer-learning accuracy significantly .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▋         | 128/1992 [08:03<2:06:33,  4.07s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Machine learning workloads are often expensive to train, taking weeks to converge. The current generation of frameworks relies on custom back-ends in order to achieve efficiency, making it impractical to train models on less common hardware where no such back-ends exist. Knossos builds on recent work that avoids the need for hand-written libraries, instead compiles machine learning models in much the same way one would compile other kinds of software. In order to make the resulting code efficient, the Knossos complier directly optimises the abstract syntax tree of the program. However in contrast to traditional compilers that employ hand-written optimisation passes, we take a rewriting approach driven by the $A^\\star$ search algorithm and a learn value function that evaluates future potential cost reduction of taking various rewriting actions to the program. We show that Knossos can automatically learned optimisations that past compliers had to implement by hand. Furthermore, we demonstrate that Knossos can achieve wall time reduction compared to\n[{'summary_text': 'the current generation relies on custom back-ends in order to achieve efficiency . Knossos builds on recent work that avoids the need for hand-written libraries, instead compiles machine learning models in much the same way one would compile other kinds of software .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   6%|▋         | 129/1992 [08:07<2:07:47,  4.12s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Differentiable planning network architecture has shown to be powerful in solving transfer planning tasks while possesses a simple end-to-end training feature. Many great planning architectures that have been proposed later in literature are inspired by this design principle in which a recursive network architecture is applied to emulate backup operations of a value  iteration algorithm. However existing frame-works can only learn and plan effectively on domains with a lattice structure, i.e. regular graphs embedded in a certain Euclidean space. In this paper, we propose a general planning network, called Graph-based Motion Planning Networks (GrMPN), that will be able to i) learn and plan on general irregular graphs, hence ii) render existing planning network architectures special cases. The proposed GrMPN framework is invariant to task graph permutation, i.e. graph isormophism. As a result, GrMPN possesses the generalization strength and data-efficiency ability. We demonstrate the performance of the proposed GrMPN method\n[{'summary_text': 'many great planning architectures that have been proposed later in literature are inspired by this design principle in which a recursive network architecture is applied to emulate backup operations of a value iteration algorithm . existing frame-works can only learn and plan effectively on domains with a lattice structure, i.e. regular graphs embedded in a certain Euclidean space .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 130/1992 [08:11<2:08:27,  4.14s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Conversational machine comprehension requires a deep understanding of the conversation history. To enable traditional, single-turn models to encode the history comprehensively, we introduce Flow, a mechanism that can incorporate intermediate representations generated during the process of answering previous questions, through an alternating parallel processing structure. Compared to shallow approaches that concatenate previous questions/answers as input, Flow integrates the latent semantics of the conversation history more deeply. Our model, FlowQA, shows superior performance on two recently proposed conversational challenges (+7.2% F1 on CoQA and +4.0% on QuAC). The effectiveness of Flow also shows in other tasks. By reducing sequential instruction understanding to conversational machine comprehension, FlowQA outperforms the best models on all three domains in SCONE, with +1.8% to +4.4% improvement in accuracy.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'to enable traditional, single-turn models to encode the history comprehensively, we introduce Flow, a mechanism that can incorporate intermediate representations generated during the process of answering previous questions . by reducing sequential instruction understanding to conversational machine comprehension, FlowQA outperforms the best models on all three domains in SCONE, with +1.8% to +4.4% improvement in accuracy .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 131/1992 [08:15<2:05:56,  4.06s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Human-computer conversation systems have attracted much attention in Natural Language Processing. Conversation systems can be roughly divided into two categories: retrieval-based and generation-based systems. Retrieval systems search a user-issued utterance (namely a query) in a large conversational repository and return a reply that best matches the query. Generative approaches synthesize new replies. Both ways have certain advantages but suffer from their own disadvantages. We propose a novel ensemble of retrieval-based and generation-based conversation system. The retrieved candidates, in addition to the original query, are fed to a reply generator via a neural network, so that the model is aware of more information. The generated reply together with the retrieved ones then participates in a re-ranking process to find the final reply to output. Experimental results show that such an ensemble system outperforms each single module by a large margin.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'retrieved candidates are fed to a reply generator via a neural network . the generated reply together with the retrieved ones participates in a re-ranking process to find the final reply to output .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 132/1992 [08:18<1:59:35,  3.86s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Human brain function as measured by functional magnetic resonance imaging\n (fMRI), exhibits a rich diversity. In response, understanding the individual variability\n of brain function and its association with behavior has become one of the\n major concerns in modern cognitive neuroscience. Our work is motivated by the\n view that generative models provide a useful tool for understanding this variability.\n To this end, this manuscript presents two novel generative models trained\n on real neuroimaging data which synthesize task-dependent functional brain images.\n Brain images are high dimensional tensors which exhibit structured spatial\n correlations. Thus, both models are 3D conditional Generative Adversarial networks\n (GANs) which apply Convolutional Neural Networks (CNNs) to learn an\n abstraction of brain image representations. Our results show that the generated\n brain images are diverse, yet task dependent. In addition to qualitative evaluation,\n we utilize the generated synthetic brain volumes as additional training data to improve\n downstream fMRI classifiers (also\n[{'summary_text': 'human brain function as measured by functional magnetic resonance imaging (fMRI), exhibits a rich diversity . to this end, this manuscript presents two novel generative models trained on real neuroimaging data which synthesize task-dependent functional brain images . both models are 3D conditional Generative Adversarial networks (GANs) which apply Convolutional Neural networks (CNNs) to learn an abstraction of brain image representations .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 133/1992 [08:23<2:04:04,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Large Transformer models routinely achieve state-of-the-art results on\n a number of tasks but training these models can be prohibitively costly,\n especially on long sequences. We introduce two techniques to improve\n the efficiency of Transformers. For one, we replace dot-product attention\n by one that uses locality-sensitive hashing, changing its complexity\n from O(L^2) to O(L), where L is the length of the sequence.\n Furthermore, we use reversible residual layers instead of the standard\n residuals, which allows storing activations only once in the training\n process instead of N times, where N is the number of layers.\n The resulting model, the Reformer, performs on par with Transformer models\n while being much more memory-efficient and much faster on long sequences.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'large Transformer models routinely achieve state-of-the-art results on a number of tasks . for one, we replace dot-product attention by one that uses locality-sensitive hashing . resulting model, the Reformer, performs on par with Transformer models .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 134/1992 [08:26<2:01:10,  3.91s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Source separation for music is the task of isolating contributions, or stems, from different instruments recorded individually and arranged together to form a song. Such components include voice, bass, drums and any other accompaniments. While end-to-end models that directly generate the waveform are state-of-the-art in many audio synthesis problems, the best multi-instrument source separation models generate masks on the magnitude spectrum and achieve performances far above current end-to-end, waveform-to-waveform models. We present an in-depth analysis of a new architecture, which we will refer to as Demucs, based on a (transposed) convolutional autoencoder, with a bidirectional LSTM at the bottleneck layer and skip-connections as in U-Networks (Ronneberger et al., 2015). Compared to the state-of-the-art waveform-to-waveform model, Wave-U-Net (St\n[{'summary_text': 'source separation for music is the task of isolating contributions, or stems, from different instruments recorded individually and arranged together to form a song . such components include voice, bass, drums and any other accompaniments . the best multi-instrument source separation models generate masks on the magnitude spectrum .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 135/1992 [08:30<2:03:40,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose a framework for extreme learned image compression based on Generative Adversarial Networks (GANs), obtaining visually pleasing images at significantly lower bitrates than previous methods. This is made possible through our GAN formulation of learned compression combined with a generator/decoder which operates on the full-resolution image and is trained in combination with a multi-scale discriminator. Additionally, if a semantic label map of the original image is available, our method can fully synthesize unimportant regions in the decoded image such as streets and trees from the label map, therefore only requiring the storage of the preserved region and the semantic label map. A user study confirms that for low bitrates, our approach is preferred to state-of-the-art methods, even when they use more than double the bits.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we propose a framework for extreme learned image compression based on Generative Adversarial Networks (GANs), obtaining visually pleasing images at significantly lower bitrates than previous methods . this is made possible through our GAN formulation of learned compression combined with a generator/decoder which operates on the full-resolution image . if a semantic label map of the original image is available, our method can fully synthesize unimportant regions in the decoded image such as streets and trees from the label'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 136/1992 [08:35<2:06:15,  4.08s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Stochastic neural networks with discrete random variables are an important class of models for their expressivity and interpretability. Since direct differentiation and backpropagation is not possible, Monte Carlo gradient estimation techniques have been widely employed for training such models. Efficient stochastic gradient estimators, such Straight-Through and Gumbel-Softmax, work well for shallow models with one or two stochastic layers. Their performance, however, suffers with increasing model complexity.\n In this work we focus on stochastic networks with multiple layers of Boolean latent variables. To analyze such such networks, we employ the framework of harmonic analysis for Boolean functions.   We use it to derive an analytic formulation for the source of bias in the biased Straight-Through estimator. Based on the analysis we propose \\emph{FouST}, a simple gradient estimation algorithm that relies on three simple bias reduction steps. Extensive experiments show that FouST performs favorably compared to state-of-the-\n[{'summary_text': 'Monte Carlo gradient estimation techniques have been widely employed for training such models . efficient stochastic gradient estimators, such straight-Through and Gumbel-Softmax, work well for shallow models with one or two stochaastic layers . their performance, however, suffers with increasing model complexity .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 137/1992 [08:39<2:06:00,  4.08s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'momentum-based methods, in conjunction with stochastic gradient descent, are widely used when training machine learning models . however, there is little theoretical understanding on the generalization error of such methods . in practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 138/1992 [08:42<2:01:32,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression.   In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In many applications this expensive initialization is not practical, for example streaming algorithms --- where inputs are ephemeral and can only be inspected a small number of times.   In this paper we explore the learning of approximate set membership over a stream of data in one-shot via meta-learning. We propose a novel memory architecture, the Neural Bloom Filter, which we show to be more compressive than Bloom Filters and several existing memory-augmented neural networks in scenarios of skewed data or structured sets.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence . in many applications this expensive initialization is not practical, for example streaming algorithms . we propose a novel memory architecture, the Neural Bloom Filter .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 139/1992 [08:46<1:58:32,  3.84s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"There is significant recent evidence in supervised learning that, in the over-parametrized setting, wider networks achieve better test error. In other words, the bias-variance tradeoff is not directly observable when increasing network width arbitrarily. We investigate whether a corresponding phenomenon is present in reinforcement learning. We experiment on four OpenAI Gym environments, increasing the width of the value and policy networks beyond their prescribed values. Our empirical results lend support to this hypothesis. However, tuning the hyperparameters of each network width separately remains as important future work in environments/algorithms where the optimal hyperparameters vary noticably across widths, confounding the results when the same hyperparameters are used for all widths.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'bias-variance tradeoff is not directly observable when increasing network width arbitrarily . we investigate whether a corresponding phenomenon is present in reinforcement learning . tuning the hyperparameters of each network width separately remains as important future work in environments/algorithms .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 140/1992 [08:50<1:55:53,  3.75s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Data augmentation is a useful technique to enlarge the size of the training set and prevent overfitting for different machine learning tasks when training data is scarce. However, current data augmentation techniques rely heavily on human design and domain knowledge, and existing automated approaches are yet to fully exploit the latent features in the training dataset. In this paper we propose  \\textit{Parallel Adaptive GAN Data Augmentation}(PAGANDA), where the training set adaptively enriches itself with sample images automatically constructed from Generative Adversarial Networks (GANs) trained in parallel. We demonstrate by experiments that our data augmentation strategy, with little model-specific considerations, can be easily adapted to cross-domain deep learning/machine learning tasks such as image classification and image inpainting, while significantly improving model performance in both tasks. Our source code and experimental details are available at \\url{https://github.com/miaojiang1987/k-folder-data-augment\n[{'summary_text': 'current data augmentation techniques rely heavily on human design and domain knowledge . existing automated approaches are yet to fully exploit the latent features in the training dataset .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 141/1992 [08:53<1:53:57,  3.69s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Inspired by the recent successes of deep generative models for Text-To-Speech (TTS) such as WaveNet (van den Oord et al., 2016) and Tacotron (Wang et al., 2017), this article proposes the use of a deep generative model tailored for Automatic Speech Recognition (ASR) as the primary acoustic model (AM) for an overall recognition system with a separate language model (LM). Two dimensions of depth are considered: (1) the use of mixture density networks, both autoregressive and non-autoregressive, to generate density functions capable of modeling acoustic input sequences with much more powerful conditioning than the first-generation generative models for ASR, Gaussian Mixture Models / Hidden Markov Models (GMM/HMMs), and (2) the use of standard LSTMs, in the spirit of the original tandem approach, to produce discriminative feature vectors for generative modeling. Combining mixture density networks\n[{'summary_text': 'the article proposes the use of a deep generative model tailored for Automatic Speech Recognition (ASR) as the primary acoustic model (AM) for an overall recognition system with a separate language model (LM) two dimensions of depth are considered: (1) use of mixture density networks, both autoregressive and non-autoregressive, to generate density functions . the first-generation generative models for ASR, Gaussian Mixture Models / Hidden Markov Models (GMM/HMMs'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 142/1992 [08:58<2:01:40,  3.95s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recent efforts on combining deep models with probabilistic graphical models are promising in providing flexible models that are also easy to interpret. We propose a variational message-passing algorithm for variational inference in such models. We make three contributions. First, we propose structured inference networks that incorporate the structure of the graphical model in the inference network of variational auto-encoders (VAE). Second, we establish conditions under which such inference networks enable fast amortized inference similar to VAE. Finally, we derive a variational message passing algorithm to perform efficient natural-gradient inference while retaining the efficiency of the amortized inference. By simultaneously enabling structured, amortized, and natural-gradient inference for deep structured models, our method simplifies and generalizes existing methods.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'recent efforts on combining deep models with probabilistic graphical models are promising in providing flexible models that are also easy to interpret . we propose a variational message-passing algorithm for variational inference in such models .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 143/1992 [09:01<1:57:51,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We introduce our Distribution Regression Network (DRN) which performs regression from input probability distributions to output probability distributions. Compared to existing methods, DRN learns with fewer model parameters and easily extends to multiple input and multiple output distributions. On synthetic and real-world datasets, DRN performs similarly or better than the state-of-the-art. Furthermore, DRN generalizes the conventional multilayer perceptron (MLP). In the framework of MLP, each node encodes a real number, whereas in DRN, each node encodes a probability distribution.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'DRN learns with less model parameters and easily extends to multiple input and multiple output distributions . compared to existing methods, DRN performs similarly or better than the state-of-the-art .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 144/1992 [09:05<1:54:53,  3.73s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"This paper presents a generic framework to tackle the crucial class mismatch problem in unsupervised domain adaptation (UDA) for multi-class distributions.   Previous adversarial learning methods condition domain alignment only on pseudo labels, but noisy and inaccurate pseudo labels may perturb the multi-class distribution embedded in probabilistic predictions, hence bringing insufficient alleviation to the latent mismatch problem.   Compared with pseudo labels, class prototypes are more accurate and reliable since they summarize over all the instances and are  able  to  represent  the  inherent  semantic  distribution  shared  across  domains. Therefore, we propose a novel Prototype-Assisted Adversarial Learning (PAAL) scheme, which incorporates instance probabilistic predictions and class prototypes together  to  provide  reliable  indicators  for  adversarial  domain  alignment.    With the PAAL scheme,  we align both the instance feature representations and class prototype  representations  to  alleviate  the  mismatch \n[{'summary_text': 'this paper presents a generic framework to tackle the crucial class mismatch problem in unsupervised domain adaptation . previous adversarial learning methods condition domain alignment only on pseudo labels, but noisy and inaccurate pseudo labels may perturb the multi-class distribution embedded in probabilistic predictions . with the PAAL scheme, we align both the instance feature representations and class prototype representations to alleviate the mismatch .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 145/1992 [09:09<1:58:48,  3.86s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Learning control policies in robotic tasks requires a large number of interactions due to small learning rates, bounds on the updates or unknown constraints. In contrast humans can infer protective and safe solutions after a single failure or unexpected observation. \n In order to reach similar performance, we developed a hierarchical Bayesian optimization algorithm that replicates the cognitive inference and memorization process for avoiding failures in motor control tasks. A Gaussian Process implements the modeling and the sampling of the acquisition function. This enables rapid learning with large learning rates while a mental replay phase ensures that policy regions that led to failures are inhibited during the sampling process.    \n The features of the hierarchical Bayesian optimization method are evaluated in a simulated and physiological humanoid postural balancing task. We quantitatively compare the human learning performance to our learning approach by evaluating the deviations of the center of mass during training. Our results show that we can reproduce the efficient learning of human subjects in postural control tasks which provides a testable model for future physiological motor\n[{'summary_text': 'this enables rapid learning with large learning rates while a mental replay phase ensures that policy regions that led to failures are inhibited during the sampling process . the features of the hierarchical Bayesian optimization method are evaluated in a simulated and physiological humanoid postural balancing task .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 146/1992 [09:13<2:02:01,  3.97s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In order to choose a neural network architecture that will be effective for a particular modeling problem, one must understand the limitations imposed by each of the potential options. These limitations are typically described in terms of information theoretic bounds, or by comparing the relative complexity needed to approximate example functions between different architectures. In this paper, we examine the topological constraints that the architecture of a neural network imposes on the level sets of all the functions that it is able to approximate. This approach is novel for both the nature of the limitations and the fact that they are independent of network depth for a broad family of activation functions.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in order to choose a neural network architecture that will be effective for a particular modeling problem, one must understand the limitations imposed by each of the potential options . these limitations are typically described in terms of information theoretic bounds, or by comparing the relative complexity needed to approximate example functions between different architectures . this approach is novel for both the nature of the limitations and the fact that they are independent of network depth .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 147/1992 [09:17<2:02:32,  3.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Understanding the groundbreaking performance of Deep Neural Networks is one\n of the greatest challenges to the scientific community today. In this work, we\n introduce an information theoretic viewpoint on the behavior of deep networks\n optimization processes and their generalization abilities. By studying the Information\n Plane, the plane of the mutual information between the input variable and\n the desired label, for each hidden layer. Specifically, we show that the training of\n the network is characterized by a rapid increase in the mutual information (MI)\n between the layers and the target label, followed by a longer decrease in the MI\n between the layers and the input variable. Further, we explicitly show that these\n two fundamental information-theoretic quantities correspond to the generalization\n error of the network, as a result of introducing a new generalization bound that is\n exponential in the representation compression. The analysis focuses on typical\n patterns of large-scale problems. For this purpose, we introduce a novel analytic\n bound on the mutual information\n[{'summary_text': 'understanding the groundbreaking performance of Deep Neural Networks is one of the greatest challenges to the scientific community today . in this work, we introduce an information theoretic viewpoint on the behavior of deep networks optimization processes and their generalization abilities . by studying the Information Plane, the plane of the mutual information between the input variable and the desired label, for each hidden layer .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 148/1992 [09:21<2:03:30,  4.02s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The information bottleneck principle is an elegant and useful approach to representation learning. In this paper, we investigate the problem of representation learning in the context of reinforcement learning using the information bottleneck framework, aiming at improving the sample efficiency of the learning algorithms.We analytically derive the optimal conditional distribution of the representation, and provide a variational lower bound. Then, we maximize this lower bound with the Stein variational (SV) gradient method. \n We incorporate this framework in the advantageous actor critic algorithm (A2C) and the proximal policy optimization algorithm (PPO). Our experimental results show that our framework can improve the sample efficiency of vanilla A2C and PPO significantly. Finally, we study the information-bottleneck (IB) perspective in deep RL with the algorithm called mutual information neural estimation(MINE).\n We experimentally verify that the information extraction-compression process also exists in deep RL and our framework is capable of accelerating this process. We also analyze the relationship between\n[{'summary_text': 'in this paper, we investigate the problem of representation learning using the information bottleneck framework . we analyse the relationship between the information-bottleneck (IB) perspective in deep RL with the algorithm called mutual information neural estimation .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   7%|▋         | 149/1992 [09:25<2:01:12,  3.95s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this work we explore a straightforward variational Bayes scheme for Recurrent Neural Networks.\n Firstly, we show that a simple adaptation of truncated backpropagation through time can yield good quality uncertainty estimates and superior regularisation at only a small extra computational cost during training, also reducing the amount of parameters by 80\\%.\n Secondly, we demonstrate how a novel kind of posterior approximation yields further improvements to the performance of Bayesian RNNs. We incorporate local gradient information into the approximate posterior to sharpen it around the current batch statistics. We show how this technique is not exclusive to recurrent neural networks and can be applied more widely to train Bayesian neural networks.\n We also empirically demonstrate how Bayesian RNNs are superior to traditional RNNs on a language modelling benchmark and an image captioning task, as well as showing how each of these methods improve our model over a variety of other schemes for training them. We also introduce a new benchmark for studying uncertainty for language models\n[{'summary_text': 'a simple adaptation of truncated backpropagation through time can yield good quality uncertainty estimates and superior regularisation at only a small extra computational cost during training, also reducing the amount of parameters by 80% . a novel kind of posterior approximation yields further improvements to the performance of Bayesian RNNs .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 150/1992 [09:29<2:03:21,  4.02s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The goal of the paper is to propose an algorithm for learning the most generalizable solution from given training data. It is shown that Bayesian approach leads to a solution that dependent on statistics of training data and not on particular\n samples. The solution is stable under perturbations of training data because it is defined by an integral contribution of multiple maxima of the likelihood and not by a single global maximum. Specifically, the Bayesian probability distribution\n of parameters (weights) of a probabilistic model given by a neural network is estimated via recurrent variational approximations. Derived recurrent update rules correspond to SGD-type rules for finding a minimum of an effective loss that is an average of an original negative log-likelihood over the Gaussian distributions of weights, which makes it a function of means and variances. The effective loss is convex for large variances and non-convex in the limit of small variances. Among stationary solutions of the update rules there are trivial\n[{'summary_text': 'the goal of the paper is to propose an algorithm for learning the most generalizable solution from given training data . it is shown that Bayesian approach leads to a solution that dependent on statistics of training data and not on particular samples . the solution is stable under perturbations of training because it is defined by an integral contribution of multiple maxima of the likelihood and not by a single global maximum .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 151/1992 [09:33<2:05:08,  4.08s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step ---adding a mean shift to the input data--- to show that a transformation with no effect on the model can cause numerous methods to incorrectly attribute. We define input invariance as the requirement that a saliency method mirror the sensitivity of the model with respect to transformations of the input. We show, through several examples, that saliency methods that do not satisfy a input invariance property are unreliable and can lead to misleading and inaccurate attribution.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Saliency methods aim to explain the predictions of deep neural networks . these methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction . we define input invariance as the requirement that a saliency method mirror the sensitivity of the model with respect to transformations .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 152/1992 [09:37<2:00:39,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Gaussian processes are the leading class of distributions on random functions, but they suffer from well known issues including difficulty scaling and inflexibility with respect to certain shape constraints (such as nonnegativity). Here we propose Deep Random Splines, a flexible class of random functions obtained by transforming Gaussian noise through a deep neural network whose output are the parameters of a spline. Unlike Gaussian processes, Deep Random Splines allow us to readily enforce shape constraints while inheriting the richness and tractability of deep generative models. We also present an observational model for point process data which uses Deep Random Splines to model the intensity function of each point process and apply it to neuroscience data to obtain a low-dimensional representation of spiking activity. Inference is performed via a variational autoencoder that uses a novel recurrent encoder architecture that can handle multiple point processes as input.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Gaussian processes are the leading class of distributions on random functions . but they suffer from well known issues including difficulty scaling and inflexibility with respect to certain shape constraints (such as nonnegativity) deep random Splines allow us to readily enforce shape constraints while inheriting the richness and tractability of deep generative models .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 153/1992 [09:41<1:59:06,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"It is well-known that neural networks are universal approximators, but that deeper networks tend in practice to be more powerful than shallower ones. We shed light on this by proving that the total number of neurons m required to approximate natural classes of multivariate polynomials of n variables grows only linearly with n for deep neural networks, but grows exponentially when merely a single hidden layer is allowed. We also provide evidence that when the number of hidden layers is increased from 1 to k, the neuron requirement grows exponentially not with n but with n^{1/k}, suggesting that the minimum number of layers required for practical expressibility grows only logarithmically with n.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the total number of neurons m required to approximate natural classes of multivariate polynomials of n variables grows only linearly with n for deep neural networks . but when the number of hidden layers is increased from 1 to k, the neuron requirement grows exponentially when merely a single hidden layer is allowed . we also provide evidence that the minimum number of layers required for practical expressibility grows only logarithmically . it is well-known that neural networks are universal approximators, but that deeper'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 154/1992 [09:45<2:02:17,  3.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep neural networks have demonstrated unprecedented success in various knowledge management applications. However, the networks created are often very complex, with large numbers of trainable edges which require extensive computational resources. We note that many successful networks nevertheless often contain large numbers of redundant edges. Moreover, many of these edges may have negligible contributions towards the overall network performance. In this paper, we propose a novel iSparse framework and experimentally show, that we can sparsify the network, by 30-50%, without impacting the network performance. iSparse leverages a novel edge significance score, E, to determine the importance of an edge with respect to the final network output. Furthermore, iSparse can be applied both while training a model or on top of a pre-trained model, making it a  retraining-free approach - leading to a minimal computational overhead. Comparisons of iSparse against PFEC, NISP, DropConnect, and Retraining-Free on benchmark datasets show that i\n[{'summary_text': 'the networks created are often very complex, with large numbers of trainable edges which require extensive computational resources . iSparse leverages a novel edge significance score, E, to determine the importance of an edge with respect to the final network output . many of these edges may have negligible contributions towards the overall network performance .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 155/1992 [09:49<2:02:31,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Knowledge graph has gained increasing attention in recent years for its successful applications of numerous tasks. Despite the rapid growth of knowledge construction, knowledge graphs still suffer from severe incompletion and inevitably involve various kinds of errors. Several attempts have been made to complete knowledge graph as well as to detect noise. However, none of them considers unifying these two tasks even though they are inter-dependent and can mutually boost the performance of each other. In this paper, we proposed to jointly combine these two tasks with a unified Generative Adversarial Networks (GAN) framework to learn noise-aware knowledge graph embedding. Extensive experiments have demonstrated that our approach is superior to existing state-of-the-art algorithms both in regard to knowledge graph completion and error detection.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'knowledge graph has gained increasing attention in recent years for its successful applications of numerous tasks . despite the rapid growth of knowledge construction, knowledge graph still suffer from severe incompletion and inevitably involve various kinds of errors . none considers unifying these two tasks even though they are inter-dependent and can mutually boost the performance of each other .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 156/1992 [09:53<1:59:51,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"As distributed approaches to natural language semantics have developed and diversified, embedders for linguistic units larger than words (e.g., sentences) have come to play an increasingly important role.   To date, such embedders have been evaluated using benchmark tasks (e.g., GLUE) and linguistic probes.   We propose a comparative approach, nearest neighbor overlap (N2O), that quantifies similarity between embedders in a task-agnostic manner.   N2O requires only a collection of examples and is simple to understand: two embedders are more similar if, for the same set of inputs, there is greater overlap between the inputs' nearest neighbors.   We use N2O to compare 21 sentence embedders and show the effects of different design choices and architectures.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'to date, such embedders have been evaluated using benchmark tasks (e.g., GLUE) and linguistic probes . N2O requires only a collection of examples and is simple to understand .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 157/1992 [09:56<1:56:57,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The detection of out of distribution samples for image classification has been widely researched. Safety critical applications, such as autonomous driving, would benefit from the ability to localise the unusual objects causing the image to be out of distribution. This paper adapts state-of-the-art methods for detecting out of distribution images for image classification to the new task of detecting out of distribution pixels, which can localise the unusual objects. It further experimentally compares the adapted methods on two new datasets derived from existing semantic segmentation datasets using PSPNet and DeeplabV3+ architectures, as well as proposing a new metric for the task. The evaluation shows that the performance ranking of the compared methods does not transfer to the new task and every method performs significantly worse than their image-level counterparts.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'safety critical applications, such as autonomous driving, would benefit from the ability to localise the unusual objects causing the image to be out of distribution . this paper adapts state-of-the-art methods for detecting out of distributed images for image classification . it further compares the adapted methods on two new datasets derived from existing semantic segmentation datasets using PSPNet and DeeplabV3+ architectures .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 158/1992 [10:00<1:58:02,  3.86s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"This paper improves upon the line of research that formulates named entity recognition (NER) as a sequence-labeling problem. We use so-called black-box long short-term memory (LSTM) encoders to achieve state-of-the-art results while providing insightful understanding of what the auto-regressive model learns with a parallel self-attention mechanism. Specifically, we decouple the sequence-labeling problem of NER into entity chunking, e.g., Barack_B Obama_E was_O elected_O, and entity typing, e.g., Barack_PERSON Obama_PERSON was_NONE elected_NONE, and analyze how the model learns to, or has difficulties in, capturing text patterns for each of the subtasks. The insights we gain then lead us to explore a more sophisticated deep cross-Bi-LSTM encoder, which proves better at capturing global interactions given both empirical results and a theoretical justification\n[{'summary_text': 'this paper improves upon the line of research that formulates named entity recognition (NER) as a sequence-labeling problem . we use so-called black-box long-term memory (LSTM) encoders to achieve state-of-the-art results while providing insightful understanding of what the auto-regressive model learns with a parallel self-attention mechanism . the insights we gain then lead us to explore a more sophisticated deep cross-Bi-LSTM encoder .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 159/1992 [10:05<2:04:05,  4.06s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations, longer training times, and unexpected model degradation. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks . but further model increases become harder due to GPU/TPU memory limitations, longer training times, and unexpected model degradation . to address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 160/1992 [10:08<2:00:25,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Basis pursuit is a compressed sensing optimization in which the l1-norm is minimized subject to model error constraints. Here we use a deep neural network prior instead of l1-regularization. Using known noise statistics, we jointly learn the prior and reconstruct images without access to ground-truth data. During training, we use alternating minimization across an unrolled iterative network and jointly solve for the neural network weights and training set image reconstructions. At inference, we fix the weights and pass the measurements through the network. We compare reconstruction performance between unsupervised and supervised (i.e. with ground-truth) methods. We hypothesize this technique could be used to learn reconstruction when ground-truth data are unavailable, such as in high-resolution dynamic MRI.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'base pursuit is a compressed sensing optimization in which the l1-norm is minimized subject to model error constraints . using known noise statistics, we jointly learn the prior and reconstruct images without access to ground-truth data . at inference, we fix the weights and pass the measurements through the network .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 161/1992 [10:13<2:01:14,  3.97s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present a deep reinforcement learning approach to minimizing the execution cost of neural network computation graphs in an optimizing compiler. Unlike earlier learning-based works that require training the optimizer on the same graph to be optimized, we propose a learning approach that trains an optimizer offline and then generalizes to previously unseen graphs without further training. This allows our approach to produce high-quality execution decisions on real-world TensorFlow graphs in seconds instead of hours. We consider two optimization tasks for computation graphs: minimizing running time and peak memory usage. In comparison to an extensive set of baselines, our approach achieves significant improvements over classical and other learning-based methods on these two tasks.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we propose a learning approach that trains an optimizer offline and then generalizes to previously unseen graphs without further training . this allows our approach to produce high-quality execution decisions on real-world TensorFlow graphs in seconds instead of hours.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 162/1992 [10:16<1:56:39,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We introduce the notion of property signatures, a representation for programs and\n program specifications meant for consumption by machine learning algorithms.\n Given a function with input type τ_in and output type τ_out, a property is a function\n of type: (τ_in, τ_out) → Bool that (informally) describes some simple property\n of the function under consideration. For instance, if τ_in and τ_out are both lists\n of the same type, one property might ask ‘is the input list the same length as the\n output list?’. If we have a list of such properties, we can evaluate them all for our\n function to get a list of outputs that we will call the property signature. Crucially,\n we can ‘guess’ the property signature for a function given only a set of input/output\n pairs meant to specify that function. We discuss several potential applications of\n property signatures and show experimentally\n[{'summary_text': 'property signatures are a representation for programs and program specifications meant for consumption by machine learning algorithms . given a function with input type and output type _out, a property is a functional of type: (_in, -out)  Bool that (informally) describes some simple property of the function under consideration . if a list of such properties, one property might ask ‘is the input list the same length as the output list?’.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 163/1992 [10:20<2:02:21,  4.01s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The success of reinforcement learning in the real world has been limited to instrumented laboratory scenarios, often requiring arduous human supervision to enable continuous learning. In this work, we discuss the required elements of a robotic system that can continually and autonomously improve with data collected in the real world, and propose a particular instantiation of such a system. Subsequently, we investigate a number of challenges of learning without instrumentation -- including the lack of episodic resets, state estimation, and hand-engineered rewards -- and propose simple, scalable solutions to these challenges. We demonstrate the efficacy of our proposed system on dexterous robotic manipulation tasks in simulation and the real world, and also provide an insightful analysis and ablation study of the challenges associated with this learning paradigm.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the success of reinforcement learning in the real world has been limited to instrumented laboratory scenarios, often requiring arduous human supervision to enable continuous learning . in this work, we discuss the required elements of a robotic system that can continually and autonomously improve with data collected in the world . we propose a particular instantiation of such a system .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 164/1992 [10:24<2:01:12,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"As Machine Learning (ML) gets applied to security-critical or sensitive domains, there is a growing need for integrity and privacy for outsourced ML computations. A pragmatic solution comes from Trusted Execution Environments (TEEs), which use hardware and software protections to isolate sensitive computations from the untrusted software stack. However, these isolation guarantees come at a price in performance, compared to untrusted alternatives. This paper initiates the study of high performance execution of Deep Neural Networks (DNNs) in TEEs by efficiently partitioning DNN computations between trusted and untrusted devices. Building upon an efficient outsourcing scheme for matrix multiplication, we propose Slalom, a framework that securely delegates execution of all linear layers in a DNN from a TEE (e.g., Intel SGX or Sanctum) to a faster, yet untrusted, co-located processor. We evaluate Slalom by running DNNs in an Intel SGX\n[{'summary_text': 'trusted Execution Environments (TEEs) use hardware and software protections to isolate sensitive computations from the untrusted software stack . this paper initiates the study of high performance execution of Deep Neural Networks (DNNs) in TEEs by efficiently partitioning DNN computations between trusted devices . however, these isolation guarantees come at a price in performance, compared to nontrusted alternatives .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 165/1992 [10:29<2:04:05,  4.08s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Adversarial training is one of the main defenses against adversarial attacks. In this paper, we provide the first rigorous study on diagnosing elements of large-scale adversarial training on ImageNet, which reveals two intriguing properties. \n\n First, we study the role of normalization. Batch normalization (BN) is a crucial element for achieving state-of-the-art performance on many vision tasks, but we show it may prevent networks from obtaining strong robustness in adversarial training. One unexpected observation is that, for models trained with BN, simply removing clean images from training data largely boosts adversarial robustness, i.e., 18.3%. We relate this phenomenon to the hypothesis that clean images and adversarial images are drawn from two different domains. This two-domain hypothesis may explain the issue of BN when training with a mixture of clean and adversarial images, as estimating normalization statistics of this mixture distribution is challenging. Guided by this two-domain\n[{'summary_text': 'in this paper, we provide the first rigorous study on diagnosing elements of large-scale adversarial training on ImageNet, which reveals two intriguing properties . first, we study the role of normalization. Batch normalization (BN) is a crucial element for achieving state-of-the-art performance on many vision tasks, but we show it may prevent networks from obtaining strong robustness . one unexpected observation is that, for models trained with BN, simply removing clean images from training data largely boosts adversari'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 166/1992 [10:33<2:08:46,  4.23s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep Infomax~(DIM) is an unsupervised representation learning framework by maximizing the mutual information between the inputs and the outputs of an encoder, while probabilistic constraints are imposed on the outputs. In this paper, we propose Supervised Deep InfoMax~(SDIM), which introduces supervised probabilistic constraints to the encoder outputs. The supervised probabilistic constraints are equivalent to a generative classifier on high-level data representations, where class conditional log-likelihoods of samples can be evaluated. Unlike other works building generative classifiers with conditional generative models, SDIMs scale on complex datasets, and can achieve comparable performance with discriminative counterparts.   With SDIM, we could perform \\emph{classification with rejection}.\nInstead of always reporting a class label, SDIM only makes predictions when test samples' largest logits surpass some pre-chosen thresholds, otherwise they will be deemed as out of the data distributions,\n[{'summary_text': \"supervised probabilistic constraints are equivalent to a generative classifier on high-level data representations . with SDIM, we could perform emphclassification with rejection . instead of always reporting a class label, SDIM only makes predictions when test samples' largest logits surpass some pre-chosen thresholds, otherwise they will be deemed as out of the data distributions.\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 167/1992 [10:38<2:08:52,  4.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Natural Language Processing models lack a unified approach to robustness testing. In this paper we introduce WildNLP - a framework for testing model stability in a natural setting where text corruptions such as keyboard errors or misspelling occur. We compare robustness of models from 4 popular NLP tasks: Q&A, NLI, NER and Sentiment Analysis by testing their performance on aspects introduced in the framework. In particular, we focus on a comparison between recent state-of-the- art text representations and non-contextualized word embeddings. In order to improve robust- ness, we perform adversarial training on se- lected aspects and check its transferability to the improvement of models with various cor- ruption types. We find that the high perfor- mance of models does not ensure sufficient robustness, although modern embedding tech- niques help to improve it. We release cor- rupted datasets and code for WildNLP frame- work for the\n[{'summary_text': 'natural language processing models lack a unified approach to robustness testing . in this paper we introduce WildNLP - a framework for testing model stability in a natural setting where text corruptions such as keyboard errors or misspelling occur . we compare robustness of models from 4 popular NLP tasks: Q&A, NLI, NER and Sentiment Analysis by testing their performance on aspects introduced in the framework .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 168/1992 [10:42<2:09:19,  4.25s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Massively multi-label prediction/classification problems arise in environments like health-care or biology where it is useful to make very precise predictions. One challenge with massively multi-label problems is that there is often a long-tailed frequency distribution for the labels, resulting in few positive examples for the rare labels. We propose a solution to this problem by modifying the output layer of a neural network to create a Bayesian network of sigmoids which takes advantage of ontology relationships between the labels to help share information between the rare and the more common labels.   We apply this method to the two massively multi-label tasks of disease prediction (ICD-9 codes) and protein function prediction (Gene Ontology terms) and obtain significant improvements in per-label AUROC and average precision.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'massively multi-label problems arise in environments like health-care or biology where it is useful to make very precise predictions . we propose a solution by modifying the output layer of a neural network to create a Bayesian network of sigmoids which takes advantage of ontology relationships between the labels to help share information between the rare and more common labels .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   8%|▊         | 169/1992 [10:46<2:05:38,  4.14s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In health, machine learning is increasingly common, yet neural network embedding (representation) learning is arguably under-utilized for physiological signals.   This inadequacy stands out in stark contrast to more traditional computer science domains, such as computer vision (CV), and natural language processing (NLP).   For physiological signals, learning feature embeddings is a natural solution to data insufficiency caused by patient privacy concerns -- rather than share data, researchers may share informative embedding models (i.e., representation models), which map patient data to an output embedding.    Here, we present the PHASE (PHysiologicAl Signal Embeddings) framework, which consists of three components: i) learning neural network embeddings of physiological signals, ii) predicting outcomes based on the learned embedding, and iii) interpreting the prediction results by estimating feature attributions in the \"stacked\" models (i.e., feature embedding model followed by prediction\n[{'summary_text': 'in health, machine learning is increasingly common, yet neural network embedding (representation) learning is arguably under-utilized for physiological signals . this inadequacy stands out in stark contrast to more traditional computer science domains, such as computer vision (CV), and natural language processing (NLP).'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▊         | 170/1992 [10:50<2:03:56,  4.08s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose a new framework for entity and event extraction based on generative adversarial imitation learning -- an inverse reinforcement learning method using generative adversarial network (GAN). We assume that instances and labels yield to various extents of difficulty and the gains and penalties (rewards) are expected to be diverse. We utilize discriminators to estimate proper rewards according to the difference between the labels committed by ground-truth (expert) and the extractor (agent).   Experiments also demonstrate that the proposed framework outperforms state-of-the-art methods.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we propose a new framework for entity and event extraction based on generative adversarial imitation learning . the proposed framework outperforms state-of-the-art methods . we assume instances and labels yield to various extents of difficulty .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▊         | 171/1992 [10:53<1:59:12,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Cloud Migration transforms customer’s data, application and services from original IT platform to one or more cloud en- vironment, with the goal of improving the performance of the IT system while reducing the IT management cost. The enterprise level Cloud Migration projects are generally com- plex, involves dynamically planning and replanning various types of transformations for up to 10k endpoints. Currently the planning and replanning in Cloud Migration are generally done manually or semi-manually with heavy dependency on the migration expert’s domain knowledge, which takes days to even weeks for each round of planning or replanning. As a result, automated planning engine that is capable of gener- ating high quality migration plan in a short time is particu- larly desirable for the migration industry. In this short paper, we briefly introduce the advantages of using AI planning in Cloud Migration, a preliminary prototype, as well as the challenges the requires attention from the planning and scheduling society.<|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the enterprise level Cloud Migration projects are generally com- plex, involves dynamically planning and replanning various types of transformations for up to 10k endpoints . as a result, automated planning engine that is capable of gener-ating high quality migration plan in a short time is particu- larly desirable for the migration industry .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▊         | 172/1992 [10:57<1:57:53,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We consider the dictionary learning problem, where the aim is to model the given data as a linear combination of a few columns of a matrix known as a dictionary, where the sparse weights forming the linear combination are known as coefficients. Since the dictionary and coefficients, parameterizing the linear model are unknown, the corresponding optimization is inherently non-convex. This was a major challenge until recently, when provable algorithms for dictionary learning were proposed. Yet, these provide guarantees only on the recovery of the dictionary, without explicit recovery guarantees on the coefficients. Moreover, any estimation error in the dictionary adversely impacts the ability to successfully localize and estimate the coefficients. This potentially limits the utility of existing provable dictionary learning methods in applications where coefficient recovery is of interest. To this end, we develop NOODL: a simple Neurally plausible alternating Optimization-based Online Dictionary Learning algorithm, which recovers both the dictionary and coefficients exactly at a geometric rate, when initialized appropriately. Our algorithm, NOODL,\n[{'summary_text': 'we consider the dictionary learning problem, where the aim is to model the given data as a linear combination of a few columns . the sparse weights forming the linear combination are known as coefficients . since the dictionary and coefficients, parameterizing the linear model are unknown, the corresponding optimization is inherently non-convex . this potentially limits the utility of existing provable dictionary learning methods in applications where coefficient recovery is of interest .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▊         | 173/1992 [11:01<2:02:22,  4.04s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"GloVe and Skip-gram word embedding methods learn word vectors by decomposing a denoised matrix of word co-occurrences into a product of low-rank matrices. In this work, we propose an iterative algorithm for computing word vectors based on modeling word co-occurrence matrices with Generalized Low Rank Models. Our algorithm generalizes both Skip-gram and GloVe as well as giving rise to other embedding methods based on the specified co-occurrence matrix, distribution of co-occurences, and the number of iterations in the iterative algorithm. For example, using a Tweedie distribution with one iteration results in GloVe and using a Multinomial distribution with full-convergence mode results in Skip-gram. Experimental results demonstrate that multiple iterations of our algorithm improves results over the GloVe method on the Google word analogy similarity task.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we propose an iterative algorithm for computing word vectors based on modeling word co-occurrence matrices with Generalized Low Rank Models . our algorithm generalizes both Skip-gram and GloVe as well as giving rise to other embedding methods . for example, using a Tweedie distribution with one iteration results in a . Multinomial distribution with full-convergence mode results . in skip-gram .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▊         | 174/1992 [11:05<2:02:58,  4.06s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We develop a new algorithm for imitation learning from a single expert demonstration. In contrast to many previous one-shot imitation learning approaches, our algorithm does not assume access to more than one expert demonstration during the training phase. Instead, we leverage an exploration policy to acquire unsupervised trajectories, which are then used to train both an encoder and a context-aware imitation policy. The optimization procedures for the encoder, imitation learner, and exploration policy are all tightly linked. This linking creates a feedback loop wherein the exploration policy collects new demonstrations that challenge the imitation learner, while the encoder attempts to help the imitation policy to the best of its abilities. We evaluate our algorithm on 6 MujoCo robotics tasks.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'our algorithm does not assume access to more than one expert demonstration during the training phase . instead, we leverage an exploration policy to acquire unsupervised trajectories, which are then used to train both an encoder and a context-aware imitation policy . the optimization procedures for the encoder, imitation learner, and exploration policy are all tightly linked .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 175/1992 [11:09<2:00:16,  3.97s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Temporal difference (TD) learning is a popular algorithm for policy evaluation in reinforcement learning, but the vanilla TD can substantially suffer from the inherent optimization variance. A variance reduced TD (VRTD) algorithm was proposed by Korda and La (2015), which applies the variance reduction technique directly to the online TD learning with Markovian samples. In this work, we first point out the technical errors in the analysis of VRTD in Korda and La (2015), and then provide a mathematically solid analysis of the non-asymptotic convergence of VRTD and its variance reduction performance. We show that VRTD is guaranteed to converge to a neighborhood of the fixed-point solution of TD at a linear convergence rate. Furthermore, the variance error (for both i.i.d. and Markovian sampling) and the bias error (for Markovian sampling) of VRTD are significantly reduced by the batch size of variance reduction in comparison to those of vanilla TD.<|endoftext|>\n[{'summary_text': 'variance reduced TD (VRTD) learning is a popular algorithm for policy evaluation in reinforcement learning . but the vanilla TD can substantially suffer from the inherent optimization variance . we first point out technical errors in the analysis of VRTD in Korda and La (2015) .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 176/1992 [11:13<1:56:54,  3.86s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Holistically exploring the perceptual and neural representations underlying animal communication has traditionally been very difficult because of the complexity of the underlying signal. We present here a novel set of techniques to project entire communicative repertoires into low dimensional spaces that can be systematically sampled from, exploring the relationship between perceptual representations, neural representations, and the latent representational spaces learned by machine learning algorithms. We showcase this method in one ongoing experiment studying sequential and temporal maintenance of context in songbird neural and perceptual representations of syllables. We further discuss how studying the neural mechanisms underlying the maintenance of the long-range information content present in birdsong can inform and be informed by machine sequence modeling.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we present here a novel set of techniques to project entire communicative repertoires into low dimensional spaces that can be systematically sampled from . we showcase this method in one ongoing experiment studying sequential and temporal maintenance of context in songbird neural .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 177/1992 [11:16<1:53:25,  3.75s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Distributed optimization is vital in solving large-scale machine learning problems. A widely-shared feature of distributed optimization techniques is the requirement that all nodes complete their assigned tasks in each computational epoch before the system can proceed to the next epoch. In such settings, slow nodes, called stragglers, can greatly slow progress. To mitigate the impact of stragglers, we propose an online distributed optimization method called Anytime Minibatch. In this approach, all nodes are given a fixed time to compute the gradients of as many data samples as possible. The result is a variable per-node minibatch size. Workers then get a fixed communication time to average their minibatch gradients via several rounds of consensus, which are then used to update primal variables via dual averaging. Anytime Minibatch prevents stragglers from holding up the system without wasting the work that stragglers can complete. We present a convergence analysis and analyze the wall time performance. Our numerical results show that our\n[{'summary_text': 'distributed optimization is vital in solving large-scale machine learning problems . in such settings, slow nodes, called stragglers, can greatly slow progress . we propose an online distributed optimization method called Anytime Minibatch .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 178/1992 [11:20<1:53:14,  3.75s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Multi-relational graph embedding which aims at achieving effective representations with reduced low-dimensional parameters, has been widely used in knowledge base completion. Although knowledge base data usually contains tree-like or cyclic structure, none of existing approaches can embed these data into a compatible space that in line with the structure. To overcome this problem, a novel framework, called Riemannian TransE, is proposed in this paper to embed the entities in a Riemannian manifold. Riemannian TransE models each relation as a move to a point and defines specific novel distance dissimilarity for each relation, so that all the relations are naturally embedded in correspondence to the structure of data. Experiments on several knowledge base completion tasks have shown that, based on an appropriate choice of manifold, Riemannian TransE achieves good performance even with a significantly reduced parameters.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'multi-relational graph embedding has been widely used in knowledge base completion . Riemannian TransE models each relation as a move to a point . no existing approaches can embed these data into a compatible space .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 179/1992 [11:24<1:50:22,  3.65s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Minecraft is a videogame that offers many interesting challenges for AI systems. In this paper, we focus in construction scenarios where an agent must build a complex structure made of individual blocks. As higher-level objects are formed of lower-level objects, the construction can naturally be modelled as a hierarchical task network. We model a house-construction scenario in classical and HTN planning and compare the advantages and disadvantages of both kinds of models.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this paper, we focus in construction scenarios where an agent must build a complex structure made of individual blocks . as higher-level objects are formed, the construction can naturally be modelled as a hierarchical task network .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 180/1992 [11:27<1:47:11,  3.55s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Statistical inference methods are fundamentally important in machine learning. Most state-of-the-art inference algorithms are \n variants of Markov chain Monte Carlo (MCMC) or variational inference (VI). However, both methods struggle with limitations in practice: MCMC methods can be computationally demanding; VI methods may have large bias. \n In this work, we aim to improve upon MCMC and VI by a novel hybrid method based on the idea of reducing simulation bias of finite-length MCMC chains using gradient-based optimisation. The proposed method can generate low-biased samples by increasing the length of MCMC simulation and optimising the MCMC hyper-parameters, which offers attractive balance between approximation bias and computational efficiency. We show that our method produces promising results on popular benchmarks when compared to recent hybrid methods of MCMC and VI.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'most state-of-the-art inference algorithms are variants of markov chain Monte Carlo (MCMC) or variational inference (VI). however, both methods struggle with limitations in practice: MCMC methods can be computationally demanding; VI methods may have large bias .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 181/1992 [11:30<1:47:09,  3.55s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Despite alarm over the reliance of machine learning systems on so-called spurious patterns in training data, the term lacks coherent meaning in standard statistical frameworks. However, the language of causality offers clarity: spurious associations are those due to a common cause (confounding) vs direct or indirect effects. In this paper, we focus on NLP, introducing methods and resources for training models insensitive to spurious patterns. Given documents and their initial labels, we task humans with revise each document to accord with a counterfactual target label, asking that the revised documents be internally coherent while avoiding any gratuitous changes. Interestingly, on sentiment analysis and natural language inference tasks, classifiers trained on original data fail on their counterfactually-revised counterparts and vice versa. Classifiers trained on combined datasets perform remarkably well, just shy of those specialized to either domain. While classifiers trained on either original or manipulated data alone  are sensitive to spurious features (e.g., mentions of genre), models trained on the\n[{'summary_text': 'despite alarm over the reliance of machine learning systems on spurious patterns in training data, the term lacks coherent meaning in standard statistical frameworks . however, the language of causality offers clarity: spurious associations are those due to a common cause (confounding) vs direct or indirect effects .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 182/1992 [11:34<1:50:59,  3.68s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned\n[{'summary_text': 'constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned . in many cases it decreases the number of parameter updates required to achieve low training error . however, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 183/1992 [11:39<1:55:11,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Hashing-based collaborative filtering learns binary vector representations (hash codes) of users and items, such that recommendations can be computed very efficiently using the Hamming distance, which is simply the sum of differing bits between two hash codes. A problem with hashing-based collaborative filtering using the Hamming distance, is that each bit is equally weighted in the distance computation, but in practice some bits might encode more important properties than other bits, where the importance depends on the user. \n To this end, we propose an end-to-end trainable variational hashing-based collaborative filtering approach that uses the novel concept of self-masking: the user hash code acts as a mask on the items (using the Boolean AND operation), such that it learns to encode which bits are important to the user, rather than the user's preference towards the underlying item property that the bits represent. This allows a binary user-level importance weighting of each item without the need to store additional weights for each user\n[{'summary_text': 'hashing-based collaborative filtering learns binary vector representations (hash codes) of users and items, such that recommendations can be computed very efficiently using the Hamming distance . each bit is equally weighted in the distance computation, but in practice some bits might encode more important properties than other bits, where the importance depends on the user .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 184/1992 [11:43<2:00:18,  3.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present a tool for Interactive Visual Exploration of Latent Space (IVELS) for model selection.   Evaluating generative models of discrete sequences from a  continuous  latent  space  is  a  challenging  problem,  since  their  optimization involves multiple competing objective terms.   We introduce a model-selection pipeline  to  compare  and  filter  models  throughout  consecutive  stages  of  more complex and expensive metrics. We present the pipeline in an interactive visual tool to enable the exploration of the metrics, analysis of the learned latent space, and selection of the best model for a given task.   We focus specifically on the variational auto-encoder family in a case study of modeling peptide sequences, which are short sequences of amino acids. This task is especially interesting due to the presence of multiple attributes we want to model. We demonstrate how an interactive visual comparison can assist in evaluating how well an unsupervised auto-encoder meaningfully\n[{'summary_text': 'generative models of discrete sequences from a continuous latent space are a challenging problem, since their optimization involves multiple competing objective terms . we introduce a model-selection pipeline to compare and filter models throughout consecutive stages of more complex and expensive metrics . this task is especially interesting due to the presence of multiple attributes we want to model .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 185/1992 [11:47<2:01:02,  4.02s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Dynamical system models (including RNNs) often lack the ability to adapt the sequence generation or prediction to a given context, limiting their real-world application. In this paper we show that hierarchical multi-task dynamical systems (MTDSs) provide direct user control over sequence generation, via use of a latent  code z that specifies the customization to the\n individual data sequence. This enables style transfer, interpolation and morphing within generated sequences. We show the MTDS can improve predictions via latent code interpolation, and avoid the long-term performance degradation of standard RNN approaches.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'hierarchical multi-task dynamical systems (MTDSs) provide direct user control over sequence generation via use of a latent code z that specifies the customization to the individual data sequence . this enables style transfer, interpolation and morphing within generated sequences .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 186/1992 [11:51<1:58:14,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The ever-increasing size of modern datasets combined with the difficulty of obtaining label information has made semi-supervised learning of significant practical importance in modern machine learning applications. In comparison to supervised learning, the key difficulty in semi-supervised learning is how to make full use of the unlabeled data. In order to utilize manifold information provided by unlabeled data, we propose a novel regularization called the tangent-normal adversarial regularization, which is composed by two parts. The two parts complement with each other and jointly enforce the smoothness along two different directions that are crucial for semi-supervised learning. One is applied along the tangent space of the data manifold, aiming to enforce local invariance of the classifier on the manifold, while the other is performed on the normal space orthogonal to the tangent space, intending to impose robustness on the classifier against the noise causing the observed data deviating from the underlying data manifold.   Both of the two\n[{'summary_text': 'in comparison to supervised learning, the key difficulty in semi-supervised learning is how to make full use of the unlabeled data . in order to utilize manifold information, we propose a novel regularization called the tangent-normal adversarial regularization . the two parts complement with each other and jointly enforce the smoothness along two different directions .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 187/1992 [11:55<2:00:18,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Fine-grained Entity Recognition (FgER) is the task of detecting and classifying entity mentions to a large set of types spanning diverse domains such as biomedical, finance and sports.    We  observe  that  when  the  type  set  spans  several  domains,  detection  of  entity mention becomes a limitation for supervised learning models.   The primary reason being lack  of  dataset  where  entity  boundaries  are  properly  annotated  while  covering  a  large spectrum of entity types.   Our work directly addresses this issue.   We propose Heuristics Allied with Distant Supervision (HAnDS) framework to automatically construct a quality dataset suitable for the FgER task.   HAnDS framework exploits the high interlink among Wikipedia  and  Freebase  in  a  pipelined  manner,  reducing  annotation  errors  introduced by naively using distant supervision approach\n[{'summary_text': 'fine-grained Entity Recognition (FgER) is the task of detecting and classifying entity mentions to a large set of types spanning diverse domains such as biomedical, finance and sports . when the type set spans several domains, detection of entity mention becomes a limitation for supervised learning models . HAnDS framework exploits the high interlink among Wikipedia and Freebase in a pipelined manner .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 188/1992 [11:59<2:03:09,  4.10s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep neural networks (DNNs) perform well on a variety of tasks despite the fact that most used in practice are vastly overparametrized and even capable of perfectly fitting randomly labeled data. Recent evidence suggests that developing \"compressible\" representations is key for adjusting the complexity of overparametrized networks to the task at hand and avoiding overfitting (Arora et al., 2018; Zhou et al., 2018). In this paper, we provide new empirical evidence that supports this hypothesis, identifying two independent mechanisms that emerge when the network’s width is increased: robustness (having units that can be removed without affecting accuracy) and redundancy (having units with similar activity). In a series of experiments with AlexNet, ResNet and Inception networks in the CIFAR-10 and ImageNet datasets, and also using shallow networks with synthetic data, we show that DNNs consistently increase either their robustness, their redundancy, or both at greater widths for a\n[{'summary_text': 'recent evidence suggests developing \"compressible\" representations is key for adjusting complexity of overparametrized networks to task at hand and avoiding overfitting . in a series of experiments with AlexNet, ResNet and Inception networks, we show that DNNs consistently increase either their robustness, their redundancy, or both at greater widths for a . aDeep neural networks (DNNs) perform well on a variety of tasks despite the fact that most used in practice are'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:   9%|▉         | 189/1992 [12:04<2:07:29,  4.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep learning training accesses vast amounts of data at high velocity, posing challenges for datasets retrieved over commodity networks and storage devices. We introduce a way to dynamically reduce the overhead of fetching and transporting training data with a method we term Progressive Compressed Records (PCRs). PCRs deviate from previous formats by leveraging progressive compression to split each training example into multiple examples of increasingly higher fidelity, without adding to the total data size. Training examples of similar fidelity are grouped together, which reduces both the system overhead and data bandwidth needed to train a model. We show that models can be trained on aggressively compressed representations of the training data and still retain high accuracy, and that PCRs can enable a 2x speedup on average over baseline formats using JPEG compression. Our results hold across deep learning architectures for a wide range of datasets: ImageNet, HAM10000, Stanford Cars, and CelebA-HQ.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we introduce a way to dynamically reduce the overhead of fetching and transporting training data . PCRs deviate from previous formats by leveraging progressive compression to split each training example into multiple examples of increasingly higher fidelity . training examples of similar fidelity are grouped together, which reduces both the system overhead and data bandwidth needed to train a model .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|▉         | 190/1992 [12:07<2:02:47,  4.09s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Click Through Rate (CTR) prediction is a critical task in industrial applications, especially for online social and commerce applications. It is challenging to find a proper way to automatically discover the effective cross features in CTR tasks. We propose a novel model for CTR tasks, called Deep neural networks with Encoder enhanced Factorization Machine (DeepEnFM). Instead of learning the cross features directly, DeepEnFM adopts the Transformer encoder as a backbone to align the feature embeddings with the clues of other fields. The embeddings generated from encoder are beneficial for the further feature interactions. Particularly, DeepEnFM utilizes a bilinear approach to generate different similarity functions with respect to different field pairs. Furthermore, the max-pooling method makes DeepEnFM feasible to capture both the supplementary and suppressing information among different attention heads. Our model is validated on the Criteo and Avazu datasets, and achieves state-of-art performance.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we propose a novel model for CTR tasks, called Deep neural networks with Encoder enhanced Factorization Machine (DeepEnFM) instead of learning the cross features directly, DeepEnFM adopts the Transformer encoder as a backbone to align the feature embeddings with the clues of other fields .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|▉         | 191/1992 [12:11<2:00:52,  4.03s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Modern neural networks are over-parametrized. In particular, each rectified linear hidden unit can be modified by a multiplicative factor by adjusting input and out- put weights, without changing the rest of the network. Inspired by the Sinkhorn-Knopp algorithm, we introduce a fast iterative method for minimizing the l2 norm of the weights, equivalently the weight decay regularizer. It provably converges to a unique solution. Interleaving our algorithm with SGD during training improves the test accuracy. For small batches, our approach offers an alternative to batch- and group- normalization on CIFAR-10 and ImageNet with a ResNet-18.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'each rectified linear hidden unit can be modified by a multiplicative factor by adjusting input and out- put weights, without changing the rest of the network . a fast iterative method for minimizing the l2 norm of the weights . it provably converges to a unique solution .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|▉         | 192/1992 [12:15<1:57:47,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software, websites, and mobile applications. In this paper, we show that deep learning methods can be leveraged to train a model end-to-end to automatically generate code from a single input image with over 77% of accuracy for three different platforms (i.e. iOS, Android and web-based technologies).<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this paper, we show that deep learning methods can be leveraged to train a model end-to-end to automatically generate code from a single input image with over 77% of accuracy for three different platforms . a designer created a screenshot into computer code is a typical task conducted by a developer in order to build customized software, websites, and mobile applications .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|▉         | 193/1992 [12:19<1:56:23,  3.88s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Identifying analogies across domains without supervision is a key task for artificial intelligence. Recent advances in cross domain image mapping have concentrated on translating images across domains. Although the progress made is impressive, the visual fidelity many times does not suffice for identifying the matching sample from the other domain. In this paper, we tackle this very task of finding exact analogies between datasets i.e. for every image from domain A find an analogous image in domain B. We present a matching-by-synthesis approach: AN-GAN, and show that it outperforms current techniques. We further show that the cross-domain mapping task can be broken into two parts: domain alignment and learning the mapping function. The tasks can be iteratively solved, and as the alignment is improved, the unsupervised translation function reaches quality comparable to full supervision.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'recent advances in cross domain image mapping have concentrated on translating images across domains . the progress made is impressive, but the visual fidelity many times does not suffice for identifying the matching sample from the other domain .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|▉         | 194/1992 [12:22<1:51:53,  3.73s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Representation learning is one of the foundations of Deep Learning and allowed important improvements on several Machine Learning tasks, such as Neural Machine Translation, Question Answering and Speech Recognition. Recent works have proposed new methods for learning representations for nodes and edges in graphs. Several of these methods are based on the SkipGram algorithm, and they usually process a large number of multi-hop neighbors in order to produce the context from which node representations are learned. In this paper, we propose an effective and also efficient method for generating node embeddings in graphs that employs a restricted number of permutations over the immediate neighborhood of a node as context to generate its representation, thus ego-centric representations. We present a thorough evaluation showing that our method outperforms state-of-the-art methods in six different datasets related to the problems of link prediction and node classification, being one to three orders of magnitude faster than baselines when generating node embeddings for very large graphs.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'recent works have proposed new methods for learning representations for nodes and edges in graphs . several of these methods are based on the SkipGram algorithm . they usually process a large number of multi-hop neighbors in order to produce the context from which node representations are learned .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|▉         | 195/1992 [12:26<1:54:09,  3.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose\n[{'summary_text': 'this paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster . we begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). this similarity is category-agnostic and can be learned from data in the source domain using a similarity network .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|▉         | 196/1992 [12:31<1:58:43,  3.97s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper, a deep boosting algorithm is developed to\n learn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs (base experts)\n with diverse capabilities, e.g., these base deep CNNs are\n sequentially trained to recognize a set of \n object classes in an easy-to-hard way according to their\n learning complexities. Our experimental results have demonstrated\n that our deep boosting algorithm can significantly improve the\n accuracy rates on large-scale visual recognition.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'a deep boosting algorithm is developed to learn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs (base experts) with diverse capabilities . our experimental results have demonstrated that our deep-boosting algorithm can significantly improve the accuracy rates on large-scale visual recognition .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|▉         | 197/1992 [12:34<1:55:10,  3.85s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Multivariate time series with missing values are common in areas such as healthcare and finance, and have grown in number and complexity over the years. This raises the question whether deep learning methodologies can outperform classical data imputation methods in this domain. However, naive applications of deep learning fall short in giving reliable confidence estimates and lack interpretability. We propose a new deep sequential latent variable model for dimensionality reduction and data imputation. Our modeling assumption is simple and interpretable: the high dimensional time series has a lower-dimensional representation which evolves smoothly in time according to a Gaussian process. The non-linear dimensionality reduction in the presence of missing data is achieved using a VAE approach with a novel structured variational approximation. We demonstrate that our approach outperforms several classical and deep learning-based data imputation methods on high-dimensional data from the domains of computer vision and healthcare, while additionally improving the smoothness of the imputations and providing interpretable uncertainty estimates.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'multivariate time series with missing values are common in areas such as healthcare and finance, and have grown in number and complexity over the years . this raises the question whether deep learning methodologies can outperform classical data imputation methods in this domain . however, naive applications of deep learning fall short in giving reliable confidence estimates and lack interpretability .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|▉         | 198/1992 [12:38<1:54:30,  3.83s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The key attribute that drives the unprecedented success of modern Recurrent Neural Networks (RNNs) on learning tasks which involve sequential data, is their ever-improving ability to model intricate long-term temporal dependencies. However, a well established measure of RNNs' long-term memory capacity is lacking, and thus formal understanding of their ability to correlate data throughout time is limited. Though depth efficiency in convolutional networks is well established by now, it does not suffice in order to account for the success of deep RNNs on inputs of varying lengths, and the need to address their 'time-series expressive power' arises. In this paper, we analyze the effect of depth on the ability of recurrent networks to express correlations ranging over long time-scales. To meet the above need, we introduce a measure of the information flow across time that can be supported by the network, referred to as the Start-End separation rank. Essentially, this measure reflects the distance of the function realized\n[{'summary_text': \"the key attribute that drives the unprecedented success of modern Recurrent Neural Networks (RNNs) on learning tasks which involve sequential data is their ever-improving ability to model intricate long-term temporal dependencies . however, a well established measure of RNNs' long -term memory capacity is lacking, and thus formal understanding of their ability to correlate data throughout time is limited . in this paper, we analyze the effect of depth on the ability of recurrent networks to express correlations ranging over long time-scale\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|▉         | 199/1992 [12:42<2:00:52,  4.04s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Stochastic gradient descent (SGD) with stochastic momentum is popular in nonconvex stochastic optimization and particularly for the training of deep neural networks. In standard SGD, parameters are updated by improving along the path of the gradient at the current iterate on a batch of examples, where the addition of a ``momentum'' term biases the update in the direction of the previous change in parameters. In non-stochastic convex optimization one can show that a momentum adjustment provably reduces convergence time in many settings, yet such results have been elusive in the stochastic and non-convex settings. At the same time, a widely-observed empirical phenomenon is that in training deep networks stochastic momentum appears to significantly improve convergence time, variants of it have flourished in the development of other popular update methods, e.g. ADAM, AMSGrad, etc. Yet theoretical justification for the use of stochastic momentum has remained a\n[{'summary_text': \"in standard SGD, parameters are updated by improving along the path of the gradient at the current iterate on a batch of examples . the addition of a momentum'' term biases the update in the direction of the previous change in parameters . in non-stochastic convex optimization one can show that a momentum adjustment provably reduces convergence time, variants of it have flourished in the development of other popular update methods .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|█         | 200/1992 [12:47<2:04:08,  4.16s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The use of imitation learning to learn a single policy for a complex task that has multiple modes or hierarchical structure can be challenging. In fact, previous work has shown that when the modes are known, learning separate policies for each mode or sub-task can greatly improve the performance of imitation learning. In this work, we discover the interaction between sub-tasks from their resulting state-action trajectory sequences using a directed graphical model. We propose a new algorithm based on the generative adversarial imitation learning framework which automatically learns sub-task policies from unsegmented demonstrations. Our approach maximizes the directed information flow in the graphical model between sub-task latent variables and their generated trajectories. We also show how our approach connects with the existing Options framework, which is commonly used to learn hierarchical policies.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the use of imitation learning to learn a single policy for a complex task that has multiple modes or hierarchical structure can be challenging . in this work, we discover the interaction between sub-tasks from their resulting state-action trajectory sequences using a directed graphical model . we propose a new algorithm based on the generative adversarial imitation learning framework .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|█         | 201/1992 [12:51<2:01:38,  4.07s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.\n[{'summary_text': 'many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs) this problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features . in experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|█         | 202/1992 [12:55<2:03:49,  4.15s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In spite of their great success, traditional factorization algorithms typically do not support features (e.g., Matrix Factorization), or their complexity scales quadratically with the number of features (e.g, Factorization Machine). On the other hand, neural methods allow large feature sets, but are often designed for a specific application. We propose novel deep factorization methods that allow efficient and flexible feature representation. For example, we enable describing items with natural language with complexity linear to the vocabulary size—this enables prediction for unseen items and avoids the cold start problem. We show that our architecture can generalize some previously published single-purpose neural architectures. Our experiments suggest improved training times and accuracy compared to shallow methods.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'traditional factorization algorithms typically do not support features (e.g., Matrix Factorization), or their complexity scales quadratically with the number of features . neural methods allow large feature sets, but are often designed for a specific application .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|█         | 203/1992 [12:59<1:57:58,  3.96s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"State-of-the-art performances on language comprehension tasks are achieved by huge language models pre-trained on massive unlabeled text corpora, with very light subsequent fine-tuning in a task-specific supervised manner. It seems the pre-training procedure learns a very good common initialization for further training on various natural language understanding tasks, such that only few steps need to be taken in the parameter space to learn each task. In this work, using Bidirectional Encoder Representations from Transformers (BERT) as an example, we verify this hypothesis by showing that task-specific fine-tuned language models are highly close in parameter space to the pre-trained one. Taking advantage of such observations, we further show that the fine-tuned versions of these huge models, having on the order of $10^8$ floating-point parameters, can be made very computationally efficient. First, fine-tuning only a fraction of critical layers suffices. Second, fine\n[{'summary_text': 'fineState-of-the-art performances on language comprehension tasks are achieved by huge language models pre-trained on unlabeled text corpora, with very light subsequent fine-tuning in a task-specific supervised manner . it seems the pre-training procedure learns a very good common initialization for further training on various natural language understanding tasks .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|█         | 204/1992 [13:03<1:59:38,  4.01s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Plan recognition aims to look for target plans to best explain the observed actions based on plan libraries and/or domain models. Despite the success of previous approaches on plan recognition, they mostly rely on correct action observations. \n Recent advances in visual activity recognition have the potential of enabling applications such as automated video surveillance. Effective approaches for such problems would require the ability to recognize the plans of agents from video information. Traditional plan recognition algorithms rely on access to detailed planning domain models. One recent promising direction involves learning approximate (or shallow) domain models directly from the observed activity sequences. Such plan recognition approaches expect observed action sequences as inputs. However, visual inference results are often noisy and uncertain, typically represented as a distribution over possible actions. In this work, we develop a visual plan recognition framework that recognizes plans with an approximate domain model learned from uncertain visual data.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'plan recognition aims to look for target plans to best explain the observed actions based on plan libraries and/or domain models . recent advances in visual activity recognition have the potential of enabling applications such as automated video surveillance . effective approaches for such problems would require the ability to recognize the plans of agents from video information.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|█         | 205/1992 [13:06<1:55:50,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Classification systems typically act in isolation, meaning they are required to implicitly memorize the characteristics of all candidate classes in order to classify. The cost of this is increased memory usage and poor sample efficiency. We propose a model which instead verifies using reference images during the classification process, reducing the burden of memorization. The model uses iterative non-differentiable queries in order to classify an image. We demonstrate that such a model is feasible to train and can match baseline accuracy while being more parameter efficient. However, we show that finding the correct balance between image recognition and verification is essential to pushing the model towards desired behavior, suggesting that a pipeline of recognition followed by verification is a more promising approach towards designing more powerful networks with simpler architectures.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'classification systems act in isolation, meaning they are required to implicitly memorize the characteristics of all candidate classes in order to classify . the cost of this is increased memory usage and poor sample efficiency . we propose a model which instead verifies using reference images during the classification process .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|█         | 206/1992 [13:10<1:52:38,  3.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the\n[{'summary_text': 'non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models . knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|█         | 207/1992 [13:14<1:55:26,  3.88s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Neural population responses to sensory stimuli can exhibit both nonlinear stimulus- dependence and richly structured shared variability. Here, we show how adversarial training can be used to optimize neural encoding models to capture both the deterministic and stochastic components of neural population data. To account for the discrete nature of neural spike trains, we use the REBAR method to estimate unbiased gradients for adversarial optimization of neural encoding models. We illustrate our approach on population recordings from primary visual cortex. We show that adding latent noise-sources to a convolutional neural network yields a model which captures both the stimulus-dependence and noise correlations of the population activity.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we illustrate our approach on population recordings from primary visual cortex . we show that adding latent noise-sources to a convolutional neural network yields a model which captures both the stimulus-dependence and noise correlations of the population activity .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|█         | 208/1992 [13:17<1:51:47,  3.76s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before.   We use simple feed-forward encoder and decoder networks, thus our model is an attractive candidate for applications where the encoding and decoding speed is critical. Additionally, this  allows us to only sample autoregressively in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of  VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known\n[{'summary_text': 'to this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before . we use simple feed-forward encoder and decoder networks, thus our model is an attractive candidate for applications where the encoding and decoding speed is critical . also, this allows us to only sample autoegressively in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  10%|█         | 209/1992 [13:22<1:59:16,  4.01s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Knowledge graphs are structured representations of real world facts. However, they typically contain only a small subset of all possible facts. Link prediction is the task of inferring missing facts based on existing ones. We propose TuckER, a relatively simple yet powerful linear model based on Tucker decomposition of the binary tensor representation of knowledge graph triples. By using this particular decomposition, parameters are shared between relations, enabling multi-task learning. TuckER outperforms previous state-of-the-art models across several standard link prediction datasets.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we propose TuckER, a relatively simple yet powerful linear model based on Tucker decomposition of the binary tensor representation of knowledge graph triples . by using this particular dedecomposition, parameters are shared between relations, enabling multi-task learning . tuckER outperforms previous state-of-the-art models across several standard link prediction datasets.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 210/1992 [13:26<1:57:17,  3.95s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We outline the problem of concept drifts for time series data. In this work, we analyze the temporal inconsistency of streaming wireless signals in the context of device-free passive indoor localization. We show that data obtained from WiFi channel state information (CSI) can be used to train a robust system capable of performing room level localization. One of the most challenging issues for such a system is the movement of input data distribution to an unexplored space over time, which leads to an unwanted shift in the learned boundaries of the output space. In this work, we propose a phase and magnitude augmented feature space along with a standardization technique that is little affected by drifts. We show that this robust representation of the data yields better learning accuracy and requires less number of retraining.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we show that data obtained from WiFi channel state information can be used to train a robust system capable of performing room level localization . one of the most challenging issues for such a system is the movement of input data distribution to an unexplored space over time .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 211/1992 [13:29<1:53:28,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Despite a growing literature on explaining neural networks, no consensus has been reached on how to explain a neural network decision or how to evaluate an explanation.\n\t Our contributions in this paper are twofold. First, we investigate schemes to combine explanation methods and reduce model uncertainty to obtain a single aggregated explanation. The aggregation is more robust and aligns better with the neural network than any single explanation method..\n\t Second, we propose a new approach to evaluating explanation methods that circumvents the need for manual evaluation and is not reliant on the alignment of neural networks and humans decision processes.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'despite a growing literature on explaining neural networks, no consensus has been reached on how to explain a neural network decision or how to evaluate an explanation . our contributions in this paper are twofold: first, we investigate schemes to combine explanation methods and reduce model uncertainty . the aggregation is more robust and aligns better with the neural network than any single explanation method .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 212/1992 [13:33<1:52:27,  3.79s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"This paper studies the undesired phenomena of over-sensitivity of representations learned by deep networks to semantically-irrelevant changes in data. We identify a cause for this shortcoming in the classical Variational Auto-encoder (VAE) objective, the evidence lower bound (ELBO). We show that the ELBO fails to control the behaviour of the encoder out of the support of the empirical data distribution and this behaviour of the VAE can lead to extreme errors in the learned representation. This is a key hurdle in the effective use of representations for data-efficient learning and transfer. To address this problem, we propose to augment the data with specifications that enforce insensitivity of the representation with respect to families of transformations. To incorporate these specifications, we propose a regularization method that is based on a selection mechanism that creates a fictive data point by explicitly perturbing an observed true data point. For certain choices of parameters, our formulation naturally leads to the minimization of the entropy regular\n[{'summary_text': 'this paper studies the undesired phenomena of over-sensitivity of representations learned by deep networks to semantically-irrelevant changes in data . we identify a cause for this shortcoming in the classical Variational Auto-encoder (VAE) objective, the evidence lower bound (ELBO) we show that the ELBO fails to control the behaviour of the encoder out of the support of the empirical data distribution . this behavior of the VAE can lead to extreme errors in the learned representation .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 213/1992 [13:38<1:59:03,  4.02s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Learning theory tells us that more data is better when minimizing the generalization error of identically distributed training and test sets. However, when training and test distribution differ, this distribution shift can have a significant effect. With a novel perspective on function transfer learning, we are able to lower bound the change of performance when transferring from training to test set with the Wasserstein distance between the embedded training and test set distribution. We find that there is a trade-off affecting performance between how invariant a function is to changes in training and test distribution and how large this shift in distribution is. Empirically across several data domains, we substantiate this viewpoint by showing that test performance correlates strongly with the distance in data distributions between training and test set. Complementary to the popular belief that more data is always better, our results highlight the utility of also choosing a training data distribution that is close to the test data distribution when the learned function is not invariant to such changes.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'learning theory tells us that more data is better when minimizing the generalization error of identically distributed training and test sets . however, when training and testing distribution differ, this distribution shift can have a significant effect . we find that there is a trade-off affecting performance between how invariant a function is to changes .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 214/1992 [13:41<1:56:26,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'implication of this is that a filter may know what it is looking at, but not where it is positioned in the image . information concerning absolute position is inherently useful, and it is reasonable to assume deep CNNs may implicitly learn to encode this information if there is a means to do so .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 215/1992 [13:45<1:53:20,  3.83s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Natural Language Inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such architecture, Densely Interactive Inference Network (DIIN), demonstrates the state-of-the-art performance on large scale NLI copora and large-scale NLI alike corpus. It's noteworthy that DIIN achieve a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'natural language inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and an artificial language hypothesis . interactive inference network (IIN) is a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 216/1992 [13:49<1:52:22,  3.80s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recent pretrained sentence encoders achieve state of the art results on language understanding tasks, but does this mean they have implicit knowledge of syntactic structures? We introduce a grammatically annotated development set for the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018), which we use to investigate the grammatical knowledge of three pretrained encoders, including the popular OpenAI Transformer (Radford et al., 2018) and BERT (Devlin et al., 2018). We fine-tune these encoders to do acceptability classification over CoLA and compare the models’ performance on the annotated analysis set. Some phenomena, e.g. modification by adjuncts, are easy to learn for all models, while others, e.g. long-distance movement, are learned effectively only by models with strong overall performance, and others still, e.g. morphological agreement, are hardly learned by any model.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'pretrained encoders achieve state of the art results on language understanding tasks . does this mean they have implicit knowledge of syntactic structures? we introduce a grammatically annotated development set for the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018)'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 217/1992 [13:52<1:51:57,  3.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We investigate methods to efficiently learn diverse strategies in reinforcement learning for a generative structured prediction problem: query reformulation. In the proposed framework an agent consists of multiple specialized sub-agents and a meta-agent that learns to aggregate the answers from sub-agents to produce a final answer. Sub-agents are trained on disjoint partitions of the training data, while the meta-agent is trained on the full training set. Our method makes learning faster, because it is highly parallelizable, and has better generalization performance than strong baselines, such as\n an ensemble of agents trained on the full data. We evaluate on the tasks of document retrieval and question answering. The\n improved performance seems due to the increased diversity of reformulation strategies. This suggests that multi-agent, hierarchical approaches might play an important role in structured prediction tasks of this kind. However, we also find that it is not obvious how to characterize diversity in this context, and a first attempt based on clustering did not produce\n[{'summary_text': 'agents are trained on disjoint partitions of training data, while the meta-agent is trained on the full training set . our method makes learning faster, because it is highly parallelizable, and has better generalization performance than strong baselines .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 218/1992 [13:56<1:53:34,  3.84s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly ﬁnd the optimal network parameters and sparse network structure in a uniﬁed optimization process with trainable pruning thresholds. These thresholds can have ﬁne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and efﬁciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures . these thresholds can have fine-grained layer-wise adjustments dynamically via backpropagation . we have several surprising observations that provide strong evidence to the effectiveness and efficiency of our algorithm .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 219/1992 [14:00<1:51:35,  3.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The reparameterization trick has become one of the most useful tools in the field of variational inference. However, the reparameterization trick is based on the standardization transformation which restricts the scope of application of this method to distributions that have tractable inverse cumulative distribution functions or are expressible as deterministic transformations of such distributions. In this paper, we generalized the reparameterization trick by allowing a general transformation. We discover that the proposed model is a special case of control variate indicating that the proposed model can combine the advantages of CV and generalized reparameterization. Based on the proposed gradient model, we propose a new polynomial-based gradient estimator which has better theoretical performance than the reparameterization trick under certain condition and can be applied to a larger class of variational distributions. In studies of synthetic and real data, we show that our proposed gradient estimator has a significantly lower gradient variance than other state-of-the-art methods thus enabling a\n[{'summary_text': 'the reparameterization trick is based on the standardization transformation which restricts the scope of application of this method to distributions that have tractable inverse cumulative distribution functions . in studies of synthetic and real data, we show that our proposed gradient estimator has a significantly lower gradient variance than other state-of-the-art methods thus enabling a .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 220/1992 [14:04<1:54:48,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at https://attackingrl.github.io.<|endoftext|><|endoftext|>\n[{'summary_text': \"an attacker is not usually able to directly modify another agent's observations . this might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 221/1992 [14:08<1:51:30,  3.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora . there have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 222/1992 [14:11<1:49:10,  3.70s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The capability of reliably detecting out-of-distribution samples is one of the key factors in deploying a good classifier, as the test distribution always does not match with the training distribution in most real-world applications. In this work, we propose a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks. Unlike the discriminative (or softmax) classifier that only focuses on the decision boundary partitioning its latent space into multiple regions, our generative classifier aims to explicitly model class-conditional distributions as separable Gaussian distributions. Thereby, we can define the confidence score by the distance between a test sample and the center of each distribution. Our empirical evaluation on multi-class images and tabular data demonstrate that the generative classifier achieves the best performances in distinguishing out-of-distribution samples, and also\n[{'summary_text': 'the test distribution always does not match with the training distribution in most real-world applications . in this work, we propose a deep generative classifier which is effective to detect out-of-distribution samples . we can define the confidence score by the distance between a test sample and the center of each distribution . the generative classification aims to explicitly model class-conditional distributions as separable Gaussian distributions .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 223/1992 [14:16<1:54:39,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose RaPP, a new methodology for novelty detection by utilizing hidden space activation values obtained from a deep autoencoder.\n Precisely, RaPP compares input and its autoencoder reconstruction not only in the input space but also in the hidden spaces.\n We show that if we feed a reconstructed input to the same autoencoder again, its activated values in a hidden space are equivalent to the corresponding reconstruction in that hidden space given the original input.\n In order to aggregate the hidden space activation values, we propose two metrics, which enhance the novelty detection performance.\n Through extensive experiments using diverse datasets, we validate that RaPP improves novelty detection performances of autoencoder-based approaches.\n Besides, we show that RaPP outperforms recent novelty detection methods evaluated on popular benchmarks.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'RaPP compares input and its autoencoder reconstruction not only in the input space but also in the hidden spaces . in order to aggregate the hidden space activation values, we propose two metrics, which enhance the novelty detection performance .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█         | 224/1992 [14:19<1:50:06,  3.74s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Disentangling underlying generative factors of a data distribution is important for interpretability and generalizable representations. In this paper,  we introduce two novel disentangling methods. Our first method, Unlabeled Disentangling GAN (UD-GAN, unsupervised), decomposes the latent noise by generating similar/dissimilar image pairs and it learns a distance metric on these pairs with siamese networks and a contrastive loss. This pairwise approach provides consistent representations for similar data points. Our second method (UD-GAN-G, weakly supervised) modifies the UD-GAN with user-defined guidance functions, which restrict the information that goes into the siamese networks. This constraint helps UD-GAN-G to focus on the desired semantic variations in the data. We  show  that  both  our  methods  outperform  existing  unsupervised approaches in quantitative metrics that measure semantic accuracy of the learned representations. In addition, we illustrate that\n[{'summary_text': 'unlabeled disentangling GAN decomposes latent noise by generating similar/dissimilar image pairs . it learns a distance metric on these pairs with siamese networks and a contrastive loss . this pairwise approach provides consistent representations for similar data points .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█▏        | 225/1992 [14:23<1:53:03,  3.84s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"What can we learn about the functional organization of cortical microcircuits from large-scale recordings of neural activity?   To obtain an explicit and interpretable model of time-dependent functional connections between neurons and to establish the dynamics of the cortical information flow, we develop 'dynamic neural relational inference' (dNRI). We study  both synthetic and real-world neural spiking data and demonstrate that the developed method is able to uncover the dynamic relations between neurons more reliably than existing baselines.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': \"to obtain an explicit and interpretable model of time-dependent functional connections between neurons and to establish the dynamics of the cortical information flow, we develop 'dynamic neural relational inference' (dNRI) the developed method is able to uncover the dynamic relations between neurons more reliably than existing baselines .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█▏        | 226/1992 [14:27<1:50:16,  3.75s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We consider two questions at the heart of machine learning; how can we predict if a minimum will generalize to the test set, and why does stochastic gradient descent find minima that generalize well? Our work responds to \\citet{zhang2016understanding}, who showed deep neural networks can easily memorize randomly labeled training data, despite generalizing well on real labels of the same inputs. We show that the same phenomenon occurs in small linear models. These observations are explained by the Bayesian evidence, which penalizes sharp minima but is invariant to model parameterization. We also demonstrate that, when one holds the learning rate fixed, there is an optimum batch size which maximizes the test set accuracy. We propose that the noise introduced by small mini-batches drives the parameters towards minima whose evidence is large. Interpreting stochastic gradient descent as a stochastic differential equation, we identify the ``noise scale\" $g = \\epsilon\n[{'summary_text': 'our work responds to citetzhang2016understanding, who showed deep neural networks can easily memorize randomly labeled training data, despite generalizing well on real labels of the same inputs . the same phenomenon occurs in small linear models, which are explained by the Bayesian evidence, which penalizes sharp minima but is invariant to model parameterization .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█▏        | 227/1992 [14:31<1:54:01,  3.88s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Many challenging prediction problems, from molecular optimization to program synthesis, involve creating complex structured objects as outputs. However, available training data may not be sufficient for a generative model to learn all possible complex transformations. By leveraging the idea that evaluation is easier than generation, we show how a simple, broadly applicable, iterative target augmentation scheme can be surprisingly effective in guiding the training and use of such models. Our scheme views the generative model as a prior distribution, and employs a separately trained filter as the likelihood. In each augmentation step, we filter the model's outputs to obtain additional prediction targets for the next training epoch. Our method is applicable in the supervised as well as semi-supervised settings. We demonstrate that our approach yields significant gains over strong baselines both in molecular optimization and program synthesis. In particular, our augmented model outperforms the previous state-of-the-art in molecular optimization by over 10% in absolute gain.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'a simple, broadly applicable, iterative target augmentation scheme can be surprisingly effective in guiding training and use of such models . our scheme views the generative model as a prior distribution, and employs a separately trained filter as the likelihood . augmented model outperforms previous state-of-the-art in molecular optimization by over 10% in absolute gain .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█▏        | 228/1992 [14:35<1:54:02,  3.88s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"There are many differences between convolutional networks and the ventral visual streams of primates. For example, standard convolutional networks lack recurrent and lateral connections, cell dynamics, etc. However, their feedforward architectures are somewhat similar to the ventral stream, and warrant a more detailed comparison. A recent study found that the feedforward architecture of the visual cortex could be closely approximated as a convolutional network, but the resulting architecture differed from widely used deep networks in several ways. The same study also found, somewhat surprisingly, that training the ventral stream of this network for object recognition resulted in poor performance. This paper examines the performance of this network in more detail. In particular, I made a number of changes to the ventral-stream-based architecture, to make it more like a DenseNet, and tested performance at each step. I chose DenseNet because it has a high BrainScore, and because it has some cortex-like architectural features such as large in-\n[{'summary_text': 'a recent study found that the feedforward architecture of the visual cortex could be closely approximated as a convolutional network, but the resulting architecture differed from widely used deep networks in several ways . the same study also found, somewhat surprisingly, that training the ventral stream of this network for object recognition resulted in poor performance .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  11%|█▏        | 229/1992 [14:39<1:55:49,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"A major component of overfitting in model-free reinforcement learning (RL) involves the case where the agent may mistakenly correlate reward with certain spurious features from the observations generated by the Markov Decision Process (MDP). We provide a general framework for analyzing this scenario, which we use to design multiple synthetic benchmarks from only modifying the observation space of an MDP. When an agent overfits to different observation spaces even if the underlying MDP dynamics is fixed, we term this observational overfitting. Our experiments expose intriguing properties especially with regards to implicit regularization, and also corroborate results from previous works in RL generalization and supervised learning (SL).<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'a major component of overfitting in model-free reinforcement learning (RL) involves the case where the agent may mistakenly correlate reward with certain spurious features from the observations generated by the Markov Decision Process (MDP) our experiments expose intriguing properties especially with regards to implicit regularization, and corroborate results from previous works in RL generalization and supervised learning (SL).'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  12%|█▏        | 230/1992 [14:43<1:54:31,  3.90s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Many biological learning systems such as the mushroom body, hippocampus, and cerebellum are built from sparsely connected networks of neurons. For a new understanding of such networks, we study the function spaces induced by sparse random features and characterize what functions may and may not be learned. A network with d inputs per neuron is found to be equivalent to an additive model of order d, whereas with a degree distribution the network combines additive terms of different orders. We identify three specific advantages of sparsity: additive function approximation is a powerful inductive bias that limits the curse of dimensionality, sparse networks are stable to outlier noise in the inputs, and sparse random features are scalable. Thus, even simple brain architectures can be powerful function approximators. Finally, we hope that this work helps popularize kernel theories of networks among computational neuroscientists.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'a network with d inputs per neuron is found to be equivalent to an additive model of order d, whereas with a degree distribution the network combines additive terms of different orders . additive function approximation is a powerful inductive bias that limits the curse of dimensionality, sparse networks are stable to outlier noise in the inputs . so, even simple brain architectures can be powerful function apps . we hope that this work helps popularize kernel theories of networks among computational neuroscientists'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  12%|█▏        | 231/1992 [14:47<1:56:54,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We derive reverse-mode (or adjoint) automatic differentiation for solutions of stochastic differential equations (SDEs), allowing time-efficient and constant-memory computation of pathwise gradients, a continuous-time analogue of the reparameterization trick.\n Specifically, we construct a backward SDE whose solution is the gradient and provide conditions under which numerical solutions converge.\n We also combine our stochastic adjoint approach with a stochastic variational inference scheme for continuous-time SDE models, allowing us to learn distributions over functions using stochastic gradient descent.\n Our latent SDE model achieves competitive performance compared to existing approaches on time series modeling.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we derive reverse-mode (or adjoint) automatic differentiation for solutions of stochastic differential equations (SDEs) allowing time-efficient and constant-memory computation of pathwise gradients, a continuous-time analogue of the reparameterization trick . our latent SDE model achieves competitive performance compared to existing approaches on time series modeling .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  12%|█▏        | 232/1992 [14:51<1:55:42,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The selection of initial parameter values for gradient-based optimization of deep neural networks is one of the most impactful hyperparameter choices in deep learning systems, affecting both convergence times and model performance. Yet despite significant empirical and theoretical analysis, relatively little has been proved about the concrete effects of different initialization schemes. In this work, we analyze the effect of initialization in deep linear networks, and provide for the first time a rigorous proof that drawing the initial weights from the orthogonal group speeds up convergence relative to the standard Gaussian initialization with iid weights. We show that for deep networks, the width needed for efficient convergence to a global minimum with orthogonal initializations is independent of the depth, whereas the width needed for efficient convergence with Gaussian initializations scales linearly in the depth. Our results demonstrate how the benefits of a good initialization can persist throughout learning, suggesting an explanation for the recent empirical successes found by initializing very deep non-linear networks according to the principle of dynamical is\n[{'summary_text': 'selection of initial parameter values for gradient-based optimization of deep neural networks is one of the most impactful hyperparameter choices in deep learning systems . but despite significant empirical and theoretical analysis, relatively little has been proved about the concrete effects of different initialization schemes . we show that for deep networks, the width needed for efficient convergence to a global minimum with orthogonal initializations is independent of the depth .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  12%|█▏        | 233/1992 [14:55<2:01:03,  4.13s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Machine learning (ML) research has investigated prototypes: examples that are representative of the behavior to be learned. We systematically evaluate five methods for identifying prototypes, both ones previously introduced as well as new ones we propose, finding all of them to provide meaningful but different interpretations. Through a human study, we confirm that all five metrics are well matched to human intuition. Examining cases where the metrics disagree offers an informative perspective on the properties of data and algorithms used in learning, with implications for data-corpus construction, efficiency, adversarial robustness, interpretability, and other ML aspects. In particular, we confirm that the \"train on hard\" curriculum approach can improve accuracy on many datasets and tasks, but that it is strictly worse when there are many mislabeled or ambiguous examples.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'research has investigated prototypes: examples that are representative of the behavior to be learned . we systematically evaluate five methods for identifying prototypes, both ones previously introduced as well as new ones we propose, finding all of them to provide meaningful but different interpretations . through a human study, we confirm that all five metrics are well matched to human intuition .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  12%|█▏        | 234/1992 [14:59<1:57:08,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"There have been multiple attempts with variational auto-encoders (VAE) to learn powerful global representations of complex data using a combination of latent stochastic variables and an autoregressive model over the dimensions of the data. However, for the most challenging natural image tasks the purely autoregressive model with stochastic variables still outperform the combined stochastic autoregressive models. In this paper, we present simple additions to the VAE framework that generalize to natural images by embedding spatial information in the stochastic layers. We significantly improve the state-of-the-art results on MNIST, OMNIGLOT, CIFAR10 and ImageNet when the feature map parameterization of the stochastic variables are combined with the autoregressive PixelCNN approach. Interestingly, we also observe close to state-of-the-art results without the autoregressive part. This opens the possibility for high quality image generation with only one forward-pass\n[{'summary_text': 'multiple attempts with variational auto-encoders (VAE) to learn powerful global representations of complex data using a combination of latent stochastic variables and an autoregressive model over the dimensions of the data . however, for the most challenging natural image tasks, the purely autoregressionive model with stocheastic variables still outperform the combined stochaastic autogressive models .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  12%|█▏        | 235/1992 [15:03<1:59:19,  4.07s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Reinforcement learning (RL) typically defines a discount factor as part of the Markov Decision Process.   The discount factor values future rewards by an exponential scheme that leads to theoretical convergence guarantees of the Bellman equation. However, evidence from psychology, economics and neuroscience suggests that humans and animals instead have hyperbolic time-preferences.   Here we extend earlier work of Kurth-Nelson and Redish and propose an efficient deep reinforcement learning agent that acts via hyperbolic discounting and other non-exponential discount mechanisms. We demonstrate that a simple approach approximates hyperbolic discount functions while still using familiar temporal-difference learning techniques in RL.   Additionally, and independent of hyperbolic discounting, we make a surprising discovery that simultaneously learning value functions over multiple time-horizons is an effective auxiliary task which often improves over state-of-the-art methods.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'discount factor values future rewards by an exponential scheme that leads to theoretical convergence guarantees of the Bellman equation . however, evidence from psychology, economics and neuroscience suggests that humans and animals instead have hyperbolic time-preferences .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  12%|█▏        | 236/1992 [15:07<1:53:47,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose a new sample-efficient methodology, called Supervised Policy Update (SPU), for deep reinforcement learning. Starting with data generated by the current policy, SPU formulates and solves a constrained optimization problem in the non-parameterized proximal policy space. Using supervised regression, it then converts the optimal non-parameterized policy to a parameterized policy, from which it draws new samples. The methodology is general in that it applies to both discrete and continuous action spaces, and can handle a wide variety of proximity constraints for the non-parameterized optimization problem. We show how the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems, and the Proximal Policy Optimization (PPO) problem can be addressed by this methodology. The SPU implementation is much simpler than TRPO. In terms of sample efficiency, our extensive experiments show SPU outperforms TRPO in Mujoco simulated robotic tasks and outperforms PPO in\n[{'summary_text': 'supervised regression converts the optimal non-parameterized policy to a parameterized policy, from which it draws new samples . the methodology is general in that it applies to both discrete and continuous action spaces, and can handle a wide variety of proximity constraints .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  12%|█▏        | 237/1992 [15:10<1:53:03,  3.87s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Inspired by the adaptation phenomenon of biological neuronal firing, we propose regularity normalization: a reparameterization of the activation in the neural network that take into account the statistical regularity in the implicit space. By considering the neural network optimization process as a model selection problem, the implicit space is constrained by the normalizing factor, the minimum description length of the optimal universal code. We introduce an incremental version of computing this universal code as normalized maximum likelihood and demonstrated its flexibility to include data prior such as top-down attention and other oracle information and its compatibility to be incorporated into batch normalization and layer normalization. The preliminary results showed that the proposed method outperforms existing normalization methods in tackling the limited and imbalanced data from a non-stationary distribution benchmarked on computer vision task. As an unsupervised attention mechanism given input data, this biologically plausible normalization has the potential to deal with other complicated real-world scenarios as well as reinforcement learning setting where the rewards are sparse\n[{'summary_text': 'we propose regularity normalization: a reparameterization of the activation in the neural network that take into account the statistical regularity in the implicit space . by considering neural network optimization process as a model selection problem, the implicit spaces are constrained by the normalizing factor, the minimum description length of the optimal universal code . the proposed method outperforms existing normalization methods in tackling the limited and imbalanced data from a non-stationary distribution benchmarked on computer vision task .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  12%|█▏        | 238/1992 [15:15<1:59:08,  4.08s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Knowledge Graph Embedding (KGE) is the task of jointly learning entity and relation embeddings for a given knowledge graph. Existing methods for learning KGEs can be seen as a two-stage process where (a) entities and relations in the knowledge graph are represented using some linear algebraic structures (embeddings), and (b) a scoring function is defined that evaluates the strength of a relation that holds between two entities using the corresponding relation and entity embeddings. Unfortunately, prior proposals for the scoring functions in the first step have been heuristically motivated, and it is unclear as to how the scoring functions in KGEs relate to the generation process of the underlying knowledge graph. To address this issue, we propose a generative account of the KGE learning task. Specifically, given a knowledge graph represented by a set of relational triples (h, R, t), where the semantic relation R holds between the two entities h (head) and t (\n[{'summary_text': 'existing methods for learning KGEs can be seen as a two-stage process . a scoring function is defined that evaluates the strength of a relation that holds between two entities using the corresponding relation and entity embeddings . prior proposals for the scoring functions in the first step have been heuristically motivated . to address this issue, we propose a generative account of the KGE learning task .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  12%|█▏        | 239/1992 [15:19<2:00:55,  4.14s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We introduce Quantum Graph Neural Networks (QGNN), a new class of quantum neural network ansatze which are tailored to represent quantum processes which have a graph structure, and are particularly suitable to be executed on distributed quantum systems over a quantum network. Along with this general class of ansatze, we introduce further specialized architectures, namely, Quantum Graph Recurrent Neural Networks (QGRNN) and Quantum Graph Convolutional Neural Networks (QGCNN). We provide four example applications of QGNN's: learning Hamiltonian dynamics of quantum systems, learning how to create multipartite entanglement in a quantum network, unsupervised learning for spectral clustering, and supervised learning for graph isomorphism classification.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': \"Quantum Graph Neural Networks introduces new class of quantum neural network ansatze . they are tailored to represent quantum processes which have a graph structure . we provide four example applications of QGNN's: learning Hamiltonian dynamics .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  12%|█▏        | 240/1992 [15:23<1:54:37,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The complex world around us is inherently multimodal and sequential (continuous). Information is scattered across different modalities and requires multiple continuous sensors to be captured. As machine learning leaps towards better generalization to real world, multimodal sequential learning becomes a fundamental research area. Arguably,  modeling arbitrarily distributed spatio-temporal dynamics within and across modalities is the biggest challenge in this research area. In this paper, we present a new transformer model, called the Factorized Multimodal Transformer (FMT) for multimodal sequential learning. FMT inherently models the intramodal and intermodal (involving two or more modalities) dynamics within its multimodal input in a factorized manner. The proposed factorization allows for increasing the number of self-attentions to better model the multimodal phenomena at hand; without encountering difficulties during training (e.g. overfitting) even on relatively low-resource setups. All the\n[{'summary_text': 'the complex world around us is inherently multimodal and sequential (continuous) information is scattered across different modalities and requires multiple continuous sensors to be captured . as machine learning leaps towards better generalization to real world, multimodal sequential learning becomes a fundamental research area .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  12%|█▏        | 241/1992 [15:27<1:55:07,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep Convolution Neural Networks (CNNs), rooted by the pioneer work of  \\cite{Hinton1986,LeCun1985,Alex2012}, and summarized in \\cite{LeCunBengioHinton2015},   have been shown to be very useful in a variety of fields.   The  state-of-the art CNN machines such as image rest net \\cite{He_2016_CVPR} are described by real value inputs and kernel convolutions followed by the local and  non-linear rectified linear outputs.   Understanding the role of these layers, the accuracy and limitations of them,  as well as making them more efficient (fewer parameters)  are all ongoing research questions. \n \n  Inspired in quantum theory, we propose the use of complex value kernel functions, followed by the local non-linear  absolute (modulus) operator square. We argue that an advantage of quantum inspired complex kernels is robust\n[{'summary_text': 'the state-of-the art CNN machines such as image rest net citeHe_2016_CVPR are described by real value inputs and kernel convolutions followed by the local and non-linear rectified linear outputs . understanding the role of these layers, the accuracy and limitations of them, as well as making them more efficient (fewer parameters) are all ongoing research questions .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  12%|█▏        | 242/1992 [15:31<1:57:18,  4.02s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We consider the problem of information compression from high dimensional data. Where many studies consider the problem of compression by non-invertible trans- formations, we emphasize the importance of invertible compression. We introduce new class of likelihood-based auto encoders with pseudo bijective architecture, which we call Pseudo Invertible Encoders. We provide the theoretical explanation of their principles. We evaluate Gaussian Pseudo Invertible Encoder on MNIST, where our model outperform WAE and VAE in sharpness of the generated images.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we introduce new class of likelihood-based auto encoders with pseudo bijective architecture, which we call Pseudo Invertible Encoders . we provide the theoretical explanation of their principles .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  12%|█▏        | 243/1992 [15:34<1:50:55,  3.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"As machine learning becomes ubiquitous, deployed systems need to be as accu- rate as they can. As a result, machine learning service providers have a surging need for useful, additional training data that benefits training, without giving up all the details about the trained program. At the same time, data owners would like to trade their data for its value, without having to first give away the data itself be- fore receiving compensation. It is difficult for data providers and model providers to agree on a fair price without first revealing the data or the trained model to the other side. Escrow systems only complicate this further, adding an additional layer of trust required of both parties. Currently, data owners and model owners don’t have a fair pricing system that eliminates the need to trust a third party and training the model on the data, which 1) takes a long time to complete, 2) does not guarantee that useful data is paid valuably and that useless data isn’t, without trusting in\n[{'summary_text': 'machine learning service providers have a surging need for useful, additional training data that benefits training, without giving up all the details about the trained program . it is difficult for data owners and model providers to agree on a fair price without first revealing the data or the trained model to the other side .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  12%|█▏        | 244/1992 [15:38<1:51:56,  3.84s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Modeling hypernymy, such as poodle is-a dog, is an important generalization aid to many NLP tasks, such as entailment, relation extraction, and question answering. Supervised learning from labeled hypernym sources, such as WordNet, limit the coverage of these models, which can be addressed by learning hypernyms from unlabeled text.   Existing unsupervised methods either do not scale to large vocabularies or yield unacceptably poor accuracy.   This paper introduces {\\it distributional inclusion vector embedding (DIVE)}, a simple-to-implement unsupervised method of hypernym discovery via per-word non-negative vector embeddings which preserve the inclusion property of word contexts. In experimental evaluations more comprehensive than any previous literature of which we are aware---evaluating on 11 datasets using multiple existing as well as newly proposed scoring functions---we find that our method provides up to double the precision of previous\n[{'summary_text': 'supervised learning from labeled hypernym sources, such as WordNet, limit the coverage of these models . existing unsupervised methods either do not scale to large vocabularies or yield unacceptably poor accuracy .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  12%|█▏        | 245/1992 [15:42<1:51:22,  3.83s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Historically, the pursuit of efficient inference has been one of the driving forces be-hind the research into new deep learning architectures and building blocks. Some of the recent examples include:  the squeeze-and-excitation module of (Hu et al.,2018), depthwise separable convolutions in Xception (Chollet, 2017), and the inverted bottleneck in MobileNet v2 (Sandler et al., 2018).   Notably, in all of these cases, the resulting building blocks enabled not only higher efficiency, but also higher accuracy, and found wide adoption in the field. In this work, we further expand the arsenal of efficient building blocks for neural network architectures; but instead of combining standard primitives (such as convolution), we advocate for the replacement of these dense primitives with their sparse counterparts.   While the idea of using sparsity to decrease the parameter count is not new (Mozer & Smolensky, 1989), the conventional wisdom is that\n[{'summary_text': 'Historically, the pursuit of efficient inference has been one of the driving forces be-hind the research into new deep learning architectures and building blocks . some of the recent examples include: the squeeze-and-excitation module of (Hu et al.,2018), depthwise separable convolutions in Xception (Chollet, 2017), and the inverted bottleneck in MobileNet v2 (Sandler, 2018). notably, in all of these cases, the resulting building blocks enabled not'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  12%|█▏        | 246/1992 [15:46<1:57:38,  4.04s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"While great progress has been made at making neural networks effective across a wide range of tasks, many are surprisingly vulnerable to small, carefully chosen perturbations of their input, known as adversarial examples. In this paper, we advocate for and experimentally investigate the use of logit regularization techniques as an adversarial defense, which can be used in conjunction with other methods for creating adversarial robustness at little to no cost. We demonstrate that much of the effectiveness of one recent adversarial defense mechanism can be attributed to logit regularization and show how to improve its defense against both white-box and black-box attacks, in the process creating a stronger black-box attacks against PGD-based models.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this paper, we advocate for and experimentally investigate the use of logit regularization techniques as an adversarial defense . many are surprisingly vulnerable to small, carefully chosen perturbations of their input . this paper shows how to improve its defense against both white-box and black-box attacks .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  12%|█▏        | 247/1992 [15:50<1:55:19,  3.97s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': \"this poses the question of how to compactly represent all the information that the population codes carry across all these dimensions . we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information . in particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  12%|█▏        | 248/1992 [15:54<1:56:23,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generative adversarial nets (GANs) are widely used to learn the data sampling process and their performance may heavily depend on the loss functions, given a limited computational budget. This study revisits MMD-GAN that uses the maximum mean discrepancy (MMD) as the loss function for GAN and makes two contributions. First, we argue that the existing MMD loss function may discourage the learning of fine details in data as it attempts to contract the discriminator outputs of real data. To address this issue, we propose a repulsive loss function to actively learn the difference among the real data by simply rearranging the terms in MMD. Second, inspired by the hinge loss, we propose a bounded Gaussian kernel to stabilize the training of MMD-GAN with the repulsive loss function. The proposed methods are applied to the unsupervised image generation tasks on CIFAR-10, STL-10, CelebA, and LSUN bedroom datasets. Results show that the repulsive loss\n[{'summary_text': 'this study revisits MMD-GAN that uses the maximum mean discrepancy (MMD) as the loss function for GAN and makes two contributions . first, we argue that the existing MMD loss function may discourage the learning of fine details in data as it attempts to contract the discriminator outputs of real data . the proposed methods are applied to the unsupervised image generation tasks on CIFAR-10, STL-10, CelebA, and LSUN bedroom datasets .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  12%|█▎        | 249/1992 [15:59<2:00:41,  4.15s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Inspired by the success of generative adversarial networks (GANs) in image domains, we introduce a novel hierarchical architecture for learning characteristic topological features from a single arbitrary input graph via GANs. The hierarchical architecture consisting of multiple GANs preserves both local and global topological features, and automatically partitions the input graph into representative stages for feature learning. The stages facilitate reconstruction and can be used as indicators of the importance of the associated topological structures. Experiments show that our method produces subgraphs retaining a wide range of topological features, even in early reconstruction stages. This paper contains original research on combining the use of GANs and graph topological analysis.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'hierarchical architecture consisting of multiple GANs preserves both local and global topological features, and automatically partitions the input graph into representative stages for feature learning . the stages facilitate reconstruction and can be used as indicators of the importance of the associated topological structures .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  13%|█▎        | 250/1992 [16:02<1:54:29,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Imitation learning from demonstrations usually relies on learning a policy from trajectories of optimal states and actions. However, in real life expert demonstrations, often the action information is missing and only state trajectories are available. We present a model-based imitation learning method that can learn environment-specific optimal actions only from expert state trajectories. Our proposed method starts with a model-free reinforcement learning algorithm with a heuristic reward signal to sample environment dynamics, which is then used to train the state-transition probability. Subsequently, we learn the optimal actions from expert state trajectories by supervised learning, while back-propagating the error gradients through the modeled environment dynamics. Experimental evaluations show that our proposed method successfully achieves performance similar to (state, action) trajectory-based traditional imitation learning methods even in the absence of action information, with much fewer iterations compared to conventional model-free reinforcement learning methods. We also demonstrate that our method can learn to act from only video demonstrations of expert agent for\n[{'summary_text': 'we present a model-based imitation learning method that can learn environment-specific optimal actions only from expert state trajectories . our proposed method starts with a heuristic reward signal to sample environment dynamics, which is then used to train the state-transition probability . supervised learning, while back-propagating the error gradients through the modeled environment dynamics .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  13%|█▎        | 251/1992 [16:06<1:56:04,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner . the learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  13%|█▎        | 252/1992 [16:10<1:52:31,  3.88s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"For typical sequence prediction problems such as language generation, maximum likelihood estimation (MLE) has commonly been adopted as it encourages the predicted sequence most consistent with the ground-truth sequence to have the highest probability of occurring. However, MLE focuses on once-to-all matching between the predicted sequence and gold-standard, consequently treating all incorrect predictions as being equally incorrect. We refer to this drawback as {\\it negative diversity ignorance} in this paper. Treating all incorrect predictions as equal unfairly downplays the nuance of these sequences' detailed token-wise structure. To counteract this, we augment the MLE loss by introducing an extra Kullback--Leibler divergence term derived by comparing a data-dependent Gaussian prior and the detailed training prediction. The proposed data-dependent Gaussian prior objective (D2GPo) is defined over a prior topological order of tokens and is poles apart from the data-independent Gaussian prior (L2 regularization) commonly adopted in smoothing the\n[{'summary_text': 'maximum likelihood estimation (MLE) has commonly been adopted . it encourages the predicted sequence most consistent with the ground-truth sequence to have the highest probability of occurring . however, MLE focuses on once-to-all matching between predicted sequence and gold-standard, consequently treating all incorrect predictions as being equally incorrect .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  13%|█▎        | 253/1992 [16:14<1:53:27,  3.91s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Simultaneous machine translation models start generating a target sequence before they have encoded or read the source sequence. Recent approach for this task either apply a fixed policy on transformer, or a learnable monotonic attention on a weaker recurrent neural network based structure. In this paper, we propose a new attention mechanism, Monotonic Multihead Attention (MMA), which introduced the monotonic attention mechanism to multihead attention. We also introduced two novel interpretable approaches for latency control that are specifically designed for multiple attentions. We apply MMA to the simultaneous machine translation task and demonstrate better latency-quality tradeoffs compared to MILk, the previous state-of-the-art approach. Code will be released upon publication.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'recent approach for this task either apply a fixed policy on transformer, or a learnable monotonic attention on a weaker recurrent neural network based structure . we also introduced two novel interpretable approaches for latency control that are specifically designed for multiple attentions .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  13%|█▎        | 254/1992 [16:18<1:50:39,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recurrent neural networks (RNNs) can learn continuous vector representations of symbolic structures such as sequences and sentences; these representations often exhibit linear regularities (analogies).  Such regularities motivate our hypothesis that RNNs that show such regularities implicitly compile symbolic structures into tensor product representations (TPRs; Smolensky, 1990), which additively combine tensor products of vectors representing roles (e.g.,  sequence positions) and vectors representing fillers (e.g., particular words) . To test this hypothesis, we introduce Tensor Product Decomposition Networks (TPDNs), which use TPRs to approximate existing vector representations . We demonstrate using synthetic data that TPDNs can successfully approximate linear and tree-based RNN autoencoder representations, suggesting that these representations exhibit interpretable compositional structure; we explore the settings that lead RNNs to induce such structure-sensitive representations .  By contrast, further TPDN experiments show that\n[{'summary_text': 'Recurrent neural networks (RNNs) can learn continuous vector representations of symbolic structures such as sequences and sentences . these representations often exhibit linear regularities (analogies)'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  13%|█▎        | 255/1992 [16:21<1:48:35,  3.75s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose Support-guided Adversarial Imitation Learning (SAIL), a generic imitation learning framework that unifies support estimation of the expert policy with the family of Adversarial Imitation Learning (AIL) algorithms. SAIL addresses two important challenges of AIL, including the implicit reward bias and potential training instability. We also show that SAIL is at least as efficient as standard AIL. In an extensive evaluation, we demonstrate that the proposed method effectively handles the reward bias and achieves better performance and training stability than other baseline methods on a wide range of benchmark control tasks.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'support-guided Adversarial Imitation Learning (SAIL) addresses two important challenges of AIL, including the implicit reward bias and potential training instability . we also show that SAIL is at least as efficient as standard AIL .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  13%|█▎        | 256/1992 [16:24<1:45:17,  3.64s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models.\n\n In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts with a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered object concepts into new images. We show\n[{'summary_text': 'visualization and understanding of GANs is largely missing . how does a GAN represent our visual world internally? what causes the artifacts in GAN results?'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  13%|█▎        | 257/1992 [16:28<1:45:41,  3.65s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Despite the growing interest in continual learning, most of its contemporary works have been studied in a rather restricted setting where tasks are clearly distinguishable, and task boundaries are known during training. However, if our goal is to develop an algorithm that learns as humans do, this setting is far from realistic, and it is essential to develop a methodology that works in a task-free manner. Meanwhile, among several branches of continual learning, expansion-based methods have the advantage of eliminating catastrophic forgetting by allocating new resources to learn new data. In this work, we propose an expansion-based approach for task-free continual learning. Our model, named Continual Neural Dirichlet Process Mixture (CN-DPM), consists of a set of neural network experts that are in charge of a subset of the data. CN-DPM expands the number of experts in a principled way under the Bayesian nonparametric framework. With extensive experiments, we show that our model successfully performs task-free continual learning\n[{'summary_text': 'despite the growing interest in continual learning, most of its contemporary works have been studied in a rather restricted setting where tasks are clearly distinguishable, and task boundaries are known during training . however, if our goal is to develop an algorithm that learns as humans do, this setting is far from realistic .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  13%|█▎        | 258/1992 [16:32<1:48:33,  3.76s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Counterfactual regret minimization (CFR) is a fundamental and effective technique for solving Imperfect Information Games (IIG). However, the original CFR algorithm only works for discrete states and action spaces, and the resulting strategy is maintained as a tabular representation. Such tabular representation limits the method from being directly applied to large games. In this paper, we propose a double neural representation for the IIGs, where one neural network represents the cumulative regret, and the other represents the average strategy.   Such neural representations allow us to avoid manual game abstraction and carry out end-to-end optimization. To make the learning efficient, we also developed several novel techniques including a robust sampling method and a mini-batch Monte Carlo Counterfactual Regret Minimization (MCCFR) method, which may be of independent interests.   Empirically, on games tractable to tabular approaches, neural strategies trained with our algorithm converge comparably to their tabular counterparts, and significantly\n[{'summary_text': 'counterfactual regret minimization (CFR) is a fundamental and effective technique for solving Imperfect Information Games (IIG) however, the original CFR algorithm only works for discrete states and action spaces, and the resulting strategy is maintained as a tabular representation . such neural representations allow us to avoid manual game abstraction and carry out end-to-end optimization .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  13%|█▎        | 259/1992 [16:36<1:51:35,  3.86s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Data augmentation (DA) is fundamental against overfitting in large convolutional neural networks, especially with a limited training dataset. In images, DA is usually based on heuristic transformations, like geometric or color transformations. Instead of using predefined transformations, our work learns data augmentation directly from the training data by learning to transform images with an encoder-decoder architecture combined with a spatial transformer network. The transformed images still belong to the same class, but are new, more complex samples for the classifier. Our experiments show that our approach is better than previous generative data augmentation methods, and comparable to predefined transformation methods when training an image classifier.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'data augmentation (DA) is fundamental against overfitting in large convolutional neural networks, especially with a limited training dataset . in images, DA is usually based on heuristic transformations, like geometric or color transformations . the transformed images still belong to the same class, but are new, more complex samples for the classifier .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  13%|█▎        | 260/1992 [16:40<1:52:11,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"State-of-the-art results on neural machine translation often use attentional sequence-to-sequence models with some form of convolution or recursion. Vaswani et. al. (2017) propose a new architecture that avoids recurrence and convolution completely. Instead, it uses only self-attention and feed-forward layers. While the proposed architecture achieves state-of-the-art results on several machine translation tasks, it requires a large number of parameters and training iterations to converge. We propose Weighted Transformer, a Transformer with modified attention layers, that not only outperforms the baseline network in BLEU score but also converges 15-40% faster. Specifically, we replace the multi-head attention by multiple self-attention branches that the model learns to combine during the training process. Our model improves the state-of-the-art performance by 0.5 BLEU points on the WMT 2014 English-to-German translation task and\n[{'summary_text': 'state-of-the-art results on neural machine translation often use attentional sequence-to-sequence models with some form of convolution or recursion . instead, it uses only self-attention and feed-forward layers .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  13%|█▎        | 261/1992 [16:44<1:51:09,  3.85s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep neural networks (DNNs) usually contain millions, maybe billions, of parameters/weights, making both storage and computation very expensive. This has motivated a large body of work to reduce the complexity of the neural network by using sparsity-inducing regularizers.   Another well-known approach for controlling the complexity of DNNs is parameter sharing/tying, where certain sets of weights are forced to share a common value. Some forms of weight sharing are hard-wired to express certain in- variances, with a notable example being the shift-invariance of convolutional layers. However, there may be other groups of weights that may be tied together during the learning process, thus further re- ducing the complexity of the network. In this paper, we adopt a recently proposed sparsity-inducing regularizer, named GrOWL (group ordered weighted l1), which encourages sparsity and, simulta- neously, learns which groups of parameters should share\n[{'summary_text': 'this has motivated a large body of work to reduce the complexity of the neural network by using sparsity-inducing regularizers . some forms of weight sharing are hard-wired to express certain in- variances, with a notable example being the shift-invariance of convolutional layers . however, there may be other groups of weights that may be tied together during the learning process, thus further re-ducing the complex network .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  13%|█▎        | 262/1992 [16:48<1:55:30,  4.01s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose a novel score-based approach to learning a directed acyclic graph (DAG) from observational data. We adapt a recently proposed continuous constrained optimization formulation to allow for nonlinear relationships between variables using neural networks. This extension allows to model complex interactions while being more global in its search compared to other greedy approaches. In addition to comparing our method to existing continuous optimization methods, we provide missing empirical comparisons to nonlinear greedy search methods. On both synthetic and real-world data sets, this new method outperforms current continuous methods on most tasks while being competitive with existing greedy search methods on important metrics for causal inference.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we adapt a recently proposed continuous constrained optimization formulation to allow for nonlinear relationships between variables using neural networks . this extension allows to model complex interactions while being more global in its search compared to other greedy approaches . in addition to comparing our method to existing continuous optimization methods, we provide missing empirical comparisons .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  13%|█▎        | 263/1992 [16:52<1:51:57,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Transfer learning for feature extraction can be used to exploit deep representations in contexts where there is very few training data, where there are limited computational resources, or when tuning the hyper-parameters needed for training is not an option. While previous contributions to feature extraction propose embeddings based on a single layer of the network, in this paper we propose a full-network embedding which successfully integrates convolutional and fully connected features, coming from all layers of a deep convolutional neural network. To do so, the embedding normalizes features in the context of the problem, and discretizes their values to reduce noise and regularize the embedding space. Significantly, this also reduces the computational cost of processing the resultant representations. The proposed method is shown to outperform single layer embeddings on several image classification tasks, while also being more robust to the choice of the pre-trained model used for obtaining the initial features. The performance gap in classification accuracy between thoroughly tuned solutions and the\n[{'summary_text': 'transfer learning for feature extraction can be used to exploit deep representations in contexts where there is very few training data, where limited computational resources, or when tuning the hyper-parameters needed for training is not an option . previous contributions to feature extraction propose embeddings based on a single layer of the network . in this paper we propose a full-network embeddedding which successfully integrates convolutional and fully connected features .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  13%|█▎        | 264/1992 [16:57<1:57:35,  4.08s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Neural Style Transfer has become a popular technique for\n generating images of distinct artistic styles using convolutional neural networks. This\n recent success in image style transfer has raised the question of\n whether similar methods can be leveraged to alter the “style” of musical\n audio. In this work, we attempt long time-scale high-quality audio transfer\n and texture synthesis in the time-domain that captures harmonic,\n rhythmic, and timbral elements related to musical style, using examples that\n may have different lengths and musical keys. We demonstrate the ability\n to use randomly initialized convolutional neural networks to transfer\n these aspects of musical style from one piece onto another using 3\n different representations of audio: the log-magnitude of the Short Time\n Fourier Transform (STFT), the Mel spectrogram, and the Constant-Q Transform\n spectrogram. We propose using these representations as a way of\n generating and modifying perceptually significant characteristics of\n musical audio\n[{'summary_text': 'this recent success in image style transfer has raised the question of whether similar methods can be leveraged to alter the “style” of musical audio . in this work, we attempt long time-scale high-quality audio transfer and texture synthesis in the time-domain that captures harmonic, rhythmic, and timbral elements related to musical style .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  13%|█▎        | 265/1992 [17:01<1:57:35,  4.09s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"A well-trained model should classify objects with unanimous score for every category. This requires the high-level semantic features should be alike among samples, despite a wide span in resolution, texture, deformation, etc. Previous works focus on re-designing the loss function or proposing new regularization constraints on the loss. In this paper, we address this problem via a new perspective. For each category, it is assumed that there are two sets in the feature space: one with more reliable information and the other with less reliable source. We argue that the reliable set could guide the feature learning of the less reliable set during training - in spirit of student mimicking teacher’s behavior and thus pushing towards a more compact class centroid in the high-dimensional space. Such a scheme also benefits the reliable set since samples become more closer within the same category - implying that it is easilier for the classifier to identify. We refer to this mutual learning process as feature intertwiner and embed the\n[{'summary_text': 'a well-trained model should classify objects with unanimous score for every category . this requires the high-level semantic features should be alike among samples . previous works focus on re-designing the loss function or proposing new regularization constraints on the loss .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  13%|█▎        | 266/1992 [17:05<1:56:38,  4.05s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recent improvements to Generative Adversarial Networks (GANs) have made it possible to generate realistic images in high resolution based on natural language descriptions such as image captions. Furthermore, conditional GANs allow us to control the image generation process through labels or even natural language descriptions. However, fine-grained control of the image layout, i.e. where in the image specific objects should be located, is still difficult to achieve. This is especially true for images that should contain multiple distinct objects at different spatial locations. We introduce a new approach which allows us to control the location of arbitrarily many objects within an image by adding an object pathway to both the generator and the discriminator. Our approach does not need a detailed semantic layout but only bounding boxes and the respective labels of the desired objects are needed. The object pathway focuses solely on the individual objects and is iteratively applied at the locations specified by the bounding boxes. The global pathway focuses on the image background and the general image\n[{'summary_text': 'conditional GANs allow us to control the image generation process through labels or even natural language descriptions . but fine-grained control of the image layout is still difficult to achieve . this is especially true for images that should contain multiple distinct objects at different spatial locations .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  13%|█▎        | 267/1992 [17:08<1:54:23,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Due to the success of deep learning to solving a variety of challenging machine learning tasks, there is a rising interest in understanding loss functions for training neural networks from a theoretical aspect. Particularly, the properties of critical points and the landscape around them are of importance to determine the convergence performance of optimization algorithms. In this paper, we provide a necessary and sufficient characterization of the analytical forms for the critical points (as well as global minimizers) of the square loss functions for linear neural networks. We show that the analytical forms of the critical points characterize the values of the corresponding loss functions as well as the necessary and sufficient conditions to achieve global minimum. Furthermore, we exploit the analytical forms of the critical points to characterize the landscape properties for the loss functions of linear neural networks and shallow ReLU networks. One particular conclusion is that: While the loss function of linear networks has no spurious local minimum, the loss function of one-hidden-layer nonlinear networks with ReLU activation function does have local minimum that is not\n[{'summary_text': 'the properties of critical points and the landscape around them are of importance to determine the convergence performance of optimization algorithms . in this paper, we provide a necessary and sufficient characterization of the analytical forms for the critical points (as well as global minimizers) of the square loss functions for linear neural networks .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  13%|█▎        | 268/1992 [17:12<1:53:30,  3.95s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The integration of a  Knowledge Base (KB) into a neural dialogue agent is one of the key challenges in Conversational AI. Memory networks has proven to be effective to encode KB information into an external memory to thus generate more fluent and informed responses. Unfortunately, such memory becomes full of latent representations during training, so the most common strategy is to overwrite old memory entries randomly. \n\n In this paper, we question this approach and provide experimental evidence showing that conventional memory networks generate many redundant latent vectors resulting in overfitting and the need for larger memories. We introduce memory dropout as an automatic technique that encourages diversity in the latent space by 1) Aging redundant memories to increase their probability of being overwritten during training 2) Sampling new memories that summarize the knowledge acquired by redundant memories. This technique allows us to incorporate  Knowledge Bases to achieve state-of-the-art dialogue generation in the Stanford Multi-Turn Dialogue dataset. Considering the same architecture, its use provides an improvement of +2\n[{'summary_text': 'the integration of a Knowledge Base (KB) into a neural dialogue agent is one of the key challenges in Conversational AI . memory networks have proven to be effective to encode KB information into an external memory to thus generate more fluent and informed responses . such memory becomes full of latent representations during training, so the most common strategy is to overwrite old memory entries randomly .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  14%|█▎        | 269/1992 [17:16<1:54:57,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose a context-adaptive entropy model for use in end-to-end optimized image compression. Our model exploits two types of contexts, bit-consuming contexts and bit-free contexts, distinguished based upon whether additional bit\n allocation is required. Based on these contexts, we allow the model to more accurately estimate the distribution of each latent representation with a more generalized form of the approximation models, which accordingly leads to an\n enhanced compression performance. Based on the experimental results, the proposed method outperforms the traditional image codecs, such as BPG and JPEG2000, as well as other previous artificial-neural-network (ANN) based approaches, in terms of the peak signal-to-noise ratio (PSNR) and multi-scale structural similarity (MS-SSIM) index. The test code is publicly available at https://github.com/JooyoungLeeETRI/CA_Entropy_Model.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'model exploits two types of contexts, bit-consuming contexts and bit-free contexts . model allows model to more accurately estimate distribution of each latent representation . proposed method outperforms traditional image codecs, such as BPG and JPEG2000 .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  14%|█▎        | 270/1992 [17:20<1:51:24,  3.88s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Gradient clipping is a widely-used technique in the training of deep networks, and is generally motivated from an optimisation lens: informally, it controls the dynamics of iterates, thus enhancing the rate of convergence to a local minimum. This intuition has been made precise in a line of recent works, which show that suitable clipping  can yield significantly faster convergence than vanilla gradient descent. In this paper, we propose a new lens for studying gradient clipping, namely, robustness: informally, one expects clipping to provide robustness to noise, since one does not overly trust any single sample. Surprisingly, we prove that  for the common problem of label noise in classification, standard gradient clipping does not in general provide robustness. On the other hand, we show that  a simple variant of gradient clipping is provably robust, and corresponds to suitably modifying the underlying loss function. This yields a simple, noise-robust alternative to the standard cross-entropy loss which performs well empirically\n[{'summary_text': 'informally, it controls the dynamics of iterates, thus enhancing the rate of convergence to a local minimum . this intuition has been made precise in a line of recent works, which show that suitable clipping can yield significantly faster convergence than vanilla gradient descent . a simple variant of gradient clipping is provably robust and corresponds to suitably modifying the underlying loss function .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  14%|█▎        | 271/1992 [17:24<1:53:50,  3.97s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We leverage recent insights from second-order optimisation for neural networks to construct a Kronecker factored Laplace approximation to the posterior over the weights of a trained network. Our approximation requires no modification of the training procedure, enabling practitioners to estimate the uncertainty of their models currently used in production without having to retrain them. We extensively compare our method to using Dropout and a diagonal Laplace approximation for estimating the uncertainty of a network. We demonstrate that our Kronecker factored method leads to better uncertainty estimates on out-of-distribution data and is more robust to simple adversarial attacks. Our approach only requires calculating two square curvature factor matrices for each layer. Their size is equal to the respective square of the input and output size of the layer, making the method efficient both computationally and in terms of memory usage. We illustrate its scalability by applying it to a state-of-the-art convolutional network architecture.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'our approximation requires no modification of the training procedure . our approach only requires calculating two square curvature factor matrices for each layer . their size is equal to the respective square of the input and output size of the layer, making the method efficient both computationally and in terms of memory usage .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  14%|█▎        | 272/1992 [17:28<1:52:23,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"With the ever increasing demand and the resultant reduced quality of services, the focus has shifted towards easing network congestion to enable more efficient flow in systems like traffic, supply chains and electrical grids. A step in this direction is to re-imagine the traditional heuristics based training of systems as this approach is incapable of modelling the involved dynamics. While one can apply Multi-Agent Reinforcement Learning (MARL) to such problems by considering each vertex in the network as an agent, most MARL-based models assume the agents to be independent. In many real-world tasks, agents need to behave as a group, rather than as a collection of individuals. In this paper, we propose a framework that induces cooperation and coordination amongst agents, connected via an underlying network, using emergent communication in a MARL-based setup. We formulate the problem in a general network setting and demonstrate the utility of communication in networks with the help of a case study on traffic systems. Furthermore, we study the emer\n[{'summary_text': 'a step in this direction is to re-imagine the traditional heuristics based training of systems as this approach is incapable of modelling the involved dynamics . in many real-world tasks, agents need to behave as a group, rather than a collection of individuals .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  14%|█▎        | 273/1992 [17:32<1:52:55,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Transformers have achieved state-of-the-art results on a variety of natural language processing tasks. \n Despite good performance, Transformers are still weak in long sentence modeling where the global attention map is too dispersed to capture valuable information.\n In such case, the local/token features that are also significant to sequence modeling are omitted to some extent.\n To address this problem, we propose a Multi-scale attention model (MUSE) by concatenating attention networks with convolutional networks and position-wise feed-forward networks to explicitly capture local and token features. Considering the parameter size and computation efficiency, we re-use the feed-forward layer in the original Transformer and adopt a lightweight dynamic convolution as implementation. \n Experimental results show that the proposed model achieves substantial performance improvements over Transformer, especially on long sentences, and pushes the state-of-the-art from 35.6 to 36.2 on IWSLT 2014  German to English translation task\n[{'summary_text': 'to address this problem, we propose a Multi-scale attention model (MUSE) by concatenating attention networks with convolutional networks and position-wise feed-forward networks to explicitly capture local and token features . the proposed model achieves substantial performance improvements over Transformer, especially on long sentences, and pushes the state-of-the-art from 35.6 to 36.2 on IWSLT 2014 German to English translation task .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  14%|█▍        | 274/1992 [17:36<1:55:54,  4.05s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep CNNs have achieved state-of-the-art performance for numerous machine learning and computer vision tasks in recent years, but as they have become increasingly deep, the number of parameters they use has also increased, making them hard to deploy in memory-constrained environments and difficult to interpret. Machine learning theory implies that such networks are highly over-parameterised and that it should be possible to reduce their size without sacrificing accuracy, and indeed many recent studies have begun to highlight specific redundancies that can be exploited to achieve this. In this paper, we take a further step in this direction by proposing a filter-sharing approach to compressing deep CNNs that reduces their memory footprint by repeatedly applying a single convolutional mapping of learned filters to simulate a CNN pipeline. We show, via experiments on CIFAR-10, CIFAR-100, Tiny ImageNet, and ImageNet that this allows us to reduce the parameter counts of networks based on common designs such as VGG\n[{'summary_text': 'machine learning theory implies that such networks are highly over-parameterised and that it should be possible to reduce their size without sacrificing accuracy . in this paper, we propose a filter-sharing approach to compressing deep CNNs that reduces their memory footprint by repeatedly applying a single convolutional mapping of learned filters to simulate a CNN pipeline .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  14%|█▍        | 275/1992 [17:40<1:55:49,  4.05s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Mixed-precision arithmetic combining both single- and half-precision operands in the same operation have been successfully applied to train deep neural networks. Despite the advantages of mixed-precision arithmetic in terms of reducing the need for key resources like memory bandwidth or register file size, it has a limited capacity for diminishing computing costs and requires 32 bits to represent its output operands. This paper proposes two approaches to replace mixed-precision for half-precision arithmetic during a large portion of the training. The first approach achieves accuracy ratios slightly slower than the state-of-the-art by using half-precision arithmetic during more than 99% of training. The second approach reaches the same accuracy as the state-of-the-art by dynamically switching between half- and mixed-precision arithmetic during training. It uses half-precision during more than 94% of the training process. This paper is the first in demonstrating that half-precision can be used for a very large\n[{'summary_text': 'mixed-precision arithmetic has been successfully applied to train deep neural networks . it has a limited capacity for diminishing computing costs and requires 32 bits to represent its output operands . the first approach achieves accuracy ratios slightly slower than the state-of-the-art .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  14%|█▍        | 276/1992 [17:45<1:58:06,  4.13s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose NovoGrad, an adaptive stochastic gradient descent method with layer-wise gradient normalization and decoupled weight decay. In our experiments on neural networks for image classification, speech recognition, machine translation, and language modeling, it performs on par or better than well tuned SGD with momentum and Adam/AdamW. \n Additionally, NovoGrad (1) is robust to the choice of learning rate and weight initialization, (2) works well in a large batch setting, and (3) has two times smaller memory footprint than Adam.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'NovoGrad is robust to the choice of learning rate and weight initialization . it performs on par or better than well tuned SGD with momentum and Adam .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  14%|█▍        | 277/1992 [17:48<1:49:32,  3.83s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model. In our model, the gradient can be directly back-propagated from the language model loss into the neural parsing network. Experiments show that the proposed model can discover the underlying syntactic structure and achieve state-of-the-art performance on word/character-level language model tasks.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'model leverages structure information to form better semantic representations and better language modeling . standard recurrent neural networks are limited by their structure and fail to efficiently use syntactic information . on the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  14%|█▍        | 278/1992 [17:51<1:48:34,  3.80s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'models equipped with adaptive embeddings are twice as fast to train than popular character input CNN while having a lower number of parameters . we perform a systematic comparison of popular choices for a self-attentional architecture .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  14%|█▍        | 279/1992 [17:55<1:45:23,  3.69s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Model distillation aims to distill the knowledge of a complex model into a simpler one. In this paper, we consider an alternative formulation called dataset distillation: we keep the model fixed and instead attempt to distill the knowledge from a large training dataset into a small one. The idea is to synthesize a small number of data points that do not need to come from the correct data distribution, but will, when given to the learning algorithm as training data, approximate the model trained on the original data. For example, we show that it is possible to compress 60,000 MNIST training images into just 10 synthetic distilled images (one per class) and achieve close to the original performance, given a fixed network initialization. We evaluate our method in various initialization settings.   Experiments on multiple datasets, MNIST, CIFAR10, PASCAL-VOC, and CUB-200, demonstrate the ad-vantage of our approach compared to alternative methods.   Finally, we\n[{'summary_text': 'model distillation aims to distill knowledge of a complex model into a simpler one . the idea is to synthesize a small number of data points that do not need to come from the correct data distribution, but will, when given to the learning algorithm as training data, approximate the model trained on the original data .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  14%|█▍        | 280/1992 [17:59<1:47:37,  3.77s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"GANs provide a framework for training generative models which mimic a data distribution. However, in many cases we wish to train a generative model to optimize some auxiliary objective function within the data it generates, such as making more aesthetically pleasing images. In some cases, these objective functions are difficult to evaluate, e.g. they may require human interaction. Here, we develop a system for efficiently training a GAN to increase a generic rate of positive user interactions, for example aesthetic ratings. To do this, we build a model of human behavior in the targeted domain from a relatively small set of interactions, and then use this behavioral model as an auxiliary loss function to improve the generative model. As a proof of concept, we demonstrate that this system is successful at improving positive interaction rates simulated from a variety of objectives, and characterize s<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in many cases we wish to train a generative model to optimize some auxiliary objective function within the data it generates, such as making more aesthetically pleasing images . in some cases, these objective functions are difficult to evaluate, e.g. they may require human interaction . here, we develop a system for efficiently training a GAN to increase a generic rate of positive user interactions .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  14%|█▍        | 281/1992 [18:03<1:48:34,  3.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The vertebrate visual system is hierarchically organized to process visual information in successive stages. Neural representations vary drastically across the first stages of visual processing: at the output of the retina, ganglion cell receptive fields (RFs) exhibit a clear antagonistic center-surround structure, whereas in the primary visual cortex (V1), typical RFs are sharply tuned to a precise orientation. There is currently no unified theory explaining these differences in representations across layers. Here, using a deep convolutional neural network trained on image recognition as a model of the visual system, we show that such differences in representation can emerge as a direct consequence of different neural resource constraints on the retinal and cortical networks, and for the first time we find a single model from which both geometries spontaneously emerge at the appropriate stages of visual processing. The key constraint is a reduced number of neurons at the retinal output, consistent with the anatomy of the optic nerve as a stringent bottleneck. Second, we find that,\n[{'summary_text': 'the vertebrate visual system is hierarchically organized to process visual information in successive stages . ganglion cell receptive fields (RFs) exhibit a clear antagonistic center-surround structure . in the primary visual cortex (V1), typical RFs are sharply tuned to a precise orientation . there is currently no unified theory explaining these differences in representations across layers .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  14%|█▍        | 282/1992 [18:07<1:51:57,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Unsupervised embedding learning aims to extract good representations from data without the use of human-annotated labels. Such techniques are apparently in the limelight because of the challenges in collecting massive-scale labels required for supervised learning. This paper proposes a comprehensive approach, called Super-AND, which is based on the Anchor Neighbourhood Discovery model. Multiple losses defined in Super-AND make similar samples gather even within a low-density space and keep features invariant against augmentation. As a result, our model outperforms existing approaches in various benchmark datasets and achieves an accuracy of 89.2% in CIFAR-10 with the Resnet18 backbone network, a 2.9% gain over the state-of-the-art.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'a comprehensive approach, called super-AND, is based on the Anchor Neighbourhood Discovery model . multiple losses defined in Super-AND make similar samples gather even within a low-density space and keep features invariant against augmentation . as a result, our model outperforms existing approaches in various benchmark datasets .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  14%|█▍        | 283/1992 [18:11<1:50:20,  3.87s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep learning models are known to be vulnerable to adversarial examples. A practical adversarial attack should require as little as possible knowledge of attacked models T. Current substitute attacks need pre-trained models to generate adversarial examples and their attack success rates heavily rely on the transferability of adversarial examples. Current score-based and decision-based attacks require lots of queries for the T. In this study, we propose a novel adversarial imitation attack. First, it produces a replica of the T by a two-player game like the generative adversarial networks (GANs). The objective of the generative model G is to generate examples which lead D returning different outputs with T. The objective of the discriminative model D is to output the same labels with T under the same inputs. Then, the adversarial examples generated by D are utilized to fool the T. Compared with the current substitute attacks, imitation attack can use less training data to produce a replica of T and improve the transferability of\n[{'summary_text': 'current substitute attacks need pre-trained models to generate adversarial examples . current score-based and decision-based attacks require lots of queries for the T .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  14%|█▍        | 284/1992 [18:14<1:47:40,  3.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Adaptive optimization methods such as AdaGrad, RMSprop and Adam have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates. Though prevailing, they are observed to generalize poorly compared with SGD or even fail to converge due to unstable and extreme learning rates. Recent work has put forward some algorithms such as AMSGrad to tackle this issue but they failed to achieve considerable improvement over existing methods. In our paper, we demonstrate that extreme learning rates can lead to poor performance. We provide new variants of Adam and AMSGrad, called AdaBound and AMSBound respectively, which employ dynamic bounds on learning rates to achieve a gradual and smooth transition from adaptive methods to SGD and give a theoretical proof of convergence. We further conduct experiments on various popular tasks and models, which is often insufficient in previous work. Experimental results show that new variants can eliminate the generalization gap between adaptive methods and SGD and maintain higher learning speed early in training at the same\n[{'summary_text': 'Adaptive optimization methods such as AdaGrad, RMSprop and Adam have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates . although prevailing, they generalize poorly compared with SGD or even fail to converge due to unstable and extreme learning rates. recent work has put forward some algorithms such as AMSGrad to tackle this issue but they failed to achieve considerable improvement over existing methods .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  14%|█▍        | 285/1992 [18:19<1:52:27,  3.95s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"An unintended consequence of feature sharing is the model fitting to correlated tasks within the dataset, termed negative transfer.   In this paper, we revisit the problem of negative transfer in multitask setting and find that its corrosive effects are applicable to a wide range of linear and non-linear models, including neural networks. We first study the effects of negative transfer in a principled way and show that previously proposed counter-measures are insufficient, particularly for trainable features. We propose an adversarial training approach to mitigate the effects of negative transfer by viewing the problem in a domain adaptation setting. Finally, empirical results on attribute prediction multi-task on AWA and CUB datasets further validate the need for correcting negative sharing in an end-to-end manner.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this paper, we revisit the problem of negative transfer in multitask setting . we find that its corrosive effects are applicable to a wide range of linear and non-linear models, including neural networks .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  14%|█▍        | 286/1992 [18:22<1:47:50,  3.79s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep learning, a rebranding of deep neural network research works, has achieved a remarkable success in recent years. With multiple hidden layers, deep learning models aim at computing the hierarchical feature representations of the observational data. Meanwhile, due to its severe disadvantages in data consumption, computational resources, parameter tuning costs and the lack of result explainability, deep learning has also suffered from lots of criticism. In this paper, we will introduce a new representation learning model, namely “Sample-Ensemble Genetic Evolutionary Network” (SEGEN), which can serve as an alternative approach to deep learning models. Instead of building one single deep model, based on a set of sampled sub-instances, SEGEN adopts a genetic-evolutionary learning strategy to build a group of unit models generations by generations. The unit models incorporated in SEGEN can be either traditional machine learning models or the recent deep learning models with a much “narrower” and “shallower�\n[{'summary_text': 'deep learning, a rebranding of deep neural network research works, has achieved a remarkable success in recent years . with multiple hidden layers, deep learning models aim at computing the hierarchical feature representations of the observational data . however, due to its severe disadvantages in data consumption, computational resources, parameter tuning costs and the lack of result explainability deep learning has also suffered .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  14%|█▍        | 287/1992 [18:26<1:51:05,  3.91s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"This paper is focused on investigating and demystifying an intriguing robustness phenomena in over-parameterized neural network training. In particular we provide empirical and theoretical evidence that first order methods such as gradient descent are provably robust to noise/corruption on a constant fraction of the labels despite over-parameterization under a rich dataset model. In particular: i) First, we show that in the first few iterations where the updates are still in the vicinity of the initialization these algorithms only fit to the correct labels essentially ignoring the noisy labels. ii) Secondly, we prove that to start to overfit to the noisy labels these algorithms must stray rather far from from the initial model which can only occur after many more iterations. Together, these show that gradient descent with early stopping is provably robust to label noise and shed light on empirical robustness of deep networks as well as commonly adopted early-stopping heuristics.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'first order methods such as gradient descent are provably robust to noise/corruption on a constant fraction of the labels despite over-parameterization under a rich dataset model . ii) Secondly, we prove that to start to overfit to the noisy labels these algorithms must stray rather far from from the initial model which can only occur after many more iterations .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  14%|█▍        | 288/1992 [18:30<1:53:19,  3.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Anatomical studies demonstrate that brain reformats input information to generate reliable responses for performing computations. However, it remains unclear how neural circuits encode complex spatio-temporal patterns. We show that neural dynamics are strongly influenced by the phase alignment between the input and the spontaneous chaotic activity. Input alignment along the dominant chaotic projections causes the chaotic trajectories to become stable channels (or attractors), hence, improving the computational capability of a recurrent network. Using mean field analysis, we derive the impact of input alignment on the overall stability of attractors formed. Our results indicate that input alignment determines the extent of intrinsic noise suppression and hence, alters the attractor state stability, thereby controlling the network's inference ability.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'neural dynamics are strongly influenced by the phase alignment between the input and the spontaneous chaotic activity . input alignment along the dominant chaotic projections causes the chaotic trajectories to become stable channels (or attractors)'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  15%|█▍        | 289/1992 [18:34<1:48:44,  3.83s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Modern neural network architectures take advantage of increasingly deeper layers, and various advances in their structure to achieve better performance. While traditional explicit regularization techniques like dropout, weight decay, and data augmentation are still being used in these new models, little about the regularization and generalization effects of these new structures have been studied. \n Besides being deeper than their predecessors, could newer architectures like ResNet and DenseNet also benefit from their structures' implicit regularization properties? \n In this work, we investigate the skip connection's effect on network's generalization features. Through experiments, we show that certain neural network architectures contribute to their generalization abilities. Specifically, we study the effect that low-level features have on generalization performance when they are introduced to deeper layers in DenseNet, ResNet as well as networks with 'skip connections'. We show that these low-level representations do help with generalization in multiple settings when both the quality and quantity of training data is decreased.<|endoftext|>\n[{'summary_text': 'traditional explicit regularization techniques like dropout, weight decay, and data augmentation are still being used in these new models . besides being deeper than their predecessors, could newer architectures like ResNet and DenseNet also benefit from their implicit regularization properties?'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  15%|█▍        | 290/1992 [18:37<1:46:10,  3.74s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Emotion is playing a great role in our daily lives. The necessity and importance of an automatic Emotion recognition system is getting increased. Traditional approaches of emotion recognition are based on facial images, measurements of heart rates, blood pressure, temperatures, tones of voice/speech, etc. However, these features can potentially be changed to fake features. So to detect hidden and real features that is not controlled by the person are data measured from brain signals. There are various ways of measuring brain waves: EEG, MEG, FMRI, etc. On the bases of cost effectiveness and performance trade-offs, EEG is chosen for emotion recognition in this work. The main aim of this study is to detect emotion based on EEG signal analysis recorded from brain in response to visual stimuli. The approaches used were the selected visual stimuli were presented to 11 healthy target subjects and EEG signal were recorded in controlled situation to minimize artefacts (muscle or/and eye movements).   The signals were filtered and type of frequency\n[{'summary_text': 'traditional approaches of emotion recognition are based on facial images, measurements of heart rates, blood pressure, temperatures, tones of voice/speech, etc. however, these features can potentially be changed to fake features . to detect hidden and real features that is not controlled by the person are data measured from brain signals .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  15%|█▍        | 291/1992 [18:41<1:48:09,  3.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose a novel framework to generate clean video frames from a single motion-blurred image.\n While a broad range of literature focuses on recovering a single image from a blurred image, in this work, we tackle a more challenging task i.e.  video restoration from a blurred image. We formulate video restoration from a single blurred image as an inverse problem by setting clean image sequence and their respective motion as latent factors, and the blurred image as an observation. Our framework is based on an encoder-decoder structure with spatial transformer network modules to restore a video sequence and its underlying motion in an end-to-end manner. We design a loss function and regularizers with complementary properties to stabilize the training and analyze variant models of the proposed network. The effectiveness and transferability of our network are highlighted through a large set of experiments on two different types of datasets: camera rotation blurs generated from panorama scenes and dynamic motion blurs in high speed videos. Our code and models will be\n[{'summary_text': 'our framework is based on an encoder-decoder structure with spatial transformer network modules to restore a video sequence and its underlying motion in an end-to-end manner . we design a loss function and regularizers with complementary properties to stabilize the training and analyze variant models of the proposed network .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  15%|█▍        | 292/1992 [18:45<1:50:14,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"An important type of question that arises in Explainable Planning is a contrastive question, of the form \"Why action A instead of action B?\". These kinds of questions can be answered with a contrastive explanation that compares properties of the original plan containing A against the contrastive plan containing B. An effective explanation of this type serves to highlight the differences between the decisions that have been made by the planner and what the user would expect, as well as to provide further insight into the model and the planning process. Producing this kind of explanation requires the generation of the contrastive plan. This paper introduces domain-independent compilations of user questions into constraints. These constraints are added to the planning model, so that a solution to the new model represents the contrastive plan. We introduce a formal description of the compilation from user question to constraints in a temporal and numeric PDDL2.1 planning setting.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'an important type of question arises in Explainable Planning is a contrastive question, of the form \"Why action A instead of action B?\" an effective explanation of this type serves to highlight the differences between the decisions that have been made by the planner and what the user would expect . this paper introduces domain-independent compilations of user questions into constraints .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  15%|█▍        | 293/1992 [18:49<1:48:35,  3.83s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"A restricted Boltzmann machine (RBM) learns a probabilistic distribution over its input samples and has numerous uses like dimensionality reduction, classification and generative modeling. Conventional RBMs accept vectorized data that dismisses potentially important structural information in the original tensor (multi-way) input. Matrix-variate and tensor-variate RBMs, named MvRBM and TvRBM, have been proposed but are all restrictive by construction. This work presents the matrix product operator RBM (MPORBM) that utilizes a tensor network generalization of Mv/TvRBM, preserves input formats in both the visible and hidden layers, and results in higher expressive power. A novel training algorithm integrating contrastive divergence and an alternating optimization procedure is also developed.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'a restricted Boltzmann machine (RBM) learns a probabilistic distribution over its input samples and has numerous uses like dimensionality reduction, classification and generative modeling . conventional RBMs accept vectorized data that dismisses potentially important structural information in the original tensor (multi-way) input .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  15%|█▍        | 294/1992 [18:53<1:51:37,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"This paper introduces CloudLSTM, a new branch of recurrent neural models tailored to forecasting over data streams generated by geospatial point-cloud sources. We design a Dynamic Point-cloud Convolution (D-Conv) operator as the core component of CloudLSTMs, which performs convolution directly over point-clouds and extracts local spatial features from sets of neighboring points that surround different elements of the input. This operator maintains the permutation invariance of sequence-to-sequence learning frameworks, while representing neighboring correlations at each time step -- an important aspect in spatiotemporal predictive learning. The D-Conv operator resolves the grid-structural data requirements of existing spatiotemporal forecasting models and can be easily plugged into traditional LSTM architectures with sequence-to-sequence learning and attention mechanisms.\n     We apply our proposed architecture to two representative, practical use cases that involve point-cloud streams, i.e. mobile service traffic forecasting and air\n[{'summary_text': 'new branch of recurrent neural models tailored to forecasting over data streams generated by geospatial point-cloud sources . we design a Dynamic Point-cloud Convolution (D-Conv) operator as the core component of CloudLSTMs . this operator maintains the permutation invariance of sequence-to-sequence learning frameworks, while representing neighboring correlations at each time step .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  15%|█▍        | 295/1992 [18:58<1:54:16,  4.04s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Direct policy gradient methods for reinforcement learning and continuous control problems are a popular\napproach for a variety of reasons: \n1) they are easy to implement without explicit knowledge of the underlying model;\n2) they are an \"end-to-end\" approach, directly optimizing the performance metric of interest;\n3) they inherently allow for richly parameterized policies.\n A notable drawback is that even in the most basic continuous control problem (that of linear quadratic regulators), these methods must solve a non-convex optimization problem, where little is understood about their efficiency from both computational and statistical perspectives. In contrast, system identification and model based planning in optimal control theory have a much more solid theoretical footing, where much is known with regards to their computational and statistical properties.   This work bridges this gap showing that (model free) policy gradient methods globally converge to the optimal solution and are efficient (polynomially so in relevant problem dependent quantities) with regards to their\n[{'summary_text': 'direct policy gradient methods for reinforcement learning and continuous control problems are a popular approach for a variety of reasons . they are easy to implement without explicit knowledge of the underlying model . a notable drawback is that even in the most basic continuous control problem, these methods must solve a non-convex optimization problem .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  15%|█▍        | 296/1992 [19:02<1:54:13,  4.04s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Many notions of fairness may be expressed as linear constraints, and the resulting constrained objective is often optimized by transforming the problem into its Lagrangian dual with additive linear penalties. In non-convex settings, the resulting problem may be difficult to solve as the Lagrangian is not guaranteed to have a deterministic saddle-point equilibrium.   In this paper, we propose to modify the linear penalties to second-order ones, and we argue that this results in a more practical training procedure in non-convex, large-data settings. For one, the use of second-order penalties allows training the penalized objective with a fixed value of the penalty coefficient, thus avoiding the instability and potential lack of convergence associated with two-player min-max games. Secondly, we derive a method for efficiently computing the gradients associated with the second-order penalties in stochastic mini-batch settings. Our resulting algorithm performs well empirically, learning an appropriately fair classifier on a number\n[{'summary_text': 'in non-convex settings, the resulting constrained objective may be difficult to solve as the Lagrangian is not guaranteed to have a deterministic saddle-point equilibrium . the use of second-order penalties allows training the penalized objective with a fixed value of the penalty coefficient, thus avoiding the instability and potential lack of convergence associated with two-player min-max games.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  15%|█▍        | 297/1992 [19:06<1:56:15,  4.12s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We show that generating English Wikipedia articles can be approached as a multi-\n document summarization of source documents. We use extractive summarization\n to coarsely identify salient information and a neural abstractive model to generate\n the article. For the abstractive model, we introduce a decoder-only architecture\n that can scalably attend to very long sequences, much longer than typical encoder-\n decoder architectures used in sequence transduction. We show that this model can\n generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia\n articles. When given reference documents, we show it can extract relevant factual\n information as reflected in perplexity, ROUGE scores and human evaluations.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'generating English Wikipedia articles can be approached as a multi- document summarization of source documents . we introduce a decoder-only architecture that can scalably attend to very long sequences . when given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  15%|█▍        | 298/1992 [19:10<1:52:47,  3.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Multilingual Neural Machine Translation (NMT) systems are capable of translating between multiple source and target languages within a single system. An important indicator of generalization within these systems is the quality of zero-shot translation - translating between language pairs that the system has never seen during training. However, until now, the zero-shot performance of multilingual models has lagged far behind the quality that can be achieved by using a two step translation process that pivots through an intermediate language (usually English). In this work, we diagnose why multilingual models under-perform in zero shot settings. We propose explicit language invariance losses that guide an NMT encoder towards learning language agnostic representations. Our proposed strategies significantly improve zero-shot translation performance on WMT English-French-German and on the IWSLT 2017 shared task, and for the first time, match the performance of pivoting approaches while maintaining performance on supervised directions.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'multilingual models are capable of translating between multiple source and target languages within a single system . an important indicator of generalization within these systems is the quality of zero-shot translation - translating between language pairs that the system has never seen during training . however, until now, the null-shot performance has lagged far behind the quality that can be achieved by using a two step translation process .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  15%|█▌        | 299/1992 [19:14<1:52:16,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Autoregressive recurrent neural decoders that generate sequences of tokens one-by-one and left-to-right are the workhorse of modern machine translation. In this work, we propose a new decoder architecture that can generate natural language sequences in an arbitrary order. Along with generating tokens from a given vocabulary, our model additionally learns to select the optimal position for each produced token. The proposed decoder architecture is fully compatible with the seq2seq framework and can be used as a drop-in replacement of any classical decoder. We demonstrate the performance of our new decoder on the IWSLT machine translation task as well as inspect and interpret the learned decoding patterns by analyzing how the model selects new positions for each subsequent token.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'recurrent neural decoders generate sequences of tokens one-by-one and left-to-right are the workhorse of modern machine translation . generating tokens from a given vocabulary, our model additionally learns to select the optimal position for each produced token .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  15%|█▌        | 300/1992 [19:17<1:50:26,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Implicit probabilistic models are models defined naturally in terms of a sampling procedure and often induces a likelihood function that cannot be expressed explicitly. We develop a simple method for estimating parameters in implicit models that does not require knowledge of the form of the likelihood function or any derived quantities, but can be shown to be equivalent to maximizing likelihood under some conditions. Our result holds in the non-asymptotic parametric setting, where both the capacity of the model and the number of data examples are finite. We also demonstrate encouraging experimental results.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we develop a simple method for estimating parameters in implicit models . our result holds in the non-asymptotic parametric setting . both the capacity of the model and the number of data examples are finite .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  15%|█▌        | 301/1992 [19:21<1:46:17,  3.77s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Parameters are one of the most critical components of machine learning models. As datasets and learning domains change, it is often necessary and time-consuming to re-learn entire models. Rather than re-learning the parameters from scratch, replacing learning with optimization, we propose a framework building upon the theory of \\emph{optimal transport} to adapt model parameters by discovering correspondences between models and data, significantly amortizing the training cost. We demonstrate our idea on the challenging problem of creating probabilistic spatial representations for autonomous robots. Although recent mapping techniques have facilitated robust occupancy mapping, learning all spatially-diverse parameters in such approximate Bayesian models demand considerable computational time, discouraging them to be used in real-world robotic mapping. Considering the fact that the geometric features a robot would observe with its sensors are similar across various environments, in this paper, we demonstrate how to re-use parameters and hyperparameters learned in different domains. This adaptation is computationally more efficient than variational inference\n[{'summary_text': 'computationally more efficient than variational inferenceParameters . we propose a framework building upon the theory of emphoptimal transport to adapt model parameters by discovering correspondences between models and data, significantly amortizing the training cost . Considering the fact that the geometric features a robot would observe with its sensors are similar across various environments .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  15%|█▌        | 302/1992 [19:25<1:49:07,  3.87s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"To select effective actions in complex environments, intelligent agents need to generalize from past experience. World models can represent knowledge about the environment to facilitate such generalization. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks purely by latent imagination. We efficiently learn behaviors by backpropagating analytic gradients of learned state values through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we present Dreamer, a reinforcement learning agent that solves long-horizon tasks purely by latent imagination . we efficiently learn behaviors by backpropagating analytic gradients of learned state values through trajectories imagined in a learned world model .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  15%|█▌        | 303/1992 [19:29<1:46:17,  3.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper we present, to the best of our knowledge, the first method to learn a generative model of 3D shapes from natural images in a fully unsupervised way. For example, we do not use any ground truth 3D or 2D annotations, stereo video, and ego-motion during the training. Our approach follows the general strategy of Generative Adversarial Networks, where an image generator network learns to create image samples that are realistic enough to fool a discriminator network into believing that they are natural images. In contrast, in our approach the image gen- eration is split into 2 stages. In the first stage a generator network outputs 3D ob- jects. In the second, a differentiable renderer produces an image of the 3D object from a random viewpoint. The key observation is that a realistic 3D object should yield a realistic rendering from any plausible viewpoint. Thus, by randomizing the choice of the viewpoint our proposed training forces the generator network\n[{'summary_text': 'in this paper we present, to the best of our knowledge, the first method to learn a generative model of 3D shapes from natural images in a fully unsupervised way . for example, we do not use any ground truth 3D or 2D annotations, stereo video, and ego-motion during the training . in our approach the image gen- eration is split into 2 stages .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  15%|█▌        | 304/1992 [19:33<1:49:52,  3.91s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIF\n[{'summary_text': 'applying mixup in training provides an effective mechanism to improve generalization performance . but in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs . this would be more efficient to actively break the locality via the globality of the model predictions.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  15%|█▌        | 305/1992 [19:37<1:50:42,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We study the problem of designing provably optimal adversarial noise algorithms that induce misclassification in settings where a learner aggregates decisions from multiple classifiers. Given the demonstrated vulnerability of state-of-the-art models to adversarial examples, recent efforts within the field of robust machine learning have focused on the use of ensemble classifiers as a way of boosting the robustness of individual models. In this paper, we design provably optimal attacks against a set of classifiers. We demonstrate how this problem can be framed as finding strategies at equilibrium in a two player, zero sum game between a learner and an adversary and consequently illustrate the need for randomization in adversarial attacks. The main technical challenge we consider is the design of best response oracles that can be implemented in a Multiplicative Weight Updates framework to find equilibrium strategies in the zero-sum game. We develop a series of scalable noise generation algorithms for deep neural networks, and show that it outperforms state-of-the\n[{'summary_text': 'in this paper, we design provably optimal attacks against a set of classifiers . we demonstrate how this problem can be framed as finding strategies at equilibrium in a two player, zero sum game between a learner and an adversary .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  15%|█▌        | 306/1992 [19:41<1:49:35,  3.90s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Prepositions are among the most frequent words. Good prepositional representation  is of great syntactic and semantic interest  in computational linguistics. Existing methods on preposition representation either treat prepositions as content words (e.g., word2vec and GloVe) or depend heavily on external linguistic resources including syntactic parsing, training task and dataset-specific representations. In this paper we use word-triple counts (one of the words is a preposition) to  capture the preposition's interaction with its head and children. Prepositional  embeddings are derived via tensor decompositions on a large unlabeled corpus.   We reveal a new geometry involving Hadamard products and empirically demonstrate its utility in paraphrasing of phrasal verbs. Furthermore, our prepositional  embeddings are used as simple features to two challenging downstream tasks: preposition selection and prepositional attachment disambiguation. We achieve comparable\n[{'summary_text': 'good prepositional representation is of great syntactic and semantic interest in computational linguistics . existing methods on preposition representation either treat prepositions as content words (e.g., word2vec and GloVe) or depend heavily on external linguistic resources .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  15%|█▌        | 307/1992 [19:44<1:49:34,  3.90s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank . we use this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP . this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  15%|█▌        | 308/1992 [19:49<1:51:20,  3.97s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"This paper revisits the problem of sequence modeling using convolutional \n architectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default\" mindset in much of\n the deep learning community is that generic sequence modeling is best handled\n using recurrent networks.  The goal of this paper is to question this assumption . \nSpecifically, we consider a simple generic temporal convolution network (TCN ),\nwhich adopts features from modern ConvNet architectures such as a dilations and \n residual connections.  We show that on a variety of sequence modeling tasks ,\nincluding many frequently used as benchmarks for evaluating recurrent networks ,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\n sometimes even highly specialized approaches.  We further show that the\n potential \"infinite memory\" advantage that RNNs have over TCNs is largely\n absent in practice: TCNs indeed\n[{'summary_text': 'this paper revisits the problem of sequence modeling using convolutional architectures . the current \"default\" mindset in much of the deep learning community is that generic sequence modeling is best handled using recurrent networks . Specifically, we consider a simple generic temporal convolution network (TCN ).'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  16%|█▌        | 309/1992 [19:52<1:50:46,  3.95s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose a method to automatically compute the importance of features at every observation in time series, by simulating counterfactual trajectories given previous observations. We define the importance of each observation as the change in the model output caused by replacing the observation with a generated one. Our method can be applied to arbitrarily complex time series models. We compare the generated feature importance to existing methods like sensitivity analyses, feature occlusion, and other explanation baselines to show that our approach generates more precise explanations and is less sensitive to noise in the input signals.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we define the importance of each observation as the change in the model output caused by replacing the observation with a generated one . we compare the generated feature importance to existing methods like sensitivity analyses, feature occlusion, and other explanation baselines .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  16%|█▌        | 310/1992 [19:56<1:45:38,  3.77s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'DEN is efficiently trained in an online manner by performing selective retraining . dynamically expands network capacity upon arrival of each task with only the necessary number of units . it effectively prevents semantic drift by splitting/duplicating units and timestamping them . we validate DEN on multiple public datasets in lifelong learning scenarios .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  16%|█▌        | 311/1992 [20:00<1:44:55,  3.75s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose learning to transfer learn (L2TL) to improve transfer learning on a target dataset by judicious extraction of information from a source dataset. L2TL considers joint optimization of vastly-shared weights between models for source and target tasks, and employs adaptive weights for scaling of constituent losses. The adaptation of the weights is based on reinforcement learning, guided with a performance metric on the target validation set. We demonstrate state-of-the-art performance of L2TL given fixed models, consistently outperforming fine-tuning baselines on various datasets. In the regimes of small-scale target datasets and significant label mismatch between source and target datasets, L2TL outperforms previous work by an even larger margin.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'L2TL considers joint optimization of vastly-shared weights between models for source and target tasks, and employs adaptive weights for scaling of constituent losses . adaptation of the weights is based on reinforcement learning, guided with a performance metric on the target validation set .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  16%|█▌        | 312/1992 [20:03<1:44:34,  3.73s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose a new application of embedding techniques to problem retrieval in adaptive tutoring. The objective is to retrieve problems similar in mathematical concepts. There are two challenges: First, like sentences, problems helpful to tutoring are never exactly the same in terms of the underlying concepts. Instead, good problems mix concepts in innovative ways, while still displaying continuity in their relationships. Second, it is difficult for humans to determine a similarity score consistent across a large enough training set. We propose a hierarchical problem embedding algorithm, called Prob2Vec, that consists of an abstraction and an embedding step. Prob2Vec achieves 96.88\\% accuracy on a problem similarity test, in contrast to 75\\% from directly applying state-of-the-art sentence embedding methods. It is surprising that Prob2Vec is able to distinguish very fine-grained differences among problems, an ability humans need time and effort to acquire. In addition, the sub-problem of concept labeling with im\n[{'summary_text': 'problems helpful to tutoring are never exactly the same in terms of the underlying concepts . but good problems mix concepts in innovative ways, while still displaying continuity in their relationships . we propose a hierarchical problem embedding algorithm, called Prob2Vec .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  16%|█▌        | 313/1992 [20:07<1:46:06,  3.79s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Collecting high-quality, large scale datasets typically requires significant resources. The aim of the present work is to improve the label efficiency of large neural networks operating on audio data through multitask learning with self-supervised tasks on unlabeled data. To this end, we trained an end-to-end audio feature extractor based on WaveNet that feeds into simple, yet versatile task-specific neural networks. We describe three self-supervised learning tasks that can operate on any large, unlabeled audio corpus. We demonstrate that, in a scenario with limited labeled training data, one can significantly improve the performance of a supervised classification task by simultaneously training it with these additional self-supervised tasks. We show that one can improve performance on a diverse sound events classification task by nearly 6\\% when jointly trained with up to three distinct self-supervised tasks. This improvement scales with the number of additional auxiliary tasks as well as the amount of unsupervised data. We also show that\n[{'summary_text': 'the aim of the present work is to improve the label efficiency of large neural networks operating on audio data through multitask learning with self-supervised tasks on unlabeled data . to this end, we trained an end-to-end audio feature extractor based on WaveNet that feeds into simple, yet versatile task-specific neural networks . we show that one can significantly improve performance on a diverse sound events classification task by nearly 6% when jointly trained .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  16%|█▌        | 314/1992 [20:11<1:50:45,  3.96s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper, we propose a generalization of the BN algorithm, diminishing batch normalization (DBN), where we update the BN parameters in a diminishing moving average way. Batch normalization (BN) is very effective in accelerating the convergence of a neural network training phase that it has become a common practice. \n Our proposed DBN algorithm remains the overall structure of the original BN algorithm while introduces a weighted averaging update to some trainable parameters. \n We provide an analysis of the convergence of the DBN algorithm that converges to a stationary point with respect to trainable parameters. Our analysis can be easily generalized for original BN algorithm by setting some parameters to constant. To the best knowledge of authors, this analysis is the first of its kind for convergence with Batch Normalization introduced. We analyze a two-layer model with arbitrary activation function. \n The primary challenge of the analysis is the fact that some parameters are updated by gradient while others are not. \n[{'summary_text': 'we propose a generalization of the BN algorithm, diminishing batch normalization (DBN) . the proposed DBN algorithm converges to a stationary point with respect to trainable parameters . to the best knowledge of authors, this analysis is the first of its kind for convergence with Batch Normalization introduced .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  16%|█▌        | 315/1992 [20:16<1:51:30,  3.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Touch interactions with current mobile devices have limited expressiveness. Augmenting devices with additional degrees of freedom can add power to the interaction, and several augmentations have been proposed and tested. However, there is still little known about the effects of learning multiple sets of augmented interactions that are mapped to different applications. To better understand whether multiple command mappings can interfere with one another, or affect transfer and retention, we developed a prototype with three pushbuttons on a smartphone case that can be used to provide augmented input to the system. The buttons can be chorded to provide seven possible shortcuts or transient mode switches. We mapped these buttons to three different sets of actions, and carried out a study to see if multiple mappings affect learning and performance, transfer, and retention. Our results show that all of the mappings were quickly learned and there was no reduction in performance with multiple mappings. Transfer to a more realistic task was successful, although with a slight reduction in accuracy. Retention after one week\n[{'summary_text': 'the buttons can be chorded to provide seven possible shortcuts or transient mode switches . all of the mappings were quickly learned and there was no reduction in performance with multiple mappings . augmentations have been proposed and tested, but little known about the effects of learning multiple sets of augmented interactions .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  16%|█▌        | 316/1992 [20:19<1:50:47,  3.97s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We study the control of symmetric linear dynamical systems with unknown dynamics and a hidden state. Using a recent spectral filtering technique for concisely representing such systems in a linear basis, we formulate optimal control in this setting as a convex program. This approach eliminates the need to solve the non-convex problem of explicit identification of the system and its latent state, and allows for provable optimality guarantees for the control signal. We give the first efficient algorithm for finding the optimal control signal with an arbitrary time horizon T, with sample complexity (number of training rollouts) polynomial only in log(T) and other relevant parameters.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'this approach eliminates the need to solve the non-convex problem of explicit identification of the system and its latent state, and allows for provable optimality guarantees for the control signal . we give the first efficient algorithm for finding the optimal control signal with an arbitrary time horizon T .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  16%|█▌        | 317/1992 [20:23<1:47:43,  3.86s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Nonlinearity is crucial to the performance of a deep (neural) network (DN).\n To date there has been little progress understanding the menagerie of available  nonlinearities, but recently progress has been made on understanding the r\\^{o}le played by piecewise affine and convex nonlinearities like the ReLU and absolute value activation functions and max-pooling.\n In particular, DN layers constructed from these operations can be interpreted as {\\em max-affine spline operators} (MASOs) that have an elegant link to vector quantization (VQ) and $K$-means.\n While this is good theoretical progress, the entire MASO approach is predicated on the requirement that the nonlinearities be piecewise affine and convex, which precludes important activation functions like the sigmoid, hyperbolic tangent, and softmax.\n {\\em This paper extends the MASO framework to these and an infinitely\n[{'summary_text': 'DN layers constructed from these operations can be interpreted as em max-affine spline operators (MASOs) that have an elegant link to vector quantization (VQ) and $K$-means . this is good theoretical progress, but the entire MASO approach is predicated on the requirement that the nonlinearities be piecewise affine and convex .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  16%|█▌        | 318/1992 [20:27<1:51:49,  4.01s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The application of deep recurrent networks to audio transcription has led to impressive gains in automatic speech recognition (ASR) systems. Many have demonstrated that small adversarial perturbations can fool deep neural networks into incorrectly predicting a specified target with high confidence. Current work on fooling ASR systems have focused on white-box attacks, in which the model architecture and parameters are known. In this paper, we adopt a black-box approach to adversarial generation, combining the approaches of both genetic algorithms and gradient estimation to solve the task. We achieve a 89.25% targeted attack similarity after 3000 generations while maintaining 94.6% audio file similarity.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the application of deep recurrent networks to audio transcription has led to impressive gains in automatic speech recognition (ASR) systems . many have demonstrated that small adversarial perturbations can fool deep neural networks into incorrectly predicting a specified target with high confidence . current work on fooling ASR systems have focused on white-box attacks, in which the model architecture and parameters are known.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  16%|█▌        | 319/1992 [20:31<1:50:06,  3.95s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Simulation is a useful tool in situations where training data for machine learning models is costly to annotate or even hard to acquire. In this work, we propose a reinforcement learning-based method for automatically adjusting the parameters of any (non-differentiable) simulator, thereby controlling the distribution of synthesized data in order to maximize the accuracy of a model trained on that data. In contrast to prior art that hand-crafts these simulation parameters or adjusts only parts of the available parameters, our approach fully controls the simulator with the actual underlying goal of maximizing accuracy, rather than mimicking the real data distribution or randomly generating a large volume of data. We find that our approach (i) quickly converges to the optimal simulation parameters in controlled experiments and (ii) can indeed discover good sets of parameters for an image rendering simulator in actual computer vision applications.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'simulation is a useful tool in situations where training data for machine learning models is costly to annotate or even hard to acquire . in this work, we propose a reinforcement learning-based method for automatically adjusting the parameters of any (non-differentiable) simulator . thereby controlling the distribution of synthesized data in order to maximize the accuracy of a model trained on that data .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  16%|█▌        | 320/1992 [20:35<1:51:49,  4.01s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"DeePa is a deep learning framework that explores parallelism in all parallelizable dimensions to accelerate the training process of convolutional neural networks. DeePa optimizes parallelism at the granularity of each individual layer in the network. We present an elimination-based algorithm that finds an optimal parallelism configuration for every layer. Our evaluation shows that DeePa achieves up to 6.5× speedup compared to state-of-the-art deep learning frameworks and reduces data transfers by up to 23×.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'deePa optimizes parallelism at the granularity of each individual layer in the network . we present an elimination-based algorithm that finds an optimal parallelisme configuration for every layer . our evaluation shows that Deepa achieves up to 6.5 speedup compared to state-of-the-art deep learning frameworks .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  16%|█▌        | 321/1992 [20:39<1:49:41,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep learning algorithms are increasingly used in modeling chemical processes. However, black box predictions without rationales have limited used in practical applications, such as drug design. To this end, we learn to identify molecular substructures -- rationales -- that are associated with the target chemical property (e.g., toxicity). The rationales are learned in an unsupervised fashion, requiring no additional information beyond the end-to-end task. We formulate this problem as a reinforcement learning problem over the molecular graph, parametrized by two convolution networks corresponding to the rationale selection and prediction based on it, where the latter induces the reward function. We evaluate the approach on two benchmark toxicity datasets. We demonstrate that our model sustains high performance under the additional constraint that predictions strictly follow the rationales. Additionally, we validate the extracted rationales through comparison against those described in chemical literature and through synthetic experiments.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'black box predictions without rationales have limited used in practical applications, such as drug design . to this end, we learn to identify molecular substructures that are associated with the target chemical property (e.g., toxicity). we formulate this problem as a reinforcement learning problem . we evaluate the approach on two benchmark toxicity datasets.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  16%|█▌        | 322/1992 [20:43<1:47:55,  3.88s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The learnability of different neural architectures can be characterized directly by computable measures of data complexity. In this paper, we reframe the problem of architecture selection as understanding how data determines the most expressive and generalizable architectures suited to that data, beyond inductive bias. After suggesting algebraic topology as a measure for data complexity, we show that the power of a network to express the topological complexity of a dataset in its decision boundary is a strictly limiting factor in its ability to generalize. We then provide the first empirical characterization of the topological capacity of neural networks. Our empirical analysis shows that at every level of dataset complexity, neural networks exhibit topological phase transitions and stratification. This observation allowed us to connect existing theory to empirically driven conjectures on the choice of architectures for a single hidden layer neural networks.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this paper, we reframe the problem of architecture selection as understanding how data determines the most expressive and generalizable architectures suited to that data, beyond inductive bias . we then provide the first empirical characterization of the topological capacity of neural networks . at every level of dataset complexity, neural networks exhibit topological phase transitions and stratification .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  16%|█▌        | 323/1992 [20:47<1:46:48,  3.84s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"An open question in the Deep Learning community is why neural networks trained with Gradient Descent generalize well on real datasets even though they are capable of fitting random data. We propose an approach to answering this question based on a hypothesis about the dynamics of gradient descent that we call Coherent Gradients: Gradients from similar examples are similar and so the overall gradient is stronger in certain directions where these reinforce each other. Thus changes to the network parameters during training are biased towards those that (locally) simultaneously benefit many examples when such similarity exists. We support this hypothesis with heuristic arguments and perturbative experiments and outline how this can explain several common empirical observations about Deep Learning. Furthermore, our analysis is not just descriptive, but prescriptive. It suggests a natural modification to gradient descent that can greatly reduce overfitting.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'an open question in the Deep Learning community is why neural networks trained with Gradient Descent generalize well on real datasets even though they are capable of fitting random data . we propose an approach to answering this question based on a hypothesis about the dynamics of gradient descent that we call Coherent Gradients . a natural modification to gradient descent can greatly reduce overfitting .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  16%|█▋        | 324/1992 [20:50<1:46:10,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We study the BERT language representation model and the sequence generation model with BERT encoder for multi-label text classification task. We experiment with both models and explore their special qualities for this setting. We also introduce and examine experimentally a mixed model, which is an ensemble of multi-label BERT and sequence generating BERT models. Our experiments demonstrated that BERT-based models and the mixed model, in particular, outperform current baselines in several metrics achieving state-of-the-art results on three well-studied multi-label classification datasets with English texts and two private Yandex Taxi datasets with Russian texts.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we experiment with both models and explore their special qualities for this setting . we introduce and examine experimentally a mixed model, which is an ensemble of multi-label BERT and sequence generating BERT models . the mixed model outperform current baselines in several metrics achieving state-of-the-art results .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  16%|█▋        | 325/1992 [20:54<1:44:25,  3.76s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this work, we propose the Sparse Deep Scattering Croisé Network (SDCSN) a novel architecture based on the Deep Scattering Network (DSN). The DSN is achieved by cascading  wavelet transform convolutions with a complex modulus and a time-invariant operator. We extend this work by first,\n crossing multiple wavelet family transforms to increase the feature diversity while avoiding any learning. Thus providing a more informative latent representation and benefit from the development of highly specialized wavelet filters over the last decades. Beside, by combining all the different wavelet representations, we reduce the amount of prior information needed regarding the signals at hand.\n Secondly, we develop an optimal thresholding strategy for over-complete filter banks that regularizes the network and controls instabilities such as inherent non-stationary noise in the signal. Our systematic and principled solution sparsifies the latent representation of the network by acting as a local mask distinguishing between activity and noise. Thus\n[{'summary_text': 'the Sparse Deep Scattering Croisé Network (SDCSN) is achieved by cascading wavelet transform convolutions with a complex modulus and a time-invariant operator . we extend this work by first, crossing multiple wavelet family transforms to increase the feature diversity while avoiding any learning . thus providing a more informative latent representation and benefit from the development of highly specialized wavelet filters over the last decades .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  16%|█▋        | 326/1992 [20:58<1:49:24,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Lifelong machine learning focuses on adapting to novel tasks without forgetting the old tasks, whereas few-shot learning strives to learn a single task given a small amount of data. These two different research areas are crucial for artificial general intelligence, however, their existing studies have somehow assumed some impractical settings when training the models. For lifelong learning, the nature (or the quantity) of incoming tasks during inference time is assumed to be known at training time. As for few-shot learning, it is commonly assumed that a large number of tasks is available during training. Humans, on the other hand, can perform these learning tasks without regard to the aforementioned assumptions. Inspired by how the human brain works, we propose a novel model, called the Slow Thinking to Learn (STL), that makes sophisticated (and slightly slower) predictions by iteratively considering interactions between current and previously seen tasks at runtime. Having conducted experiments, the results empirically demonstrate the effectiveness of STL for more realistic lifelong and few-shot learning settings\n[{'summary_text': 'lifelong machine learning focuses on adapting to novel tasks without forgetting old tasks . few-shot learning strives to learn a single task given a small amount of data . for lifelong learning, the nature (or the quantity) of incoming tasks during inference time is assumed to be known at training time .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  16%|█▋        | 327/1992 [21:02<1:49:50,  3.96s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Many tasks in artificial intelligence require the collaboration of multiple agents. We exam deep reinforcement learning for multi-agent domains. Recent research efforts often take the form of two seemingly conflicting perspectives, the decentralized perspective, where each agent is supposed to have its own controller; and the centralized perspective, where one assumes there is a larger model controlling all agents. In this regard, we revisit the idea of the master-slave architecture by incorporating both perspectives within one framework. Such a hierarchical structure naturally leverages advantages from one another. The idea of combining both perspective is intuitive and can be well motivated from many real world systems, however, out of a variety of possible realizations, we highlights three key ingredients, i.e. composed action representation, learnable communication and independent reasoning. With network designs to facilitate these explicitly, our proposal consistently outperforms latest competing methods both in synthetics experiments and when applied to challenging StarCraft  micromanagement tasks.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we examine deep reinforcement learning for multi-agent domains . recent research efforts often take the form of two seemingly conflicting perspectives, the decentralized perspective, where each agent is supposed to have its own controller . the centralized perspective where one assumes there is a larger model controlling all agents .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  16%|█▋        | 328/1992 [21:06<1:46:49,  3.85s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Neural networks offer high-accuracy solutions to a range of problems, but are computationally costly to run in production systems. We propose a technique called Deep Learning Approximation to take an already-trained neural network model and build a faster (and almost equally accurate) network by manipulating the network structure and coefficients without requiring re-training or access to the training data. Speedup is achieved by applying a sequential series of independent optimizations that reduce the floating-point operations (FLOPs) required to perform a forward pass. An optimal lossy approximation is chosen for each layer by weighing the relative accuracy loss and FLOP reduction. On PASCAL VOC 2007 with the YOLO network, we show an end-to-end 2x speedup in a network forward pass with a $5$\\% drop in mAP that can be re-gained by finetuning, enabling this network (and others like it) to be deployed in compute-constrained systems\n[{'summary_text': 'Neural networks offer high-accuracy solutions to a range of problems . but are computationally costly to run in production systems . we propose a technique called Deep Learning Approximation to take an already-trained neural network model .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  17%|█▋        | 329/1992 [21:10<1:46:34,  3.85s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"It has long been assumed that high dimensional continuous control problems cannot be solved effectively by discretizing individual dimensions of the action space due to the exponentially large number of bins over which policies would have to be learned. In this paper, we draw inspiration from the recent success of sequence-to-sequence models for structured prediction problems to develop policies over discretized spaces. Central to this method is the realization that complex functions over high dimensional spaces can be modeled by neural networks that predict one dimension at a time. Specifically, we show how Q-values and policies over continuous spaces can be modeled using a next step prediction model over discretized dimensions. With this parameterization, it is possible to both leverage the compositional structure of action spaces during learning, as well as compute maxima over action spaces (approximately). On a simple example task we demonstrate empirically that our method can perform global search, which effectively gets around the local optimization issues that plague DDPG. We apply the technique to off-\n[{'summary_text': 'in this paper, we draw inspiration from the recent success of sequence-to-sequence models for structured prediction problems to develop policies over discretized spaces . central to this method is the realization that complex functions over high dimensional spaces can be modeled by neural networks that predict one dimension at a time . on a simple example task we demonstrate empirically that our method can perform global search, which effectively gets around the local optimization issues that plague DDPG.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  17%|█▋        | 330/1992 [21:14<1:50:55,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Textual entailment (or NLI) data has proven useful as pretraining data for tasks requiring language understanding, even when building on an already-pretrained model like RoBERTa. The standard protocol for collecting NLI was not designed for the creation of pretraining data, and it is likely far from ideal for this purpose. With this application in mind we propose four alternative protocols, each aimed at improving either the ease with which annotators can produce sound training examples or the quality and diversity of those examples. Using these alternatives and a simple MNLIbased baseline, we collect and compare five new 9k-example training sets. Our primary results are largely negative, with none of these new methods showing major improvements in transfer learning. However, we make several observations that should inform future work on NLI data, such as that the use of automatically provided seed sentences for inspiration improves the quality of the resulting data on most measures, and all of the interventions we investigated dramatically reduce previously observed issues with annotation\n[{'summary_text': 'the standard protocol for collecting NLI was not designed for the creation of pretraining data, and it is likely far from ideal for this purpose . with this application in mind we propose four alternative protocols, each aimed at improving either the ease with which annotators can produce sound training examples or the quality and diversity of those examples .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  17%|█▋        | 331/1992 [21:18<1:50:40,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present a 3D capsule architecture for processing of point clouds that is equivariant with respect to the SO(3) rotation group, translation and permutation of the unordered input sets. The network operates on a sparse set of local reference frames, computed from an input point cloud and establishes end-to-end equivariance through a novel 3D quaternion group capsule layer, including an equivariant dynamic routing procedure. The capsule layer enables us to disentangle geometry from pose, paving the way for more informative descriptions and a structured latent space. In the process, we theoretically connect the process of dynamic routing between capsules to the well-known Weiszfeld algorithm, a scheme for solving iterative re-weighted least squares (IRLS) problems with provable convergence properties, enabling robust pose estimation between capsule layers. Due to the sparse equivariant quaternion capsules, our architecture allows joint object classification and orientation estimation, which we validate empirically on common benchmark\n[{'summary_text': 'the network operates on a sparse set of local reference frames, computed from an input point cloud and establishes end-to-end equivariance . the capsule layer enables us to disentangle geometry from pose, paving the way for more informative descriptions and a structured latent space .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  17%|█▋        | 332/1992 [21:22<1:50:28,  3.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose Cooperative Training (CoT) for training generative models that measure a tractable density for discrete data. CoT coordinately trains a generator G and an auxiliary predictive mediator M. The training target of M is to estimate a mixture density of the learned distribution G and the target distribution P, and that of G is to minimize the Jensen-Shannon divergence estimated through M. CoT achieves independent success without the necessity of pre-training via Maximum Likelihood Estimation or involving high-variance algorithms like REINFORCE. This low-variance algorithm is theoretically proved to be superior for both sample generation and likelihood prediction. We also theoretically and empirically show the superiority of CoT over most previous algorithms in terms of generative quality and diversity, predictive generalization ability and computational cost.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'CoT coordinately trains a generator G and an auxiliary predictive mediator M . the training target of M is to estimate a mixture density of the learned distribution G and the target distribution P . that of G is to minimize the Jensen-Shannon divergence estimated through M. CoT achieves independent success without the necessity of pre-training via Maximum Likelihood Estimation .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  17%|█▋        | 333/1992 [21:26<1:49:55,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Hierarchical reinforcement learning methods offer a powerful means of planning flexible behavior in complicated domains. However, learning an appropriate hierarchical decomposition of a domain into subtasks remains a substantial challenge. We present a novel algorithm for subtask discovery, based on the recently introduced multitask linearly-solvable Markov decision process (MLMDP) framework. The MLMDP can perform never-before-seen tasks by representing them as a linear combination of a previously learned basis set of tasks. In this setting, the subtask discovery problem can naturally be posed as finding an optimal low-rank approximation of the set of tasks the agent will face in a domain. We use non-negative matrix factorization to discover this minimal basis set of tasks, and show that the technique learns intuitive decompositions in a variety of domains. Our method has several qualitatively desirable features: it is not limited to learning subtasks with single goal states, instead learning distributed patterns of preferred states; it learns qualitatively\n[{'summary_text': 'we present a novel algorithm for subtask discovery, based on the recently introduced multitask linearly-solvable Markov decision process (MLMDP) framework . the MLMDP can perform never-before-seen tasks by representing them as a linear combination of a previously learned basis set of tasks .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  17%|█▋        | 334/1992 [21:30<1:49:42,  3.97s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Convolutional architectures have recently been shown to be competitive on many\n sequence modelling tasks when compared to the de-facto standard of recurrent neural networks (RNNs) while providing computational and modelling advantages due to inherent parallelism. However, currently, there remains a performance\n gap to more expressive stochastic RNN variants, especially those with several layers of dependent random variables. In this work, we propose stochastic temporal convolutional networks (STCNs), a novel architecture that combines the computational advantages of temporal convolutional networks (TCN) with the representational power and robustness of stochastic latent spaces. In particular, we propose a hierarchy of stochastic latent variables that captures temporal dependencies at different time-scales. The architecture is modular and flexible due to the decoupling of the deterministic and stochastic layers. We show that the proposed architecture achieves state of the art log-likelihoods across several tasks. Finally, the model\n[{'summary_text': 'modelConvolutional architectures have recently been shown to be competitive on many sequence modelling tasks when compared to the de-facto standard of recurrent neural networks (RNNs) while providing computational and modelling advantages due to inherent parallelism . however, currently, there remains a performance gap to more expressive stochastic RNN variants, especially those with several layers of dependent random variables .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  17%|█▋        | 335/1992 [21:34<1:51:24,  4.03s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We develop end-to-end learned reconstructions for lensless mask-based cameras, including an experimental system for capturing aligned lensless and lensed images for training.   Various reconstruction methods are explored, on a scale from classic iterative approaches (based on the physical imaging model) to deep learned methods with many learned parameters.   In the middle ground, we present several variations of unrolled alternating direction method of multipliers (ADMM) with varying numbers of learned parameters. The network structure combines knowledge of the physical imaging model with learned parameters updated from the data, which compensate for artifacts caused by physical approximations. Our unrolled approach is 20X faster than classic methods and produces better reconstruction quality than both the classic and deep methods on our experimental system.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Various reconstruction methods are explored, on a scale from classic iterative approaches to deep learned methods with many learned parameters . the network structure combines knowledge of the physical imaging model with learned parameters updated from the data, which compensate for artifacts caused by physical approximations .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  17%|█▋        | 336/1992 [21:38<1:48:17,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We study the benefit of sharing representations among tasks to enable the effective use of deep neural networks in Multi-Task Reinforcement Learning. We leverage the assumption that learning from different tasks, sharing common properties, is helpful to generalize the knowledge of them resulting in a more effective feature extraction compared to learning a single task. Intuitively, the resulting set of features offers performance benefits when used by Reinforcement Learning algorithms. We prove this by providing theoretical guarantees that highlight the conditions for which is convenient to share representations among tasks, extending the well-known finite-time bounds of Approximate Value-Iteration to the multi-task setting. In addition, we complement our analysis by proposing multi-task extensions of three Reinforcement Learning algorithms that we empirically evaluate on widely used Reinforcement Learning benchmarks showing significant improvements over the single-task counterparts in terms of sample efficiency and performance.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'intuitively, the resulting set of features offers performance benefits when used by Reinforcement Learning algorithms . we leverage the assumption that learning from different tasks, sharing common properties, is helpful to generalize the knowledge of them resulting in a more effective feature extraction .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  17%|█▋        | 337/1992 [21:42<1:45:43,  3.83s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The International Competition on Knowledge Engineering for Planning and Scheduling (ICKEPS) plays a pivotal role in fostering the development of new Knowledge Engineering (KE) tools, and in emphasising the importance of principled approaches for all the different KE aspects that are needed for the successful long-term use of planning in real-world applications.  \n In this paper, as an exercise in synthesis and for the sake of stimulating thoughts and discussion, we review the format of previous ICKEPS, to suggest alternative formats for future competitions, ideally to motivate someone to step up and organise the next ones.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the International Competition on Knowledge Engineering for Planning and Scheduling (ICKEPS) plays a pivotal role in fostering the development of new Knowledge Engineering tools . in this paper, we review the format of previous ICKEPS, to suggest alternative formats for future competitions . ideally to motivate someone to step up and organise the next ones .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  17%|█▋        | 338/1992 [21:45<1:44:20,  3.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Backdoor attacks aim to manipulate a subset of training data by injecting adversarial triggers such that machine learning models trained on the tampered dataset will make arbitrarily (targeted) incorrect prediction on the testset with the same trigger embedded. While federated learning (FL) is capable of aggregating information provided by different parties for training a better model, its distributed learning methodology and inherently heterogeneous data distribution across parties may bring new vulnerabilities. In addition to recent centralized backdoor attacks on FL where each party embeds the same global trigger during training, we propose the distributed backdoor attack (DBA) --- a novel threat assessment framework developed by fully exploiting the distributed nature of FL. DBA decomposes a global trigger pattern into separate local patterns and embed them into the training set of different adversarial parties respectively. Compared to standard centralized backdoors, we show that DBA is substantially more persistent and stealthy against FL on diverse datasets such as finance and image data. We conduct extensive experiments to show that the attack success\n[{'summary_text': 'backdoor attacks aim to manipulate subset of training data by injecting adversarial triggers such that machine learning models trained on the tampered dataset will make arbitrarily (targeted) incorrect prediction on the testset with the same trigger embedded . federated learning (FL) is capable of aggregating information provided by different parties for training a better model, but its distributed learning methodology and inherently heterogeneous data distribution across parties may bring new vulnerabilities .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  17%|█▋        | 339/1992 [21:50<1:49:15,  3.97s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Survival function estimation is used in many disciplines, but it is most common in medical analytics in the form of the Kaplan-Meier estimator. Sensitive data (patient records) is used in the estimation without any explicit control on the information leakage, which is a significant privacy concern. We propose a first differentially private estimator of the survival function and show that it can be easily extended to provide differentially private confidence intervals and test statistics without spending any extra privacy budget. We further provide extensions for differentially private estimation of the competing risk cumulative incidence function. Using nine real-life clinical datasets, we provide empirical evidence that our proposed method provides good utility while simultaneously providing strong privacy guarantees.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'survival function estimation is used in many disciplines, but it is most common in medical analytics in the form of the Kaplan-Meier estimator . a first differentially private estimator of the survival function is used without any explicit control on the information leakage, which is a significant privacy concern .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  17%|█▋        | 340/1992 [21:53<1:45:57,  3.85s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep generative models seek to recover the process with which the observed data was generated. They may be used to synthesize new samples or to subsequently extract representations. Successful approaches in the domain of images are driven by several core inductive biases. However, a bias to account for the compositional way in which humans structure a visual scene in terms of objects has frequently been overlooked. In this work we propose to structure the generator of a GAN to consider objects and their relations explicitly, and generate images by means of composition. This provides a way to efficiently learn a more accurate generative model of real-world images, and serves as an initial step towards learning corresponding object representations. We evaluate our approach on several multi-object image datasets, and find that the generator learns to identify and disentangle information corresponding to different objects at a representational level. A human study reveals that the resulting generative model is better at generating images that are more faithful to the reference distribution.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'successful approaches in the domain of images are driven by several core inductive biases . but a bias to account for the compositional way in which humans structure a visual scene in terms of objects has frequently been overlooked . in this work we propose to structure the generator of a GAN to consider objects and their relations explicitly, and generate images by means of composition .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  17%|█▋        | 341/1992 [21:57<1:45:28,  3.83s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We introduce an unsupervised structure learning algorithm for deep, feed-forward, neural networks. We propose a new interpretation for depth and inter-layer connectivity where a hierarchy of independencies in the input distribution is encoded in the network structure. This results in structures allowing neurons to connect to neurons in any deeper layer skipping intermediate layers. Moreover, neurons in deeper layers encode low-order (small condition sets) independencies and have a wide scope of the input, whereas neurons in the first layers encode higher-order (larger condition sets) independencies and have a narrower scope. Thus, the depth of the network is automatically determined---equal to the maximal order of independence in the input distribution, which is the recursion-depth of the algorithm. The proposed algorithm constructs two main graphical models: 1) a generative latent graph (a deep belief network) learned from data and 2) a deep discriminative graph constructed from the generative latent graph. We prove that conditional dependencies between the nodes\n[{'summary_text': 'the proposed algorithm constructs two main graphical models: a generative latent graph (a deep belief network) learned from data and 2) a deep discriminative graph constructed from the generative letent graph . we propose a new interpretation for depth and inter-layer connectivity . this results in structures allowing neurons to connect to neurons in any deeper layer skipping intermediate layers .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  17%|█▋        | 342/1992 [22:01<1:47:41,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"This paper introduces NEMO, an approach to unsupervised object detection that uses motion---instead of image labels---as a cue to learn object detection. To discriminate between motion of the target object and other changes in the image, it relies on negative examples that show the scene without the object. The required data can be collected very easily by recording two short videos, a positive one showing the object in motion and a negative one showing the scene without the object. Without any additional form of pretraining or supervision and despite of occlusions, distractions, camera motion, and adverse lighting, those videos are sufficient to learn object detectors that can be applied to new videos and even generalize to unseen scenes and camera angles. In a baseline comparison, unsupervised object detection outperforms off-the shelf template matching and tracking approaches that are given an initial bounding box of the object. The learned object representations are also shown to be accurate enough to capture the relevant information from manipulation task demonstrations, which makes\n[{'summary_text': 'NEMO is an approach to unsupervised object detection that uses motion---instead of image labels---as a cue to learn object detection . the required data can be collected very easily by recording two short videos, a positive showing the object in motion and a negative showing the scene without the object . despite occlusions, distractions, camera motion, and adverse lighting, those videos are sufficient to learn objects detectors that can be applied to new videos .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  17%|█▋        | 343/1992 [22:06<1:52:00,  4.08s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Employing deep neural networks as natural image priors to solve inverse problems either requires large amounts of data to sufficiently train expressive generative models or can succeed with no data via untrained neural networks. However, very few works have considered how to interpolate between these no- to high-data regimes. In particular, how can one use the availability of a small amount of data (even 5-25 examples) to one's advantage in solving these inverse problems and can a system's performance increase as the amount of data increases as well? In this work, we consider solving linear inverse problems when given a small number of examples of images that are drawn from the same distribution as the image of interest. Comparing to untrained neural networks that use no data, we show how one can pre-train a neural network with a few given examples to improve reconstruction results in compressed sensing and semantic image recovery problems such as colorization. Our approach leads to improved reconstruction as the amount of available data increases and is on\n[{'summary_text': \"inverse problems either require large amounts of data to sufficiently train expressive generative models or can succeed with no data via untrained neural networks . however, very few works have considered how to interpolate between these no- to high-data regimes . in particular, how can one use the availability of a small amount of data (even 5-25 examples) to one's advantage in solving these problems?\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  17%|█▋        | 344/1992 [22:10<1:54:38,  4.17s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The Hamiltonian formalism plays a central role in classical and quantum physics. Hamiltonians are the main tool for modelling the continuous time evolution of systems with conserved quantities, and they come equipped with many useful properties, like time reversibility and smooth interpolation in time. These properties are important for many machine learning problems - from sequence prediction to reinforcement learning and density modelling - but are not typically provided out of the box by standard tools such as recurrent neural networks. In this paper, we introduce the Hamiltonian Generative Network (HGN), the first approach capable of consistently learning Hamiltonian dynamics from high-dimensional observations (such as images) without restrictive domain assumptions. Once trained, we can use HGN to sample new trajectories, perform rollouts both forward and backward in time, and even speed up or slow down the learned dynamics. We demonstrate how a simple modification of the network architecture turns HGN into a powerful normalising flow model, called Neural Hamiltonian Flow (NHF), that uses\n[{'summary_text': 'Hamiltonians are the main tool for modelling the continuous time evolution of systems with conserved quantities . they come equipped with many useful properties, like time reversibility and smooth interpolation in time . these properties are important for many machine learning problems - from sequence prediction to reinforcement learning and density modelling . once trained, we can use HGN to sample new trajectories, perform rollouts both forward and backward in time and even speed up or slow down the learned dynamics .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  17%|█▋        | 345/1992 [22:14<1:57:22,  4.28s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We aim to build complex humanoid agents that integrate perception, motor control, and memory. In this work, we partly factor this problem into low-level motor control from proprioception and high-level coordination of the low-level skills informed by vision. We develop an architecture capable of surprisingly flexible, task-directed motor control of a relatively high-DoF humanoid body by combining pre-training of low-level motor controllers with a high-level, task-focused controller that switches among low-level sub-policies. The resulting system is able to control a physically-simulated humanoid body to solve tasks that require coupling visual perception from an unstabilized egocentric RGB camera during locomotion in the environment. Supplementary video link:  https://youtu.be/fBoir7PNxPk<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'resulting system is able to control a physically-simulated humanoid body to solve tasks that require coupling visual perception from an unstabilized egocentric RGB camera during locomotion in the environment . we develop an architecture capable of surprisingly flexible, task-directed motor control .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  17%|█▋        | 346/1992 [22:18<1:53:01,  4.12s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Human annotation for syntactic parsing is expensive, and large resources are available only for a  fraction of languages. A question we ask is whether one can leverage abundant unlabeled texts to improve syntactic parsers, beyond just using the texts to obtain more generalisable lexical features (i.e. beyond word embeddings). To this end, we propose a novel latent-variable generative model for semi-supervised syntactic dependency parsing. As exact inference is intractable, we introduce a differentiable relaxation to obtain approximate samples and compute gradients with respect to the parser parameters. Our method (Differentiable Perturb-and-Parse) relies on differentiable dynamic programming over stochastically perturbed edge scores. We demonstrate effectiveness of our approach with experiments on English, French and Swedish.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'human annotation for syntactic parsing is expensive, and large resources are available only for a fraction of languages . the method relies on differentiable dynamic programming over stochastically perturbed edge scores . we demonstrate effectiveness of our approach with experiments on English, French and Swedish .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  17%|█▋        | 347/1992 [22:22<1:48:40,  3.96s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Sequence-to-sequence attention-based models are a promising approach for end-to-end speech recognition. The increased model power makes the training procedure more difficult, and analyzing failure modes of these models becomes harder because of the end-to-end nature. In this work, we present various analyses to better understand training and model properties. We investigate on pretraining variants such as growing in depth and width, and their impact on the final performance, which leads to over 8% relative improvement in word error rate. For a better understanding of how the attention process works, we study the encoder output and the attention energies and weights. Our experiments were performed on Switchboard, LibriSpeech and Wall Street Journal.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the increased model power makes the training procedure more difficult . analyzing failure modes becomes harder because of the end-to-end nature . in this work, we present various analyses to better understand training and model properties .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  17%|█▋        | 348/1992 [22:25<1:43:31,  3.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Learning rich representation from data is an important task for deep generative models such as variational auto-encoder (VAE). However, by extracting high-level abstractions in the bottom-up inference process, the goal of preserving all factors of variations for top-down generation is compromised. Motivated by the concept of “starting small”, we present a strategy to progressively learn independent hierarchical representations from high- to low-levels of abstractions. The model starts with learning the most abstract representation, and then progressively grow the network architecture to introduce new  representations at different levels of abstraction. We quantitatively demonstrate the ability of the presented model to improve disentanglement in comparison to existing works on two benchmark datasets using three disentanglement metrics, including a new metric we proposed to complement the previously-presented metric of mutual information gap. We further present both qualitative and quantitative evidence on how the progression of learning improves disentangling of hierarchical representations. By drawing on the respective\n[{'summary_text': 'learning rich representation from data is an important task for deep generative models . but by extracting high-level abstractions in the bottom-up inference process, the goal of preserving all factors of variations for top-down generation is compromised . Motivated by the concept of “starting small”, we present a strategy to progressively learn independent hierarchical representations from high- to low-levels of abstractions .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  18%|█▊        | 349/1992 [22:29<1:47:18,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Humans rely on episodic memory constantly, in remembering the name of someone they met 10 minutes ago, the plot of a movie as it unfolds, or where they parked the car. Endowing reinforcement learning agents with episodic memory is a key step on the path toward replicating human-like general intelligence. We analyze why standard RL agents lack episodic memory today, and why existing RL tasks don't require it. We design a new form of external memory called Masked Experience Memory, or MEM, modeled after key features of human episodic memory. To evaluate episodic memory we define an RL task based on the common children's game of Concentration. We find that a MEM RL agent leverages episodic memory effectively to master Concentration, unlike the baseline agents we tested.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': \"to evaluate episodic memory we define an RL task based on the common children's game of Concentration . we find that a MEM RL agent leverages episodic memories effectively to master Concentration, unlike the baseline agents we tested . a new form of external memory called Masked Experience Memory, or MEM, modeled after key features .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  18%|█▊        | 350/1992 [22:33<1:45:49,  3.87s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty  and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable approximate Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions\n[{'summary_text': 'deep ensembles were theoretically motivated by the bootstrap . non-bootstrap ensembles trained with just random initialization also perform well in practice . scalable approximate Bayesian methods tend to focus on a single mode .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  18%|█▊        | 351/1992 [22:37<1:46:31,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"A distinct commonality between HMMs and RNNs is that they both learn hidden representations for sequential data. In addition, it has been noted that the backward computation  of  the  Baum-Welch  algorithm  for  HMMs  is  a  special  case  of  the back propagation algorithm used for neural networks (Eisner (2016)).   Do these observations  suggest  that,  despite  their  many apparent  differences,  HMMs  are a special case of RNNs?    In this paper,  we investigate a series of architectural transformations between HMMs and RNNs, both through theoretical derivations and empirical hybridization, to answer this question. In particular, we investigate three key design factors—independence assumptions between the hidden states and the observation, the placement of softmax, and the use of non-linearity—in order to pin down their empirical effects.   We present a comprehensive empirical study to\n[{'summary_text': 'the backward computation of the Baum-Welch algorithm for HMMs is a special case of the back propagation algorithm used for neural networks (Eisner (2016)). do these observations suggest that, despite their many apparent differences, . they both learn hidden representations for sequential data .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  18%|█▊        | 352/1992 [22:41<1:46:30,  3.90s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"With innovations in architecture design, deeper and wider neural network models deliver improved performance on a diverse variety of tasks. But the increased memory footprint of these models presents a challenge during training, when all intermediate layer activations need to be stored for back-propagation. Limited GPU memory forces practitioners to make sub-optimal choices: either train inefficiently with smaller batches of examples; or limit the architecture to have lower depth and width, and fewer layers at higher spatial resolutions. This work introduces an approximation strategy that significantly reduces a network's memory footprint during training, but has negligible effect on training performance and computational expense. During the forward pass, we replace activations with lower-precision approximations immediately after they have been used by subsequent layers, thus freeing up memory. The approximate activations are then used during the backward pass. This approach limits the accumulation of errors across the forward and backward pass---because the forward computation across the network still happens at full precision, and the approximation has a limited\n[{'summary_text': 'limited GPU memory forces practitioners to make sub-optimal choices . During the forward pass, we replace activations with lower-precision approximations immediately after they have been used by subsequent layers, thus freeing up memory . the approximate activations are then used during the backward pass .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  18%|█▊        | 353/1992 [22:45<1:47:42,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep generative models can emulate the perceptual properties of complex image datasets, providing a latent representation of the data. However, manipulating such representation to perform meaningful and controllable transformations in the data space remains challenging without some form of supervision. While previous work has focused on exploiting statistical independence to \\textit{disentangle} latent factors, we argue that such requirement can be advantageously relaxed and propose instead a non-statistical framework that relies on identifying a modular organization of the network, based on counterfactual manipulations. Our experiments support that modularity between groups of channels is achieved to a certain degree on a variety of generative models. This allowed the design of targeted interventions on complex image datasets, opening the way to applications such as computationally efficient style transfer and the automated assessment of robustness to contextual changes in pattern recognition systems.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'generative models can emulate the perceptual properties of complex image datasets, providing a latent representation of the data . however, manipulating such representation to perform meaningful and controllable transformations in the data space remains challenging without some form of supervision . previous work has focused on exploiting statistical independence to textitdisentangle latent factors, we argue that such requirement can be advantageously relaxed and propose instead a non-statistical framework that relies on identifying a modular organization of the network'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  18%|█▊        | 354/1992 [22:49<1:50:04,  4.03s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Sparsely available data points cause a numerical error on finite differences which hinder to modeling the dynamics of physical systems. The discretization error becomes even larger when the sparse data are irregularly distributed so that the data defined on an unstructured grid, making it hard to build deep learning models to handle physics-governing observations on the unstructured grid. In this paper, we propose a novel architecture named Physics-aware Difference Graph Networks (PA-DGN) that exploits neighboring information to learn finite differences inspired by physics equations. PA-DGN further leverages data-driven end-to-end learning to discover underlying dynamical relations between the spatial and temporal differences in given observations. We demonstrate the superiority of PA-DGN in the approximation of directional derivatives and the prediction of graph signals on the synthetic data and the real-world climate observations from weather stations.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'discretization error becomes even larger when sparse data are irregularly distributed so that the data defined on an unstructured grid . a novel architecture called Physics-aware Difference Graph Networks (PA-DGN) exploits neighboring information to learn finite differences inspired by physics equations .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  18%|█▊        | 355/1992 [22:53<1:46:59,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Word embedding is a powerful tool in natural language processing. In this paper we consider the problem of word embedding composition \\--- given vector representations of two words, compute a vector for the entire phrase. We give a generative model that can capture specific syntactic relations between words. Under our model, we prove that the correlations between three words (measured by their PMI) form a tensor that has an approximate low rank Tucker decomposition. The result of the Tucker decomposition gives the word embeddings as well as a core tensor, which can be used to produce better compositions of the word embeddings. We also complement our theoretical results with experiments that verify our assumptions, and demonstrate the effectiveness of the new composition method.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this paper we consider the problem of word embedding composition --- given vector representations of two words, compute a vector for the entire phrase . under our model, we prove that the correlations between three words (measured by their PMI) form a tensor that has an approximate low rank Tucker decomposition .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  18%|█▊        | 356/1992 [22:57<1:44:55,  3.85s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Targeted clean-label poisoning is a type of adversarial attack on machine learning systems where the adversary injects a few correctly-labeled, minimally-perturbed samples into the training data thus causing the deployed model to misclassify a particular test sample during inference. Although defenses have been proposed for general poisoning attacks (those which aim to reduce overall test accuracy), no reliable defense for clean-label attacks has been demonstrated, despite the attacks' effectiveness and their realistic use cases. In this work, we propose a set of simple, yet highly-effective defenses against these attacks. \n We test our proposed approach against two recently published clean-label poisoning attacks, both of which use the CIFAR-10 dataset. After reproducing their experiments, we demonstrate that our defenses are able to detect over 99% of poisoning examples in both attacks and remove them without any compromise on model performance. Our simple defenses show that current clean-label poisoning attack strategies can be annulled, and serve\n[{'summary_text': 'targeted clean-label poisoning is a type of adversarial attack on machine learning systems where the adversary injects a few correctly-labeled, minimally-perturbed samples into the training data thus causing the deployed model to misclassify a particular test sample during inference .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  18%|█▊        | 357/1992 [23:01<1:49:03,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose Adversarial Inductive Transfer Learning (AITL), a method for addressing discrepancies in input and output spaces between source and target domains. AITL utilizes adversarial domain adaptation and multi-task learning to address these discrepancies. Our motivating application is pharmacogenomics where the goal is to predict drug response in patients using their genomic information. The challenge is that clinical data (i.e. patients) with drug response outcome is very limited, creating a need for transfer learning to bridge the gap between large pre-clinical pharmacogenomics datasets (e.g. cancer cell lines) and clinical datasets. Discrepancies exist between 1) the genomic data of pre-clinical and clinical datasets (the input space), and 2) the different measures of the drug response (the output space). To the best of our knowledge, AITL is the first adversarial inductive transfer learning method to address both input and output discrepancies. Experimental results indicate that AITL outperforms state-of\n[{'summary_text': 'AITL uses adversarial domain adaptation and multi-task learning to address these discrepancies . clinical data (i.e. patients) with drug response outcome is very limited, creating a need for transfer learning to bridge the gap between large pre-clinical pharmacogenomics datasets .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  18%|█▊        | 358/1992 [23:05<1:48:27,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generating visualizations and interpretations from high-dimensional data is a\n common problem in many fields. Two key approaches for tackling this problem \n are clustering and representation learning. There are very performant deep\n clustering models on the one hand and interpretable representation learning techniques, \n often relying on latent topological structures such as self-organizing maps,\n on the other hand. However, current methods do not yet successfully combine\n these two approaches. We present a new deep architecture for probabilistic clustering, \n VarPSOM, and its extension to time series data, VarTPSOM, composed of VarPSOM \n modules connected by LSTM cells. We show that they achieve superior \n clustering performance compared to current deep clustering methods on static \n MNIST/Fashion-MNIST data as well as medical time series, while inducing an\n interpretable representation. Moreover, on the medical time series, VarTPSOM\n successfully predicts\n[{'summary_text': 'two key approaches for tackling this problem are clustering and representation learning . there are very performant deep clustering models on the one hand and interpretable representation learning techniques, often relying on latent topological structures such as self-organizing maps . but current methods do not successfully combine these two approaches .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  18%|█▊        | 359/1992 [23:09<1:47:49,  3.96s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"State-of-the-art results in imitation learning are currently held by adversarial methods that iteratively estimate the divergence between student and expert policies and then minimize this divergence to bring the imitation policy closer to expert behavior. Analogous techniques for imitation learning from observations alone (without expert action labels), however, have not enjoyed the same ubiquitous successes. \n Recent work in adversarial methods for generative models has shown that the measure used to judge the discrepancy between real and synthetic samples is an algorithmic design choice, and that different choices can result in significant differences in model performance. Choices including Wasserstein distance and various $f$-divergences have already been explored in the adversarial networks literature, while more recently the latter class has been investigated for imitation learning. Unfortunately, we find that in practice this existing imitation-learning framework for using $f$-divergences suffers from numerical instabilities stemming from the combination of function approximation and policy-gradient reinforcement learning. In\n[{'summary_text': 'state-of-the-art results in imitation learning are currently held by adversarial methods that iteratively estimate the divergence between student and expert policies . the measure used to judge the discrepancy between real and synthetic samples is an algorithmic design choice, and that different choices can result in significant differences in model performance.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  18%|█▊        | 360/1992 [23:13<1:48:09,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We show that if the usual training loss is augmented by a Lipschitz regularization term, then the networks generalize.   We prove generalization by first establishing a stronger convergence result, along with a rate of convergence.    A second result resolves a question posed in Zhang et al. (2016): how can a model distinguish between the case of clean labels, and randomized labels?   Our answer is that Lipschitz regularization using the Lipschitz constant of the clean data makes this distinction.   In this case, the model learns a different function which we hypothesize correctly fails to learn the dirty labels.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we prove generalization by first establishing a stronger convergence result, along with a rate of convergence . a second result resolves a question posed in Zhang et al. (2016): how can a model distinguish between the case of clean labels, and randomized labels?'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  18%|█▊        | 361/1992 [23:17<1:45:40,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Convolution Neural Network (CNN) has gained tremendous success in computer vision tasks with its outstanding ability to capture the local latent features. Recently, there has been an increasing interest in extending CNNs to the general spatial domain. Although various types of graph convolution and geometric convolution methods have been proposed, their connections to traditional 2D-convolution are not well-understood. In this paper, we show that depthwise separable convolution is a path to unify the two kinds of convolution methods in one mathematical view, based on which we derive a novel Depthwise Separable Graph Convolution that subsumes existing graph convolution methods as special cases of our formulation. Experiments show that the proposed approach consistently outperforms other graph convolution and geometric convolution baselines on benchmark datasets in multiple domains.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'convolution Neural Network (CNN) has gained tremendous success in computer vision tasks with its outstanding ability to capture the local latent features . recent interest in extending CNNs to the general spatial domain are not well-understood .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  18%|█▊        | 362/1992 [23:20<1:41:43,  3.74s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We argue that the widely used Omniglot and miniImageNet benchmarks are too simple because their class semantics do not vary across episodes, which defeats their intended purpose of evaluating few-shot classification methods. The class semantics of Omniglot is invariably “characters” and the class semantics of miniImageNet, “object category”. Because the class semantics are so similar, we propose a new method called Centroid Networks which can achieve surprisingly high accuracies on Omniglot and miniImageNet without using any labels at metaevaluation time. Our results suggest that those benchmarks are not adapted for supervised few-shot classification since the supervision itself is not necessary during meta-evaluation. The Meta-Dataset, a collection of 10 datasets, was recently proposed as a harder few-shot classification benchmark. Using our method, we derive a new metric, the Class Semantics Consistency Criterion, and use it to quantify the difficulty of Meta-Dataset.\n[{'summary_text': 'the widely used Omniglot and miniImageNet benchmarks are too simple . the class semantics are invariably “characters” and the classification semantics of miniImagenet, “object category” . we propose a new method called Centroid Networks which can achieve surprisingly high accuracies .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  18%|█▊        | 363/1992 [23:24<1:43:36,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Concerns about interpretability, computational resources, and principled inductive priors have motivated efforts to engineer sparse  neural  models for NLP tasks. If sparsity is important for NLP, might well-trained neural models naturally become roughly sparse? Using the Taxi-Euclidean norm to measure sparsity, we find that frequent input words are associated with concentrated or sparse activations, while frequent target words are associated with dispersed activations but concentrated gradients. We find that  gradients associated with function words are more concentrated than the gradients of content words, even controlling for word frequency.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'concerns about interpretability, computational resources, and principled inductive priors have motivated efforts to engineer sparse neural models for NLP tasks . Using the Taxi-euclidean norm to measure sparsity, we find that frequent input words are associated with concentrated or sparsing activations . frequent target words are associate with dispersed activations but concentrated gradients .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  18%|█▊        | 364/1992 [23:28<1:43:24,  3.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"End-to-end acoustic-to-word speech recognition models have recently gained popularity because they are easy to train, scale well to large amounts of training data, and do not require a lexicon. In addition, word models may also be easier to integrate with downstream tasks such as spoken language understanding, because inference (search) is much simplified compared to phoneme, character or any other sort of sub-word units. In this paper, we describe methods to construct contextual acoustic word embeddings directly from a supervised sequence-to-sequence acoustic-to-word speech recognition model using the learned attention distribution. On a suite of 16 standard sentence evaluation tasks, our embeddings show competitive performance against a word2vec model trained on the speech transcriptions. In addition, we evaluate these embeddings on a spoken language understanding task and observe that our embeddings match the performance of text-based embeddings in a pipeline of first performing speech recognition and then constructing word embeddings\n[{'summary_text': 'acoustic-to-word speech recognition models have gained popularity . they are easy to train, scale well to large amounts of training data . inference (search) is much simplified compared to phoneme, character or any other sort of sub-word units .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  18%|█▊        | 365/1992 [23:32<1:43:41,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Point clouds, as a form of Lagrangian representation, allow for powerful and flexible applications in a large number of computational disciplines. We propose a novel deep-learning method to learn stable and temporally coherent feature spaces for points clouds that change over time. We identify a set of inherent problems with these approaches: without knowledge of the time dimension, the inferred solutions can exhibit strong flickering, and easy solutions to suppress this flickering can result in undesirable local minima that manifest themselves as halo structures. We propose a novel temporal loss function that takes into account higher time derivatives of the point positions, and encourages mingling, i.e., to prevent the aforementioned halos. We combine these techniques in a super-resolution method with a truncation approach to flexibly adapt the size of the generated positions. We show that our method works for large, deforming point sets from different sources to demonstrate the flexibility of our approach.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'point clouds, as a form of Lagrangian representation, allow for powerful and flexible applications in a large number of computational disciplines . we propose a novel deep-learning method to learn stable and temporally coherent feature spaces for points clouds that change over time . without knowledge of the time dimension, the inferred solutions can exhibit strong flickering .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  18%|█▊        | 366/1992 [23:35<1:43:24,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recent deep generative models can provide photo-realistic images as well as visual or textual content embeddings useful to address various tasks of computer vision and natural language processing. Their usefulness is nevertheless often limited by the lack of control over the generative process or the poor understanding of the learned representation. To overcome these major issues, very recent works have shown the interest of studying the semantics of the latent space of generative models. In this paper, we propose to advance on the interpretability of the latent space of generative models by introducing a new method to find meaningful directions in the latent space of any generative model along which we can move to control precisely specific properties of the generated image like position or scale of the object in the image. Our method is weakly supervised and particularly well suited for the search of directions encoding simple transformations of the generated image, such as translation, zoom or color variations. We demonstrate the effectiveness of our method qualitatively and quantitatively, both for GANs and\n[{'summary_text': 'recent deep generative models can provide photo-realistic images as well as visual or textual content embeddings useful to address various tasks of computer vision and natural language processing . their usefulness is often limited by the lack of control over the generative process or the poor understanding of the learned representation .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  18%|█▊        | 367/1992 [23:39<1:44:06,  3.84s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The dependency of the generalization error of neural networks on model and dataset size is of critical importance both in practice and for understanding the theory of neural networks. Nevertheless, the functional form of this dependency remains elusive. In this work, we present a functional form which approximates well the generalization error in practice. Capitalizing on the successful concept of model scaling (e.g., width, depth), we are able to simultaneously construct such a form and specify the exact models which can attain it across model/data scales. Our construction follows insights obtained from observations conducted over a range of model/data scales, in various model types and datasets, in vision and language tasks. We show that the form both fits the observations well across scales, and provides accurate predictions from small- to large-scale models and data.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'dependency of generalization error of neural networks on model and dataset size remains elusive . in this work, we present a functional form which approximates well the generalization errors in practice . we show that the form both fits the observations well across scales .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  18%|█▊        | 368/1992 [23:43<1:40:54,  3.73s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The recent direction of unpaired image-to-image translation is on one hand very exciting as it alleviates the big burden in obtaining label-intensive pixel-to-pixel supervision, but it is on the other hand not fully satisfactory due to the presence of artifacts and degenerated transformations. In this paper, we take a manifold view of the problem by introducing a smoothness term over the sample graph to attain harmonic functions to enforce consistent mappings during the translation. We develop HarmonicGAN to learn bi-directional translations between the source and the target domains. With the help of similarity-consistency, the inherent self-consistency property of samples can be maintained. Distance metrics defined on two types of features including histogram and CNN are exploited. Under an identical problem setting as CycleGAN, without additional manual inputs and only at a small training-time cost, HarmonicGAN demonstrates a significant qualitative and quantitative improvement over the state of the art, as well as improved interpretability.\n[{'summary_text': 'the recent direction of unpaired image-to-image translation is on one hand very exciting . but it is on the other hand not fully satisfactory due to the presence of artifacts and degenerated transformations . in this paper, we take a manifold view of the problem by introducing a smoothness term over the sample graph to attain harmonic functions .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  19%|█▊        | 369/1992 [23:47<1:44:16,  3.86s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Learning to learn is a powerful paradigm for enabling models to learn from data more effectively and efficiently. A popular approach to meta-learning is to train a recurrent model to read in a training dataset as input and output the parameters of a learned model, or output predictions for new test inputs. Alternatively, a more recent approach to meta-learning aims to acquire deep representations that can be effectively fine-tuned, via standard gradient descent, to new tasks. In this paper, we consider the meta-learning problem from the perspective of universality, formalizing the notion of learning algorithm approximation and comparing the expressive power of the aforementioned recurrent models to the more recent approaches that embed gradient descent into the meta-learner. In particular, we seek to answer the following question: does deep representation combined with standard gradient descent have sufficient capacity to approximate any learning algorithm? We find that this is indeed true, and further find, in our experiments, that gradient-based meta-learning consistently leads to learning strategies that general\n[{'summary_text': 'learning to learn is a powerful paradigm for enabling models to learn from data more effectively and efficiently . a popular approach to meta-learning is to train a recurrent model to read in a training dataset as input and output the parameters of a learned model, or output predictions for new test inputs .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  19%|█▊        | 370/1992 [23:51<1:44:47,  3.88s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The recent development of Natural Language Processing (NLP) has achieved great success using large pre-trained models with hundreds of millions of parameters. However, these models suffer from the heavy model size and high latency such that we cannot directly deploy them to resource-limited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like BERT, MobileBERT is task-agnostic; that is, it can be universally applied to various downstream NLP tasks via fine-tuning. MobileBERT is a slimmed version of BERT-LARGE augmented with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we use a bottom-to-top progressive scheme to transfer the intrinsic knowledge of a specially designed Inverted Bottleneck BERT-LARGE teacher to it. Empirical studies show that MobileBERT is 4.3x smaller and 4\n[{'summary_text': 'mobileBERT is a slimmed version of BERT-LARGE augmented with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks . this paper proposes MobileBERT for compressing and accelerating the popular BERT model .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  19%|█▊        | 371/1992 [23:55<1:46:11,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Equilibrium Propagation (EP) is a learning algorithm that bridges Machine Learning and Neuroscience, by computing gradients closely matching those of Backpropagation Through Time (BPTT), but with a learning rule local in space.\n Given an input x and associated target y, EP proceeds in two phases: in the first phase neurons evolve freely towards a first steady state; in the second phase output neurons are nudged towards y until they reach a second steady state.\n However, in existing implementations of EP, the learning rule is not local in time:\n the weight update is performed after the dynamics of the second phase have converged and requires information of the first phase that is no longer available physically.\n This is a major impediment to the biological plausibility of EP and its efficient hardware implementation.\n In this work, we propose a version of EP named Continual Equilibrium Propagation (C-EP) where neuron and synapse dynamics occur simultaneously throughout the second phase,\n[{'summary_text': 'Equilibrium Propagation (EP) is a learning algorithm that bridges Machine Learning and Neuroscience . given an input x and associated target y, EP proceeds in two phases . in the first phase neurons evolve freely towards a first steady state; in the second phase output neurons are nudged towards y until they reach a second steady state .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  19%|█▊        | 372/1992 [23:59<1:47:20,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generative models such as Variational Auto Encoders (VAEs) and Generative Adversarial Networks (GANs) are typically trained for a fixed prior distribution in the latent space, such as uniform or Gaussian. After a trained model is obtained, one can sample the Generator in various forms for exploration and understanding, such as interpolating between two samples, sampling in the vicinity of a sample or exploring differences between a pair of samples applied to a third sample. However, the latent space operations commonly used in the literature so far induce a distribution mismatch between the resulting outputs and the prior distribution the model was trained on. Previous works have attempted to reduce this mismatch with heuristic modification to the operations or by changing the latent distribution and re-training models. In this paper, we propose a framework for modifying the latent space operations such that the distribution mismatch is fully eliminated. Our approach is based on optimal transport maps, which adapt the latent space operations such that they fully match the prior distribution,\n[{'summary_text': 'Generative models such as Variational Auto Encoders are typically trained for a fixed prior distribution in the latent space, such as uniform or Gaussian . after a trained model is obtained, one can sample the Generator in various forms for exploration and understanding . previous works have attempted to reduce this mismatch with heuristic modification to the operations .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  19%|█▊        | 373/1992 [24:03<1:49:43,  4.07s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We introduce Doc2Dial, an end-to-end framework for generating conversational data grounded in business documents via crowdsourcing. Such data can be used to train automated dialogue agents performing customer care tasks for the enterprises or organizations. In particular, the framework takes the documents as input and generates the tasks for obtaining the annotations for simulating dialog flows. The dialog flows are used to guide the collection of utterances produced by crowd workers. The outcomes include dialogue data grounded in the given documents, as well as various types of annotations that help ensure the quality of the data and the flexibility to (re)composite dialogues.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Doc2Dial is an end-to-end framework for generating conversational data grounded in business documents via crowdsourcing . the framework takes the documents as input and generates the tasks for obtaining the annotations for simulating dialog flows . outcomes include dialogue data based in the given documents, as well as various types of annotations that help ensure the quality of the data .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  19%|█▉        | 374/1992 [24:07<1:47:35,  3.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Neural Architecture Search (NAS) is an exciting new field which promises to be as much as a game-changer as Convolutional Neural Networks were in 2012. Despite many great works leading to substantial improvements on a variety of tasks, comparison between different methods is still very much an open issue. While most algorithms are tested on the same datasets, there is no shared experimental protocol followed by all. As such, and due to the under-use of ablation studies, there is a lack of clarity regarding why certain methods are more effective than others. Our first contribution is a benchmark of 8 NAS methods on 5 datasets. To overcome the hurdle of comparing methods with different search spaces, we propose using a method’s relative improvement over the randomly sampled average architecture, which effectively removes advantages arising from expertly engineered search spaces or training protocols. Surprisingly, we find that many NAS techniques struggle to significantly beat the average architecture baseline. We perform further experiments with the commonly used DARTS search space in order\n[{'summary_text': 'despite many great works leading to substantial improvements on a variety of tasks, comparison between different methods is still very much an open issue . most algorithms are tested on the same datasets, there is no shared experimental protocol followed by all . despite the under-use of ablation studies there is a lack of clarity regarding why certain methods are more effective than others .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  19%|█▉        | 375/1992 [24:11<1:48:18,  4.02s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We introduce ES-MAML, a new framework for solving the model agnostic meta learning (MAML) problem based on Evolution Strategies (ES). Existing algorithms for MAML are based on policy gradients, and incur significant difficulties when attempting to estimate second derivatives using backpropagation on stochastic policies. We show how ES can be applied to MAML to obtain an algorithm which avoids the problem of estimating second derivatives, and is also conceptually simple and easy to implement. Moreover, ES-MAML can handle new types of nonsmooth adaptation operators, and other techniques for improving performance and estimation of ES methods become applicable. We show empirically that ES-MAML is competitive with existing methods and often yields better adaptation with fewer queries.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'ES-MAML is a new framework for solving the model agnostic meta learning (MAML) problem based on Evolution Strategies (ES) existing algorithms for MAML are based . on policy gradients, and incur significant difficulties when attempting to estimate second derivatives using backpropagation on stochastic policies . ES can handle new types of nonsmooth adaptation operators, and other techniques for improving performance and estimation of ES methods become applicable .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  19%|█▉        | 376/1992 [24:15<1:48:43,  4.04s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Knowledge bases (KB), both automatically and manually constructed, are often incomplete --- many valid facts can be inferred from the KB by synthesizing existing information. A popular approach to KB completion is to infer new relations by combinatory reasoning over the information found along other paths connecting a pair of entities. Given the enormous size of KBs and the exponential number of paths, previous path-based models have considered only the problem of predicting a missing relation given two entities, or evaluating the truth of a proposed triple. Additionally, these methods have traditionally used random paths between fixed entity pairs or more recently learned to pick paths between them. We propose a new algorithm, MINERVA, which addresses the much more difficult and practical task of answering questions where the relation is known, but only one entity. Since random walks are impractical in a setting with unknown destination and combinatorially many paths from a start node, we present a neural reinforcement learning approach which learns how to navigate the graph conditioned on the input query to find\n[{'summary_text': 'a popular approach to KB completion is to infer new relations by combinatory reasoning over the information found along other paths connecting a pair of entities . previous path-based models have considered only the problem of predicting a missing relation given two entities, or evaluating the truth of a proposed triple .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  19%|█▉        | 377/1992 [24:19<1:48:21,  4.03s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Model-based reinforcement learning (MBRL) has been shown to be a powerful framework for data-efficiently learning control of continuous tasks. Recent work in MBRL has mostly focused on using more advanced function approximators and planning schemes, leaving the general framework virtually unchanged since its conception. In this paper, we identify a fundamental issue of the standard MBRL framework -- what we call the objective mismatch issue. Objective mismatch arises when one objective is optimized in the hope that a second, often uncorrelated, metric will also be optimized. In the context of MBRL, we characterize the objective mismatch between training the forward dynamics model w.r.t. the likelihood of the one-step ahead prediction, and the overall goal of improving performance on a downstream control task. For example, this issue can emerge with the realization that dynamics models effective for a specific task do not necessarily need to be globally accurate, and vice versa globally accurate models might not be sufficiently accurate locally to obtain good control performance on\n[{'summary_text': 'model-based reinforcement learning (MBRL) has been shown to be a powerful framework for data-efficiently learning control of continuous tasks . recent work in MBRL has mostly focused on using more advanced function approximators and planning schemes . objective mismatch arises when one objective is optimized in the hope that a second, often uncorrelated, metric will also be optimized .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  19%|█▉        | 378/1992 [24:23<1:49:16,  4.06s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and\n[{'summary_text': 'generative models of audio directly generate samples in one of two domains: time or frequency . a third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception . but these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  19%|█▉        | 379/1992 [24:27<1:48:50,  4.05s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recent work has focused on combining kernel methods and deep learning. With this in mind, we introduce Deepström networks -- a new architecture of neural networks which we use to replace top dense layers of standard convolutional architectures with an approximation of a kernel function by relying on the Nyström approximation. \n Our approach is easy highly flexible. It is compatible with any kernel function and it allows exploiting multiple kernels. \n We show that Deepström networks reach state-of-the-art performance on standard datasets like SVHN and CIFAR100. One benefit of the method lies in its limited number of learnable parameters which make it particularly suited for small training set sizes, e.g. from 5 to 20 samples per class. Finally we illustrate two ways of using multiple kernels, including a multiple Deepström  setting, that exploits a kernel on each feature map output by the convolutional part of the model.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'our approach is easy highly flexible. it is compatible with any kernel function and it allows exploiting multiple kernels . deepström networks reach state-of-the-art performance on standard datasets like SVHN and CIFAR100 .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  19%|█▉        | 380/1992 [24:31<1:44:03,  3.87s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep neural networks have become the state-of-the-art models in numerous machine learning tasks. However, general guidance to network architecture design is still missing. In our work, we bridge deep neural network design with numerical differential equations. We show that many effective networks, such as ResNet, PolyNet, FractalNet and RevNet, can be interpreted as different numerical discretizations of differential equations. This finding brings us a brand new perspective on the design of effective deep architectures. We can take advantage of the rich knowledge in numerical analysis to guide us in designing new and potentially more effective deep networks. As an example, we propose a linear multi-step architecture (LM-architecture) which is inspired by the linear multi-step method solving ordinary differential equations. The LM-architecture is an effective structure that can be used on any ResNet-like networks. In particular, we demonstrate that LM-ResNet and LM-ResNeXt (i.e.\n[{'summary_text': 'deep neural networks have become the state-of-the-art models in numerous machine learning tasks . but general guidance to network architecture design is still missing . in our work, we bridge deep neural network design with numerical differential equations .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  19%|█▉        | 381/1992 [24:35<1:43:04,  3.84s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Batch Normalization (BN) and its variants have seen widespread adoption in the deep learning community because they improve the training of deep neural networks. Discussions of why this normalization works so well remain unsettled.   We make explicit the relationship between ordinary least squares and partial derivatives computed when back-propagating through BN. We recast the back-propagation of BN as a least squares fit, which zero-centers and decorrelates partial derivatives from normalized activations. This view, which we term {\\em gradient-least-squares}, is an extensible and arithmetically accurate description of BN. To further explore this perspective, we motivate, interpret, and evaluate two adjustments to BN.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Batch Normalization (BN) and its variants have seen widespread adoption in the deep learning community because they improve the training of deep neural networks . Discussions of why this normalization works so well remain unsettled . we make explicit the relationship between ordinary least squares and partial derivatives computed when back-propagating through BN .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  19%|█▉        | 382/1992 [24:38<1:41:56,  3.80s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Training agents to operate in one environment often yields overfitted models that are unable to generalize to the changes in that environment. However, due to the numerous variations that can occur in the real-world, the agent is often required to be robust in order to be useful. This has not been the case for agents trained with reinforcement learning (RL) algorithms. In this paper, we investigate the overfitting of RL agents to the training environments in visual navigation tasks. Our experiments show that deep RL agents can overfit even when trained on multiple environments simultaneously. \n We propose a regularization method which combines RL with supervised learning methods by adding a term to the RL objective that would encourage the invariance of a policy to variations in the observations that ought not to affect the action taken. The results of this method, called invariance regularization, show an improvement in the generalization of policies to environments not seen during training.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the results of this method, called invariance regularization, show an improvement in the generalization of policies to environments not seen during training . this has not been the case for agents trained with reinforcement learning algorithms .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  19%|█▉        | 383/1992 [24:42<1:38:38,  3.68s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Score matching provides an effective approach to learning flexible unnormalized models, but its scalability is limited by the need to evaluate a second-order derivative.   In this paper,we connect a general family of learning objectives including score matching to Wassersteingradient flows. This connection enables us to design a scalable approximation to theseobjectives, with a form similar to single-step contrastive divergence. We present applications in training implicit variational and Wasserstein auto-encoders with manifold-valued priors.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this paper,we connect a general family of learning objectives including score matching to Wassersteingradient flows . this connection enables us to design a scalable approximation to theseobjectives, with a form similar to single-step contrastive divergence . we present applications in training implicit variational and Wasserstein auto-encoders with manifold-valued priors.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  19%|█▉        | 384/1992 [24:45<1:39:15,  3.70s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Network embedding (NE) methods aim to learn low-dimensional representations of network nodes as vectors, typically in Euclidean space. These representations are then used for a variety of downstream prediction tasks. Link prediction is one of the most popular choices for assessing the performance of NE methods. However, the complexity of link prediction requires a carefully designed evaluation pipeline to provide consistent, reproducible and comparable results. We argue this has not been considered sufficiently in recent works. The main goal of this paper is to overcome difficulties associated with evaluation pipelines and reproducibility of results. We introduce EvalNE, an evaluation framework to transparently assess and compare the performance of NE methods on link prediction. EvalNE provides automation and abstraction for tasks such as hyper-parameter tuning, model validation, edge sampling, computation of edge embeddings and model validation. The framework integrates efficient procedures for edge and non-edge sampling and can be used to easily evaluate any off-the-shelf embedding method. The framework\n[{'summary_text': 'the frameworkNetwork embedding (NE) methods aim to learn low-dimensional representations of network nodes as vectors, typically in Euclidean space . these representations are then used for a variety of downstream prediction tasks . the complexity of link prediction requires a carefully designed evaluation pipeline .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  19%|█▉        | 385/1992 [24:49<1:41:38,  3.80s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper, a new intrinsic reward generation method for sparse-reward reinforcement learning is proposed based on an ensemble of dynamics models. In the proposed method, the mixture of multiple dynamics models is used to approximate the true unknown transition probability, and the intrinsic reward is designed as the minimum of the surprise seen from each dynamics model to the mixture of the dynamics models. In order to show the effectiveness of the proposed intrinsic reward generation method, a working algorithm is constructed by combining the proposed intrinsic reward generation method with the proximal policy optimization (PPO) algorithm. Numerical results show that for representative locomotion tasks, the proposed model-ensemble-based intrinsic reward generation method outperforms the previous methods based on a single dynamics model.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'a new intrinsic reward generation method for sparse-reward reinforcement learning is proposed based on an ensemble of dynamics models . in the proposed method, the mixture of multiple dynamics models is used to approximate the true unknown transition probability . the intrinsic reward is designed as the minimum of the surprise seen from each dynamics model to the mixture .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  19%|█▉        | 386/1992 [24:53<1:40:23,  3.75s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Transforming one probability distribution to another is a powerful tool in Bayesian inference and machine learning. Some prominent examples are constrained-to-unconstrained transformations of distributions for use in Hamiltonian Monte-Carlo and constructing flexible and learnable densities such as normalizing flows. We present Bijectors.jl, a software package for transforming distributions implemented in Julia, available at github.com/TuringLang/Bijectors.jl. The package provides a flexible and composable way of implementing transformations of distributions without being tied to a computational framework. \n\n We demonstrate the use of Bijectors.jl on improving variational inference by encoding known statistical dependencies into the variational posterior using normalizing flows, providing a general approach to relaxing the mean-field assumption usually made in variational inference.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'some prominent examples are constrained-to-unconstrained transformations of distributions for use in Hamiltonian Monte-Carlo . we present Bijectors.jl, a software package for transforming distributions implemented in Julia . the package provides a flexible and composable way of implementing transformations without being tied to a computational framework .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  19%|█▉        | 387/1992 [24:57<1:40:57,  3.77s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Semi-Supervised Learning (SSL) approaches have been an influential framework for the usage of unlabeled data when there is not a sufficient amount of labeled data available over the course of training. SSL methods based on Convolutional Neural Networks (CNNs) have recently provided successful results on standard benchmark tasks such as image classification. In this work, we consider the general setting of  SSL problem where the labeled and unlabeled data  come from the same underlying probability distribution. We  propose a new approach that adopts  an Optimal Transport (OT) technique serving as a metric of similarity between discrete empirical probability measures to  provide pseudo-labels for the unlabeled data, which can then be used in conjunction with the initial labeled data to train the CNN model in an SSL manner. We have evaluated and compared our proposed method with state-of-the-art SSL algorithms on standard datasets to demonstrate the superiority and effectiveness of our  SSL algorithm.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'SSL methods based on Convolutional Neural Networks (CNNs) have recently provided successful results on standard benchmark tasks such as image classification . in this work, we consider the general setting of SSL problem where the labeled and unlabeled data come from the same underlying probability distribution . we propose a new approach that adopts an Optimal Transport technique serving as a metric of similarity between discrete empirical probability measures .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  19%|█▉        | 388/1992 [25:01<1:43:12,  3.86s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We study the use of the Wave-U-Net architecture for speech enhancement, a model introduced by Stoller et al for the separation of music vocals and accompaniment.   This end-to-end learning method for audio source separation operates directly in the time domain, permitting the integrated modelling of phase information and being able to take large  temporal contexts into account.   Our experiments show that the proposed method improves several metrics, namely PESQ, CSIG, CBAK, COVL and SSNR, over the  state-of-the-art with respect to the speech enhancement task on the Voice Bank corpus (VCTK) dataset. We find that a reduced number of hidden layers is sufficient for speech enhancement in comparison to the original system designed for singing voice separation in music. We see this initial result as an encouraging signal to further explore speech enhancement in the time-domain, both as an end in itself and as a pre-processing step to speech recognition systems.\n[{'summary_text': 'the proposed method improves several metrics, namely PESQ, CSIG, CBAK, COVL and SSNR, over the state-of-the-art . we find that a reduced number of hidden layers is sufficient for speech enhancement in comparison to the original system designed for singing voice separation in music .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  20%|█▉        | 389/1992 [25:05<1:44:28,  3.91s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Capturing long-range feature relations has been a central issue on convolutional neural networks(CNNs). To tackle this, attempts to integrate end-to-end trainable attention module on CNNs are widespread. Main goal of these works is to adjust feature maps considering spatial-channel correlation inside a convolution layer. In this paper, we focus on modeling relationships among layers and propose a novel structure, 'Recurrent Layer Attention network,' which stores the hierarchy of features into recurrent neural networks(RNNs) that concurrently propagating with CNN and adaptively scales feature volumes of all layers. We further introduce several structural derivatives for demonstrating the compatibility on recent attention modules and the expandability of proposed network. For semantic understanding on learned features, we also visualize intermediate layers and plot the curve of layer scaling coefficients(i.e., layer attention). Recurrent Layer Attention network achieves significant performance enhancement requiring a slight increase on parameters in an image classification task with CIFAR and ImageNet-1K\n[{'summary_text': 'attempts to integrate end-to-end trainable attention module on CNNs are widespread . main goal of these works is to adjust feature maps considering spatial-channel correlation inside a convolution layer . in this paper, we focus on modeling relationships among layers .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  20%|█▉        | 390/1992 [25:09<1:43:17,  3.87s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The linear transformations in converged deep networks show fast eigenvalue decay. The distribution of eigenvalues looks like a Heavy-tail distribution, where the vast majority of eigenvalues is small, but not actually zero, and only a few spikes of large eigenvalues exist.\n We use a stochastic approximator to generate histograms of eigenvalues. This allows us to investigate layers with hundreds of thousands of dimensions. We show how the distributions change over the course of image net training, converging to a similar heavy-tail spectrum across all intermediate layers.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'linear transformations in converged deep networks show fast eigenvalue decay . the distribution looks like a Heavy-tail distribution, where the vast majority of eigevalues is small, but not actually zero . we use a stochastic approximator to generate histograms . this allows us to investigate layers with hundreds of thousands of dimensions .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  20%|█▉        | 391/1992 [25:13<1:42:26,  3.84s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Learning to imitate expert behavior from demonstrations can be challenging, especially in environments with high-dimensional, continuous observations and unknown dynamics. Supervised learning methods based on behavioral cloning (BC) suffer from distribution shift: because the agent greedily imitates demonstrated actions, it can drift away from demonstrated states due to error accumulation. Recent methods based on reinforcement learning (RL), such as inverse RL and generative adversarial imitation learning (GAIL), overcome this issue by training an RL agent to match the demonstrations over a long horizon. Since the true reward function for the task is unknown, these methods learn a reward function from the demonstrations, often using complex and brittle approximation techniques that involve adversarial training. We propose a simple alternative that still uses RL, but does not require learning a reward function. The key idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. We accomplish this\n[{'summary_text': 'supervised learning methods based on behavioral cloning suffer from distribution shift . because the agent greedily imitates demonstrated actions, it can drift away from demonstrated states due to error accumulation . inverse RL and generative adversarial imitation learning (GAIL) overcome this issue by training an RL agent to match the demonstrations over a long horizon .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  20%|█▉        | 392/1992 [25:17<1:44:42,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We analyze speed of convergence to global optimum for gradient descent training a deep linear neural network by minimizing the L2 loss over whitened data.   Convergence at a linear rate is guaranteed when the following hold: (i) dimensions of hidden layers are at least the minimum of the input and output dimensions; (ii) weight matrices at initialization are approximately balanced; and (iii) the initial loss is smaller than the loss of any rank-deficient solution.   The assumptions on initialization (conditions (ii) and (iii)) are necessary, in the sense that violating any one of them may lead to convergence failure.   Moreover, in the important case of output dimension 1, i.e. scalar regression, they are met, and thus convergence to global optimum holds, with constant probability under a random initialization scheme.   Our results significantly extend previous analyses, e.g., of deep linear residual networks (Bartlett et al., 2018).<|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we analyze speed of convergence to global optimum for gradient descent training a deep linear neural network by minimizing the L2 loss over whitened data . dimensions of hidden layers are at least the minimum of the input and output dimensions; weight matrices at initialization are approximately balanced . the initial loss is smaller than the loss of any rank-deficient solution .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  20%|█▉        | 393/1992 [25:21<1:43:54,  3.90s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"When data arise from multiple latent subpopulations, machine learning frameworks typically estimate parameter values independently for each sub-population. In this paper, we propose\n to overcome these limits by considering samples as tasks in a multitask learning framework.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'when data arise from multiple latent subpopulations, machine learning frameworks typically estimate parameter values independently for each sub-population . in this paper, we propose to overcome these limits by considering samples as tasks in a multitask learning framework .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  20%|█▉        | 394/1992 [25:24<1:39:06,  3.72s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this work we study generalization of neural networks in gradient-based meta-learning by analyzing various properties of the objective landscapes. We experimentally demonstrate that as meta-training progresses, the meta-test solutions obtained by adapting the meta-train solution of the model to new tasks via few steps of gradient-based fine-tuning, become flatter, lower in loss, and further away from the meta-train solution. We also show that those meta-test solutions become flatter even as generalization starts to degrade, thus providing an experimental evidence against the correlation between generalization and flat minima in the paradigm of gradient-based meta-leaning. Furthermore, we provide empirical evidence that generalization to new tasks is correlated with the coherence between their adaptation trajectories in parameter space, measured by the average cosine similarity between task-specific trajectory directions, starting from a same meta-train solution. We also show that coherence of meta-test gradients, measured by the average inner\n[{'summary_text': 'in this work we study generalization of neural networks in gradient-based meta-learning by analyzing various properties of the objective landscapes . we demonstrate that as meta-training progresses, the meta-test solutions obtained by adapting a meta-train solution of the model become flatter even as generalization starts to degrade . also show that generalization to new tasks is correlated with the coherence between their adaptation trajectories in parameter space, measured by the average cosine similarity between task-specific trajectory directions'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  20%|█▉        | 395/1992 [25:28<1:45:46,  3.97s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We consider the task of few shot link prediction, where the goal is to predict missing edges across multiple graphs using only a small sample of known edges. We show that current link prediction methods are generally ill-equipped to handle this task---as they cannot effectively transfer knowledge between graphs in a multi-graph setting and are unable to effectively learn from very sparse data. To address this challenge, we introduce a new gradient-based meta learning framework, Meta-Graph, that leverages higher-order gradients along with a learned graph signature function that conditionally generates a graph neural network initialization. Using a novel set of few shot link prediction benchmarks, we show that Meta-Graph enables not only fast adaptation but also better final convergence and can effectively learn using only a small sample of true edges.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we consider the task of few shot link prediction, where the goal is to predict missing edges across multiple graphs using only a small sample of known edges . to address this challenge, we introduce a new gradient-based meta learning framework, Meta-Graph, that leverages higher-order gradients along with a learned graph signature function that conditionally generates a graph neural network initialization .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  20%|█▉        | 396/1992 [25:32<1:44:43,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam, Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train\n[{'summary_text': 'first order stochastic optimization methods commonly used in the Euclidean domain have already been adapted to certain Riemannian settings . however, some of the most popular of these optimization tools - namely Adam, Adagrad and the more recent Amsgrad - remain to be generalized . we discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemennian setting, and then provide algorithms and convergence proofs .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  20%|█▉        | 397/1992 [25:37<1:48:03,  4.06s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Multi-agent cooperation is an important feature of the natural world. Many tasks involve individual incentives that are misaligned with the common good, yet a wide range of organisms from bacteria to insects and humans are able to overcome their differences and collaborate. Therefore, the emergence of cooperative behavior amongst self-interested individuals is an important question for the fields of multi-agent reinforcement learning (MARL) and evolutionary theory. Here, we study a particular class of multi-agent problems called intertemporal social dilemmas (ISDs), where the conflict between the individual and the group is particularly sharp. By combining MARL with appropriately structured natural selection, we demonstrate that individual inductive biases for cooperation can be learned in a model-free way. To achieve this, we introduce an innovative modular architecture for deep reinforcement learning agents which supports multi-level selection. We present results in two challenging environments, and interpret these in the context of cultural and ecological evolution.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'multi-agent cooperation is an important feature of the natural world . many tasks involve individual incentives that are misaligned with the common good . however, a wide range of organisms from bacteria to insects and humans are able to overcome their differences and collaborate .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  20%|█▉        | 398/1992 [25:40<1:46:07,  3.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Reinforcement learning typically requires carefully designed reward functions in order to learn the desired behavior. We present a novel reward estimation method that is based on a finite sample of optimal state trajectories from expert demon- strations and can be used for guiding an agent to mimic the expert behavior. The optimal state trajectories are used to learn a generative or predictive model of the “good” states distribution. The reward signal is computed by a function of the difference between the actual next state acquired by the agent and the predicted next state given by the learned generative or predictive model. With this inferred reward function, we perform standard reinforcement learning in the inner loop to guide the agent to learn the given task. Experimental evaluations across a range of tasks demonstrate that the proposed method produces superior performance compared to standard reinforcement learning with both complete or sparse hand engineered rewards. Furthermore, we show that our method successfully enables an agent to learn good actions directly from expert player video of games such as the Super Mario Bros and\n[{'summary_text': 'the optimal state trajectories are used to learn a generative or predictive model of the “good” states distribution . the reward signal is computed by a function of the difference between the actual next state acquired by the agent and the predicted next state given . with this inferred reward function, we perform standard reinforcement learning in the inner loop .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  20%|██        | 399/1992 [25:45<1:47:03,  4.03s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present an information-theoretic framework for understanding trade-offs in unsupervised learning of deep latent-variables models using variational inference. This framework emphasizes the need to consider latent-variable models along two dimensions: the ability to reconstruct inputs (distortion) and the communication cost (rate). We derive the optimal frontier of generative models in the two-dimensional rate-distortion plane, and show how the standard evidence lower bound objective is insufficient to select between points along this frontier. However, by performing targeted optimization to learn generative models with different rates, we are able to learn many models that can achieve similar generative performance but make vastly different trade-offs in terms of the usage of the latent variable. Through experiments on MNIST and Omniglot with a variety of architectures, we show how our framework sheds light on many recent proposed extensions to the variational autoencoder family.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we derive the optimal frontier of generative models in the two-dimensional rate-distortion plane . we show how the standard evidence lower bound objective is insufficient to select between points along this frontier . but by performing targeted optimization to learn models with different rates, we are able to learn many models that can achieve similar generative performance but make vastly different trade-offs .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  20%|██        | 400/1992 [25:48<1:45:45,  3.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Reinforcement learning in an actor-critic setting relies on accurate value estimates of the critic. However, the combination of function approximation, temporal difference (TD) learning and off-policy training can lead to an overestimating value function. A solution is to use Clipped Double Q-learning (CDQ), which is used in the TD3 algorithm and computes the minimum of two critics in the TD-target. \n We show that CDQ induces an underestimation bias and propose a new algorithm that accounts for this by using a weighted average of the target from CDQ and the target coming from a single critic.\n The weighting parameter is adjusted during training such that the value estimates match the actual discounted return on the most recent episodes and by that it balances over- and underestimation.\n Empirically, we obtain more accurate value estimates and demonstrate state of the art results on several OpenAI gym tasks.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the combination of function approximation, temporal difference (TD) learning and off-policy training can lead to an overestimating value function . a solution is to use Clipped Double Q-learning (CDQ) which is used in the TD3 algorithm and computes the minimum of two critics . the weighting parameter is adjusted during training such that the value estimates match the actual discounted return on the most recent episodes .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  20%|██        | 401/1992 [25:53<1:46:52,  4.03s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep Reinforcement Learning algorithms lead to agents that can solve difficult decision making problems in complex environments. However, many difficult multi-agent competitive games, especially real-time strategy games are still considered beyond the capability of current deep reinforcement learning algorithms, although there has been a recent effort to change this \\citep{openai_2017_dota, vinyals_2017_starcraft}. Moreover, when the opponents in a competitive game are suboptimal, the current \\textit{Nash Equilibrium} seeking, self-play algorithms are often unable to generalize their strategies to opponents that play strategies vastly different from their own. This suggests that a learning algorithm that is beyond conventional self-play is necessary. We develop Hierarchical Agent with Self-play (HASP), a learning approach for obtaining hierarchically structured policies that can achieve higher performance than conventional self-play on competitive games through the use of a diverse pool of sub-policies we get from Counter Self\n[{'summary_text': 'deep reinforcement learning algorithms lead to agents that can solve difficult decision making problems in complex environments . many difficult multi-agent competitive games, especially real-time strategy games are still considered beyond the capability of current deep reinforcement algorithms . this suggests that a learning algorithm that is beyond conventional self-play is necessary .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  20%|██        | 402/1992 [25:57<1:45:45,  3.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Unsupervised domain adaptation aims to generalize the hypothesis trained in a source domain to an unlabeled target domain. One popular approach to this problem is to learn a domain-invariant representation for both domains. In this work, we study, theoretically and empirically, the explicit effect of the embedding on generalization to the target domain. In particular, the complexity of the class of embeddings affects an upper bound on the target domain's risk. This is reflected in our experiments, too.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': \"unsupervised domain adaptation aims to generalize the hypothesis trained in a source domain to an unlabeled target domain . one popular approach to this problem is to learn a domain-invariant representation for both domains . complexity of the class of embeddings affects an upper bound on the target domain's risk .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  20%|██        | 403/1992 [26:00<1:42:43,  3.88s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generative Adversarial Networks (GANs) are a very powerful framework for generative modeling. However, they are often hard to train, and learning of GANs often becomes unstable. Wasserstein GAN (WGAN) is a promising framework to deal with the instability problem as it has a good convergence property. One drawback of the WGAN is that it evaluates the Wasserstein distance in the dual domain, which requires some approximation, so that it may fail to optimize the true Wasserstein distance. In this paper, we propose evaluating the exact empirical optimal transport cost efficiently in the primal domain and performing gradient descent with respect to its derivative to train the generator network. Experiments on the MNIST dataset show that our method is significantly stable to converge, and achieves the lowest Wasserstein distance among the WGAN variants at the cost of some sharpness of generated images. Experiments on the 8-Gaussian toy dataset show that better gradients for the generator are obtained in\n[{'summary_text': 'waterstein GAN (WGAN) is a promising framework to deal with the instability problem . but they are often hard to train, and learning of GANs often becomes unstable . WGAN evaluates the Wasserstein distance in the dual domain, which requires some approximation, so that it may fail to optimize the true waterstein distance.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  20%|██        | 404/1992 [26:04<1:44:18,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We describe two end-to-end autoencoding models for semi-supervised graph-based dependency parsing. The first model is a Local Autoencoding Parser (LAP) encoding the input using continuous latent variables in a sequential manner; The second model is a Global Autoencoding Parser (GAP) encoding the input into dependency trees as latent variables, with exact inference. Both models consist of two parts: an encoder enhanced by deep neural networks (DNN) that can utilize the contextual information to encode the input into latent variables, and a decoder which is a generative model able to reconstruct the input. Both LAP and GAP admit a unified structure with different loss functions for labeled and unlabeled data with shared parameters. We conducted experiments on WSJ and UD dependency parsing data sets, showing that our models can exploit the unlabeled data to boost the performance given a limited amount of labeled data.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'two end-to-end autoencoding models for semi-supervised graph-based dependency parsing . the first model is a Local Autoencoding Parser (LAP) encoding the input using continuous latent variables in a sequential manner . a second model is an encoder enhanced by deep neural networks (DNN) that can utilize the contextual information to encode the input into dependency trees .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  20%|██        | 405/1992 [26:08<1:45:09,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep reinforcement learning has achieved great success in many previously difficult reinforcement learning tasks, yet recent studies show that deep RL agents are also unavoidably susceptible to adversarial perturbations, similar to deep neural networks in classification tasks. Prior works mostly focus on model-free adversarial attacks and agents with discrete actions. In this work, we study the problem of continuous control agents in deep RL with adversarial attacks and propose the first two-step algorithm based on learned model dynamics. Extensive experiments on various MuJoCo domains (Cartpole, Fish, Walker, Humanoid) demonstrate that our proposed framework is much more effective and efficient than model-free based attacks baselines in degrading agent performance as well as driving agents to unsafe states.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'deep RL agents are also unavoidably susceptible to adversarial perturbations, similar to deep neural networks in classification tasks . prior works mostly focus on model-free adversarials and agents with discrete actions .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  20%|██        | 406/1992 [26:12<1:42:24,  3.87s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Continual learning is the problem of sequentially learning new tasks or knowledge while protecting previously acquired knowledge. However, catastrophic forgetting poses a grand challenge for neural networks performing such learning process. Thus, neural networks that are deployed in the real world often struggle in scenarios where the data distribution is non-stationary (concept drift), imbalanced, or not always fully available, i.e., rare edge cases. We propose a Differentiable Hebbian Consolidation model which is composed of a Differentiable Hebbian Plasticity (DHP) Softmax layer that adds a rapid learning plastic component (compressed episodic memory) to the fixed (slow changing) parameters of the softmax output layer; enabling learned representations to be retained for a longer timescale. We demonstrate the flexibility of our method by integrating well-known task-specific synaptic consolidation methods to penalize changes in the slow weights that are important for each target task. We evaluate our approach on the Permuted MNIST, Split MN\n[{'summary_text': 'neural networks that are deployed in the real world often struggle in scenarios where the data distribution is non-stationary (concept drift), imbalanced, or not always fully available, i.e., rare edge cases . we propose a Differentiable Hebbian Consolidation model which is composed of a differentiable hebbian plasticity (DHP) Softmax layer that adds a rapid learning plastic component (compressed episodic memory) to the fixed (slow changing) parameters of the softmax output'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  20%|██        | 407/1992 [26:16<1:47:50,  4.08s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present an end-to-end trained memory system that quickly adapts to new data and generates samples like them. Inspired by Kanerva's sparse distributed memory, it has a robust  distributed reading and writing mechanism. The memory is analytically tractable, which enables optimal on-line compression via a Bayesian update-rule. We formulate it as a hierarchical conditional generative model, where memory provides a rich data-dependent prior distribution. Consequently, the top-down memory and bottom-up perception are combined to produce the code representing an observation. Empirically, we demonstrate that the adaptive memory significantly improves generative models trained on both the Omniglot and CIFAR datasets. Compared with the Differentiable Neural Computer (DNC) and its variants, our memory model has greater capacity and is significantly easier to train.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'adaptive memory significantly improves generative models trained on both the Omniglot and CIFAR datasets . compared with the Differentiable Neural Computer (DNC), our memory model has greater capacity and is significantly easier to train .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  20%|██        | 408/1992 [26:20<1:42:09,  3.87s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We consider a simple and overarching representation for permutation-invariant functions of sequences (or set functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with k-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average . this allows us to leverage the rich and mature literature on permutations . we consider three kinds of approximations: canonical orderings of sequences, functions with k-order interactions, and stochastic optimization algorithms .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  21%|██        | 409/1992 [26:24<1:43:02,  3.91s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"As the size and complexity of models and datasets grow, so does the need for communication-efficient variants of stochastic gradient descent that can be deployed on clusters to perform model fitting in parallel. Alistarh et al. (2017) describe two variants of data-parallel SGD that quantize and encode gradients to lessen communication costs. For the first variant, QSGD, they provide strong theoretical guarantees. For the second variant, which we call QSGDinf, they demonstrate impressive empirical gains for distributed training of large neural networks. Building on their work, we propose an alternative scheme for quantizing gradients and show that it yields stronger theoretical guarantees than exist for QSGD while matching the empirical performance of QSGDinf.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Alistarh et al. describe two variants of data-parallel SGD that quantize and encode gradients to lessen communication costs . for the first variant, QSGD, they provide strong theoretical guarantees . based on their work, we propose an alternative scheme for quantizing gradients .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  21%|██        | 410/1992 [26:27<1:40:28,  3.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Real-world Question Answering (QA) tasks consist of thousands of words that often represent many facts and entities. Existing models based on LSTMs require a large number of parameters to support external memory and do not generalize well for long sequence inputs. Memory networks attempt to address these limitations by storing information to an external memory module but must examine all inputs in the memory. Hence, for longer sequence inputs the intermediate memory components proportionally scale in size resulting in poor inference times and high computation costs.\n\n In this paper, we present Adaptive Memory Networks (AMN) that process input question pairs to dynamically construct a network architecture optimized for lower inference times. During inference, AMN parses input text into entities within different memory slots. However, distinct from previous approaches, AMN is a dynamic network architecture that creates variable numbers of memory banks weighted by question relevance. Thus, the decoder can select a variable number of memory banks to construct an answer using fewer banks, creating a\n[{'summary_text': 'existing models based on LSTMs require a large number of parameters to support external memory and do not generalize well for long sequence inputs . memory networks attempt to address these limitations by storing information to an external memory module but must examine all inputs in the memory . resulting in poor inference times and high computation costs .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  21%|██        | 411/1992 [26:31<1:41:56,  3.87s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"While tasks could come with varying the number of instances and classes in realistic settings, the existing meta-learning approaches for few-shot classification assume that number of instances per task and class is fixed. Due to such restriction, they learn to equally utilize the meta-knowledge across all the tasks, even when the number of instances per task and class largely varies. Moreover, they do not consider distributional difference in unseen tasks, on which the meta-knowledge may have less usefulness depending on the task relatedness. To overcome these limitations, we propose a novel meta-learning model that adaptively balances the effect of the meta-learning and task-specific learning within each task. Through the learning of the balancing variables, we can decide whether to obtain a solution by relying on the meta-knowledge or task-specific learning. We formulate this objective into a Bayesian inference framework and tackle it using variational inference. We validate our Bayesian Task-Adaptive Meta-Learning (Bayesian TAML) on\n[{'summary_text': 'the existing meta-learning approaches for few-shot classification assume that number of instances per task and class is fixed . due to such restriction, they learn to equally utilize the meta-knowledge across all the tasks . they do not consider distributional difference in unseen tasks, on which the meta .knowledge may have less usefulness depending on the task relatedness . we formulate this objective into a Bayesian inference framework .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  21%|██        | 412/1992 [26:36<1:45:13,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recurrent Neural Networks have long been the dominating choice for sequence modeling. However, it severely suffers from two issues: impotent in capturing very long-term dependencies and unable to parallelize the sequential computation procedure. Therefore, many non-recurrent sequence models that are built on convolution and attention operations have been proposed recently. Notably, models with multi-head attention such as Transformer have demonstrated extreme effectiveness in capturing long-term dependencies in a variety of sequence modeling tasks. Despite their success, however, these models lack necessary components to model local structures in sequences and heavily rely on position embeddings that have limited effects and require a considerable amount of design efforts. In this paper, we propose the R-Transformer which enjoys the advantages of both RNNs and the multi-head attention mechanism while avoids their respective drawbacks. The proposed model can effectively capture both local structures and global long-term dependencies in sequences without any use of position embeddings. We evaluate R-Transformer through extensive\n[{'summary_text': 'models with multi-head attention such as Transformer have demonstrated extreme effectiveness in capturing long-term dependencies in a variety of sequence modeling tasks . despite their success, these models lack necessary components to model local structures in sequences and heavily rely on position embeddings that have limited effects .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  21%|██        | 413/1992 [26:40<1:44:18,  3.96s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper, we study the problem of optimizing a two-layer artificial neural network that best fits a training dataset. We look at this problem in the setting where the number of parameters is greater than the number of sampled points. We show that for a wide class of differentiable activation functions (this class involves most nonlinear functions and excludes piecewise linear functions), we have that arbitrary first-order optimal solutions satisfy global optimality provided the hidden layer is non-singular. We essentially show that these non-singular hidden layer matrix satisfy a ``\"good\" property for these big class of activation functions. Techniques involved in proving this result inspire us to look at a new algorithmic, where in between two gradient step of hidden layer, we add a stochastic gradient descent (SGD) step of the output layer. In this new algorithmic framework, we extend our earlier result and show that for all finite iterations the hidden layer satisfies the``good\" property mentioned earlier therefore partially explaining\n[{'summary_text': 'in this paper, we study the problem of optimizing a two-layer artificial neural network that best fits a training dataset . we show that for a wide class of differentiable activation functions we have that arbitrary first-order optimal solutions satisfy global optimality provided the hidden layer is non-singular .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  21%|██        | 414/1992 [26:44<1:44:36,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Machine learning (ML) models trained by differentially private stochastic gradient descent (DP-SGD) have much lower utility than the non-private ones. To mitigate this degradation, we propose a DP Laplacian smoothing SGD (DP-LSSGD) to train ML models with differential privacy (DP) guarantees. At the core of DP-LSSGD is the Laplacian smoothing, which smooths out the Gaussian noise used in the Gaussian mechanism. Under the same amount of noise used in the Gaussian mechanism, DP-LSSGD attains the same DP guarantee, but a better utility especially for the scenarios with strong DP guarantees. In practice, DP-LSSGD makes training both convex and nonconvex ML models more stable and enables the trained models to generalize better. The proposed algorithm is simple to implement and the extra computational complexity and memory overhead compared with DP-SGD are negligible. DP-LSS\n[{'summary_text': 'DP-LSSMachine learning (ML) models trained by differentially private stochastic gradient descent (DP-SGD) have much lower utility than the non-private ones . to mitigate this degradation, we propose a DP Laplacian smoothing SGD to train ML models with differential privacy (DP) guarantees.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  21%|██        | 415/1992 [26:48<1:44:29,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The ability to transfer knowledge to novel environments and tasks is a sensible desiderata for general learning agents. Despite the apparent promises, transfer in RL is still an open and little exploited research area. In this paper, we take a brand-new perspective about transfer: we suggest that the ability to assign credit unveils structural invariants in the tasks that can be transferred to make RL more sample efficient. Our main contribution is Secret, a novel approach to transfer learning for RL that uses a backward-view credit assignment mechanism based on a self-attentive architecture. Two aspects are key to its generality: it learns to assign credit as a separate offline supervised process and exclusively modifies the reward function. Consequently, it can be supplemented by transfer methods that do not modify the reward function and it can be plugged on top of any RL algorithm.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'despite the apparent promises, transfer in RL is still an open and little exploited research area . in this paper, we suggest that the ability to assign credit unveils structural invariants in the tasks that can be transferred to make RL more sample efficient .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  21%|██        | 416/1992 [26:51<1:40:16,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Validation is a key challenge in the search for safe autonomy. Simulations are often either too simple to provide robust validation, or too complex to tractably compute. Therefore, approximate validation methods are needed to tractably find failures without unsafe simplifications. This paper presents the theory behind one such black-box approach: adaptive stress testing (AST). We also provide three examples of validation problems formulated to work with AST.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'simulations are often either too simple to provide robust validation, or too complex to tractably compute . AST provides three examples of validation problems formulated to work with .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  21%|██        | 417/1992 [26:54<1:36:09,  3.66s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Spoken term detection (STD) is the task of determining whether and where a given word or phrase appears in a given segment of speech. Algorithms for STD are often aimed at maximizing the gap between the scores of positive and negative examples. As such they are focused on ensuring that utterances where the term appears are ranked higher than utterances where the term does not appear. However, they do not determine a detection threshold between the two. In this paper, we propose a new approach for setting an absolute detection threshold for all terms by introducing a new calibrated loss function. The advantage of minimizing this loss function during training is that it aims at maximizing not only the relative ranking scores, but also adjusts the system to use a fixed threshold and thus enhances system robustness and maximizes the detection accuracy rates. We use the new loss function in the structured prediction setting and extend the discriminative keyword spotting algorithm for learning the spoken term detector with a single threshold for all terms. We further demonstrate the effectiveness\n[{'summary_text': 'algorithms for STD are often aimed at maximizing the gap between the scores of positive and negative examples . as such they are focused on ensuring that utterances where the term appears are ranked higher than the term does not appear . however, they do not determine a detection threshold between the two .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  21%|██        | 418/1992 [26:58<1:38:54,  3.77s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present Value Propagation (VProp), a parameter-efficient differentiable planning module built on Value Iteration which can successfully be trained in a reinforcement learning fashion to solve unseen tasks, has the capability to generalize to larger map sizes, and can learn to navigate in dynamic environments. We evaluate on configurations of MazeBase grid-worlds, with randomly generated environments of several different sizes. Furthermore, we show that the module enables to learn to plan when the environment also includes stochastic elements, providing a cost-efficient learning system to build low-level size-invariant planners for a variety of interactive navigation problems.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'value Propagation (VProp) is a parameter-efficient differentiable planning module built on Value Iteration . it can be trained in a reinforcement learning fashion to solve unseen tasks . the module enables to learn to plan when the environment also includes stochastic elements, providing a cost-efficient learning system .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  21%|██        | 419/1992 [27:02<1:37:57,  3.74s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Graph networks have recently attracted considerable interest, and in particular in the context of semi-supervised learning. These methods typically work by generating node representations that are propagated throughout a given weighted graph.\n\n Here we argue that for semi-supervised learning, it is more natural to consider propagating labels in the graph instead. Towards this end, we propose a differentiable neural version of the classic Label Propagation (LP) algorithm. This formulation can be used for learning edge weights, unlike other methods where weights are set heuristically. Starting from a layer implementing a single iteration of LP, we proceed by adding several important non-linear steps that significantly enhance the label-propagating mechanism.\n\n Experiments in two distinct settings demonstrate the utility of our approach.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Graph networks have recently attracted considerable interest in semi-supervised learning . these methods typically work by generating node representations that are propagated throughout a given weighted graph . Towards this end, we propose a differentiable neural version of the classic Label Propagation (LP) algorithm . this formulation can be used for learning edge weights, unlike other methods where weights are set heuristically .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  21%|██        | 420/1992 [27:06<1:39:41,  3.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generative Adversarial Networks (GANs) are one of the most popular tools for learning complex high dimensional distributions. However, generalization properties of GANs have not been well understood. In this paper, we analyze the generalization of GANs in practical settings. We show that discriminators trained on discrete datasets with the original GAN loss have poor generalization capability and do not approximate the theoretically optimal discriminator. We propose a zero-centered gradient penalty for improving the generalization of the discriminator by pushing it toward the optimal discriminator. The penalty guarantees the generalization and convergence of GANs. Experiments on synthetic and large scale datasets verify our theoretical analysis.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we show that discriminators trained on discrete datasets with the original GAN loss have poor generalization capability . we propose a zero-centered gradient penalty for improving the generalization of the discriminator by pushing it toward the optimal discriminator .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  21%|██        | 421/1992 [27:10<1:39:14,  3.79s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Relational reasoning, the ability to model interactions and relations between objects, is valuable for robust multi-object tracking and pivotal for trajectory prediction. In this paper, we propose MOHART, a class-agnostic, end-to-end multi-object tracking and trajectory prediction algorithm, which explicitly accounts for permutation invariance in its relational reasoning. We explore a number of permutation invariant architectures and show that multi-headed self-attention outperforms the provided baselines and better accounts for complex physical interactions in a challenging toy experiment. We show on three real-world tracking datasets that adding relational reasoning capabilities in this way increases the tracking and trajectory prediction performance, particularly in the presence of ego-motion, occlusions, crowded scenes, and faulty sensor inputs. To the best of our knowledge, MOHART is the first fully end-to-end multi-object tracking from vision approach applied to real-world data reported in the literature.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this paper, we propose MOHART, a class-agnostic, end-to-end multi-object tracking and trajectory prediction algorithm, which explicitly accounts for permutation invariance in its relational reasoning . we show that multi-headed self-attention outperforms the provided baselines and better accounts for complex physical interactions in a challenging toy experiment .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  21%|██        | 422/1992 [27:14<1:42:10,  3.90s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"This paper introduces an information theoretic co-training objective for unsupervised learning.  We consider the problem of predicting the future . Rather than predict future sensations (image pixels or sound waves) we predict ``hypotheses'' to be confirmed by future sensations . More formally, we assume a population distribution on pairs $(x,y)$ where we can think of $x$ as a past sensation and $y$ as a future sensation . We train both a predictor model $P_\\Phi(z|x)$ and a confirmation model $P_\\Psi(z|y)$ where we view $z$ as hypotheses (when predicted) or facts (when confirmed ). For a population distribution on pairs $(x,y)$ we focus on the problem of measuring the mutual information between $x$ and $ y$. By the data processing inequality this mutual information is at least as large as the mutual information between $x$ and $z$ under the distribution\n[{'summary_text': \"this paper introduces an information theoretic co-training objective for unsupervised learning. We consider the problem of predicting the future . Rather than predict future sensations (image pixels or sound waves) we predict hypotheses'' to be confirmed by future sensation . more formally, we assume a population distribution on pairs $(x,y)$ where we can think of $x$ as a past sensation and $y$ as an future sensation.\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  21%|██        | 423/1992 [27:18<1:46:13,  4.06s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Off-policy learning, the task of evaluating and improving policies using historic data collected from a logging policy, is important because on-policy evaluation is usually expensive and has adverse impacts. One of the major challenge of off-policy learning is to derive counterfactual estimators that also has low variance and thus low generalization error. \n In this work, inspired by learning bounds for importance sampling problems, we present a new counterfactual learning principle for off-policy learning with bandit feedbacks.Our method regularizes the generalization error by minimizing the distribution divergence between the logging policy and the new policy, and removes the need for iterating through all training samples to compute sample variance regularization in prior work. With neural network policies, our end-to-end training algorithms using variational divergence minimization showed significant improvement over conventional baseline algorithms and is also consistent with our theoretical results.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'on-policy evaluation is usually expensive and has adverse impacts . one of the major challenge of off-police learning is to derive counterfactual estimators that also has low variance and thus low generalization error . with neural network policies, our end-to-end training algorithms using variational divergence minimization showed significant improvement .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  21%|██▏       | 424/1992 [27:22<1:43:10,  3.95s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The high-quality node embeddings learned from the Graph Neural Networks (GNNs) have been applied to a wide range of node-based applications and some of them have achieved state-of-the-art (SOTA) performance. However, when applying node embeddings learned from GNNs to generate graph embeddings, the scalar node representation may not suffice to preserve the node/graph properties efficiently, resulting in sub-optimal graph embeddings.\n\n Inspired by the Capsule Neural Network (CapsNet), we propose the Capsule Graph Neural Network (CapsGNN), which adopts the concept of capsules to address the weakness in existing GNN-based graph embeddings algorithms. By extracting node features in the form of capsules, routing mechanism can be utilized to capture important information at the graph level. As a result, our model generates multiple embeddings for each graph to capture graph properties from different aspects. The attention module incorporated in Caps\n[{'summary_text': 'the high-quality node embeddings learned from the Graph Neural Networks (GNNs) have been applied to a wide range of node-based applications . some of them have achieved state-of-the-art (SOTA) performance . however, when applying node embeddeds learned . from GNNs, the scalar node representation may not suffice to preserve the . node/graph properties efficiently, resulting in sub-optimal graph embedderings .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  21%|██▏       | 425/1992 [27:27<1:48:25,  4.15s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Batch Normalization (BN) has become a cornerstone of deep learning across diverse architectures, appearing to help optimization as well as generalization. While the idea makes intuitive sense, theoretical analysis of its effectiveness has been lacking. Here theoretical support is provided for one of its conjectured properties, namely, the ability to allow gradient descent to succeed with less tuning of learning rates. It is shown that even if we fix the learning rate of scale-invariant parameters (e.g., weights of each layer with BN) to a constant (say, 0.3), gradient descent still approaches a stationary point (i.e., a solution where gradient is zero) in the rate of T^{−1/2} in T iterations, asymptotically matching the best bound for gradient descent with well-tuned learning rates. A similar result with convergence rate T^{−1/4} is also shown for stochastic gradient descent.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Batch Normalization (BN) has become a cornerstone of deep learning across diverse architectures . it is shown that even if we fix learning rate of scale-invariant parameters (e.g., weights of each layer with BN) to a constant (say, 0.3), gradient descent still approaches a stationary point . a similar result with convergence rate T1/4 is also shown for stochastic gradient descent .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  21%|██▏       | 426/1992 [27:31<1:47:42,  4.13s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Watermarks have been used for various purposes. Recently, researchers started to look into using them for deep neural networks. Some works try to hide attack triggers on their adversarial samples when attacking neural networks and others want to watermark neural networks to prove their ownership against plagiarism. Implanting a backdoor watermark module into a neural network is getting more attention from the community. In this paper, we present a general purpose encoder-decoder joint training method, inspired by generative adversarial networks (GANs). Unlike GANs, however, our encoder and decoder neural networks cooperate to find the best watermarking scheme given data samples. In other words, we do not design any new watermarking strategy but our proposed two neural networks will find the best suited method on their own. After being trained, the decoder can be implanted into other neural networks to attack or protect them (see Appendix for their use cases and real implementations). To this end, the decoder should\n[{'summary_text': 'researchers started to look into using them for deep neural networks . some works try to hide attack triggers on their adversarial samples . others want to watermark neural networks to prove ownership against plagiarism .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  21%|██▏       | 427/1992 [27:34<1:44:16,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We study the problem of learning similarity functions over very large corpora using neural network embedding models. These models are typically trained using SGD with random sampling of unobserved pairs, with a sample size that grows quadratically with the corpus size, making it expensive to scale.\n We propose new efficient methods to train these models without having to sample unobserved pairs. Inspired by matrix factorization, our approach relies on adding a global quadratic penalty and expressing this term as the inner-product of two generalized Gramians. We show that the gradient of this term can be efficiently computed by maintaining estimates of the Gramians, and develop variance reduction schemes to improve the quality of the estimates. We conduct large-scale experiments that show a significant improvement both in training time and generalization performance compared to sampling methods.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'these models are typically trained using SGD with random sampling of unobserved pairs, with a sample size that grows quadratically with the corpus size, making it expensive to scale . inspired by matrix factorization, our approach relies on adding a global quadratic penalty and expressing this term as the inner-product of two generalized Gramians .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  21%|██▏       | 428/1992 [27:38<1:41:47,  3.90s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Modelling statistical relationships beyond the conditional mean is crucial in many settings. Conditional density estimation (CDE) aims to learn the full conditional probability density from data. Though highly expressive, neural network based CDE models can suffer from severe over-fitting when trained with the maximum likelihood objective. Due to the inherent structure of such models, classical regularization approaches in the parameter space are rendered ineffective. To address this issue, we develop a model-agnostic noise regularization method for CDE that adds random perturbations to the data during training. We demonstrate that the proposed approach corresponds to a smoothness regularization and prove its asymptotic consistency. In our experiments, noise regularization significantly and consistently outperforms other regularization methods across seven data sets and three CDE models. The effectiveness of noise regularization makes neural network based CDE the preferable method over previous non- and semi-parametric approaches, even when training data is scarce.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'conditional density estimation aims to learn the full conditional probability density from data . although highly expressive, neural network based CDE models can suffer from severe over-fitting when trained with the maximum likelihood objective . due to the inherent structure of such models, classical regularization approaches in the parameter space are rendered ineffective .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  22%|██▏       | 429/1992 [27:42<1:39:45,  3.83s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Parallel developments in neuroscience and deep learning have led to mutually productive exchanges, pushing our understanding of real and artificial neural networks in sensory and cognitive systems. However, this interaction between fields is less developed in the study of motor control. In this work, we develop a virtual rodent as a platform for the grounded study of motor activity in artificial models of embodied control. We then use this platform to study motor activity across contexts by training a model to solve four complex tasks. Using methods familiar to neuroscientists, we describe the behavioral representations and algorithms employed by different layers of the network using a neuroethological approach to characterize motor activity relative to the rodent's behavior and goals. We find that the model uses two classes of representations which respectively encode the task-specific behavioral strategies and task-invariant behavioral kinematics. These representations are reflected in the sequential activity and population dynamics of neural subpopulations. Overall, the virtual rodent facilitates grounded collaborations between deep reinforcement learning and motor neuroscience.<|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this work, we develop a virtual rodent as a platform for the grounded study of motor activity in artificial models of embodied control . we then use this platform to study motor activity across contexts by training a model to solve four complex tasks . model uses two classes of representations which encode task-specific behavioral strategies and task-invariant behavioral kinematics .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  22%|██▏       | 430/1992 [27:46<1:40:12,  3.85s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present local ensembles, a method for detecting extrapolation at test time in a pre-trained model. We focus on underdetermination as a key component of extrapolation: we aim to detect when many possible predictions are consistent with the training data and model class. Our method uses local second-order information to approximate the variance of predictions across an ensemble of models from the same class. We compute this approximation by estimating the norm of the component of a test point's gradient that aligns with the low-curvature directions of the Hessian, and provide a tractable method for estimating this quantity. Experimentally, we show that our method is capable of detecting when a pre-trained model is extrapolating on test data, with applications to out-of-distribution detection, detecting spurious correlates, and active learning.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': \"local ensembles, a method for detecting extrapolation at test time in a pre-trained model, uses local second-order information to approximate the variance of predictions across an ensemble of models from the same class . we compute this approximation by estimating the norm of the component of a test point's gradient that aligns with the low-curvature directions of the Hessian .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  22%|██▏       | 431/1992 [27:50<1:40:12,  3.85s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"This paper focuses on the synthetic generation of human mobility data in urban areas. We present a novel and scalable application of Generative Adversarial Networks (GANs) for modeling and generating human mobility data. We leverage actual ride requests from ride sharing/hailing services from four major cities in the US to train our GANs model. Our model captures the spatial and temporal variability of the ride-request patterns observed for all four cities on any typical day and over any typical week. Previous works have succinctly characterized the spatial and temporal properties of human mobility data sets using the fractal dimensionality and the densification power law, respectively, which we utilize to validate our GANs-generated synthetic data sets. Such synthetic data sets can avoid privacy concerns and be extremely useful for researchers and policy makers on urban mobility and intelligent transportation.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'this paper focuses on the synthetic generation of human mobility data in urban areas . we leverage actual ride requests from ride sharing/hailing services to train our GANs model . our model captures the spatial and temporal variability of the ride-request patterns observed for all four cities on any typical day .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  22%|██▏       | 432/1992 [27:53<1:38:22,  3.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code is publicly available.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'attention can completely replace convolution and achieve state-of-the-art performance on vision tasks . this raises the question: do learned attention layers operate similarly to convolutional layers? this work provides evidence that attention layers can perform convolution . Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  22%|██▏       | 433/1992 [27:57<1:39:11,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generative models with both discrete and continuous latent variables are highly motivated by the structure of many real-world data sets. They present, however, subtleties in training often manifesting in the discrete latent variable not being leveraged. In this paper, we show why such models struggle to train using traditional log-likelihood maximization, and that they are amenable to training using the Optimal Transport framework of Wasserstein Autoencoders. We find our discrete latent variable to be fully leveraged by the model when trained, without any modifications to the objective function or significant fine tuning. Our model generates comparable samples to other approaches while using relatively simple neural networks, since the discrete latent variable carries much of the descriptive burden. Furthermore, the discrete latent provides significant control over generation.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in this paper, we show why such models struggle to train using traditional log-likelihood maximization . we find our discrete latent variable to be fully leveraged by the model when trained, without any modifications to the objective function or significant fine tuning .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  22%|██▏       | 434/1992 [28:01<1:36:30,  3.72s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Lexical ambiguity, i.e., the presence of two or more meanings for a single word, is an inherent and challenging problem for machine translation systems. Even though the use of recurrent neural networks and attention mechanisms are expected to solve this problem, machine translation systems are not always able to correctly translate lexically ambiguous sentences. In this work, I attempt to resolve the problem of lexical ambiguity in English--Japanese neural machine translation systems by combining a pretrained Bidirectional Encoder Representations from Transformer (BERT) language model that can produce contextualized word embeddings and a Transformer translation model, which is a state-of-the-art architecture for the machine translation task. These two proposed architectures have been shown to be more effective in translating ambiguous sentences than a vanilla Transformer model and the Google Translate system. Furthermore, one of the proposed models, the Transformer_BERT-WE, achieves a higher BLEU score compared to the vanilla Transformer model in\n[{'summary_text': 'two proposed architectures have been shown to be more effective in translating ambiguous sentences than a vanilla Transformer model and the Google Translate system . one of the proposed models, the Transformer_BERT-WE, achieves a higher BLEU score compared to the vanilla transformer model inLexical ambiguity .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  22%|██▏       | 435/1992 [28:05<1:39:12,  3.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Reinforcement learning algorithms, though successful, tend to over-fit to training environments, thereby hampering their application to the real-world. This paper proposes $\\text{W}\\text{R}^{2}\\text{L}$ -- a robust reinforcement learning algorithm with significant robust performance on low and high-dimensional control tasks. Our method formalises robust reinforcement learning as a novel min-max game with a Wasserstein constraint for a correct and convergent solver. Apart from the formulation, we also propose an efficient and scalable solver following a novel zero-order optimisation method that we believe can be useful to numerical optimisation in general. \n We empirically demonstrate significant gains compared to standard and robust state-of-the-art algorithms on high-dimensional MuJuCo environments<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'our method formalises robust reinforcement learning as a novel min-max game with a Wasserstein constraint for a correct and convergent solver . we also propose an efficient and scalable solver following an novel zero-order optimisation method that we believe can be useful to numerical optimisation .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  22%|██▏       | 436/1992 [28:08<1:37:22,  3.76s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In order to alleviate the notorious mode collapse phenomenon in generative adversarial networks (GANs), we propose a novel training method of GANs in which certain fake samples can be reconsidered as real ones during the training process. This strategy can reduce the gradient value that generator receives in the region where gradient exploding happens. We show that the theoretical equilibrium between the generators and discriminations actually can be seldom realized in practice. And this results in an unbalanced generated distribution that deviates from the target one, when fake datepoints overfit to real ones, which explains the non-stability of GANs. We also prove that, by penalizing the difference between discriminator outputs and considering certain fake datapoints as real for adjacent real and fake sample pairs, gradient exploding can be alleviated. Accordingly, a modified GAN training method is proposed with a more stable training process and a better generalization. Experiments on different datasets verify our theoretical analysis.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in order to alleviate the notorious mode collapse phenomenon in generative adversarial networks (GANs), we propose a novel training method of GANs in which certain fake samples can be reconsidered as real ones . this strategy can reduce the gradient value that generator receives in the region where gradient exploding happens . we show that the theoretical equilibrium between the generators and discriminations actually can be seldom realized in practice .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  22%|██▏       | 437/1992 [28:12<1:38:46,  3.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The ability to learn new concepts with small amounts of data is a critical aspect of intelligence that has proven challenging for deep learning methods. Meta-learning has emerged as a promising technique for leveraging data from previous tasks to enable efficient learning of new tasks. However, most meta-learning algorithms implicitly require that the meta-training tasks be mutually-exclusive, such that no single model can solve all of the tasks at once. For example, when creating tasks for few-shot image classification, prior work uses a per-task random assignment of image classes to N-way classification labels. If this is not done, the meta-learner can ignore the task training data and learn a single model that performs all of the meta-training tasks zero-shot, but does not adapt effectively to new image classes.   This requirement means that the user must take great care in designing the tasks, for example by shuffling labels or removing task identifying information from the inputs. In some domains, this makes meta-\n[{'summary_text': 'meta-learning has emerged as a promising technique for leveraging data from previous tasks to enable efficient learning of new tasks . however, most algorithms implicitly require that the meta-training tasks be mutually-exclusive, such that no single model can solve all of the tasks at once . this requirement means that the user must take great care in designing the tasks, for example by shuffling labels .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  22%|██▏       | 438/1992 [28:16<1:41:26,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We extend the Consensus Network framework to Transductive Consensus Network (TCN), a semi-supervised multi-modal classification framework, and identify its two mechanisms: consensus and classification. By putting forward three variants as ablation studies, we show both mechanisms should be functioning together. Overall, TCNs outperform or align with the best benchmark algorithms when only 20 to 200 labeled data points are available.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'putting forward three variants as ablation studies, we show both mechanisms should be functioning together . overall, TCNs outperform or align with best benchmark algorithms when only 20 to 200 labeled data points are available .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  22%|██▏       | 439/1992 [28:20<1:36:15,  3.72s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this work, we first conduct mathematical analysis on the memory, which is\n defined as a function that maps an element in a sequence to the current output,\n of three RNN cells; namely, the simple recurrent neural network (SRN), the long\n short-term memory (LSTM) and the gated recurrent unit (GRU). Based on the\n analysis, we propose a new design, called the extended-long short-term memory\n (ELSTM), to extend the memory length of a cell. Next, we present a multi-task\n RNN model that is robust to previous erroneous predictions, called the dependent\n bidirectional recurrent neural network (DBRNN), for the sequence-in-sequenceout\n (SISO) problem. Finally, the performance of the DBRNN model with the\n ELSTM cell is demonstrated by experimental results.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we first conduct mathematical analysis on the memory, which is defined as a function that maps an element in a sequence to the current output, of three RNN cells . based on the analysis, we propose a new design, called the extended-long short-term memory (ELSTM), to extend the memory length of a cell . the performance of the DBRNN model with the ELSTM cell is demonstrated by experimental results .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  22%|██▏       | 440/1992 [28:24<1:38:10,  3.80s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The goal of generative models is to model the underlying data distribution of a\n sample based dataset. Our intuition is that an accurate model should in principle\n also include the sample based dataset as part of its induced probability distribution.\n To investigate this, we look at fully trained generative models using the Generative\n Adversarial Networks (GAN) framework and analyze the resulting generator\n on its ability to memorize the dataset. Further, we show that the size of the initial\n latent space is paramount to allow for an accurate reconstruction of the training\n data. This gives us a link to compression theory, where Autoencoders (AE) are\n used to lower bound the reconstruction capabilities of our generative model. Here,\n we observe similar results to the perception-distortion tradeoff (Blau & Michaeli\n (2018)). Given a small latent space, the AE produces low quality and the GAN\n produces high quality outputs from a perceptual viewpoint. In contrast, the distortion\n[{'summary_text': 'our intuition is that an accurate model should in principle also include the sample based dataset as part of its induced probability distribution . to investigate this, we look at fully trained generative models using the Generative Adversarial Networks framework and analyze the resulting generator on its ability to memorize the dataset .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  22%|██▏       | 441/1992 [28:28<1:39:51,  3.86s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Saliency maps are often used to suggest explanations of the behavior of deep rein- forcement learning (RL) agents. However, the explanations derived from saliency maps are often unfalsifiable and can be highly subjective. We introduce an empirical approach grounded in counterfactual reasoning to test the hypotheses generated from saliency maps and show that explanations suggested by saliency maps are often not supported by experiments. Our experiments suggest that saliency maps are best viewed as an exploratory tool rather than an explanatory tool.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the explanations derived from saliency maps are often unfalsifiable and can be highly subjective . we introduce an empirical approach grounded in counterfactual reasoning to test the hypotheses generated from the maps . our experiments suggest that saliencies maps are best viewed as an exploratory tool rather than an explanatory tool .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  22%|██▏       | 442/1992 [28:31<1:38:10,  3.80s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a considerable amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires considerable human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and backward policy, with the backward policy resetting the environment for a subsequent attempt. By learning a value function for the backward policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the backward policy can greatly reduce the number of manual resets required to learn a task and can reduce the number of unsafe actions that lead to non-reversible states.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt . however, not all tasks are easily or automatically reversible . this learning process requires considerable human intervention . in this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and backward policy .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  22%|██▏       | 443/1992 [28:35<1:38:22,  3.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose the Information Maximization Autoencoder (IMAE), an information theoretic approach to simultaneously learn continuous and discrete representations in an unsupervised setting. Unlike the Variational Autoencoder framework, IMAE starts from a stochastic encoder that seeks to map each input data to a hybrid discrete and continuous representation with the objective of maximizing the mutual information between the data and their representations. A decoder is included to approximate the posterior distribution of the data given their representations, where a high fidelity approximation can be achieved by leveraging the informative representations.     \n We show that the proposed objective is theoretically valid and provides a principled framework for understanding the tradeoffs regarding informativeness of each representation factor, disentanglement of representations, and decoding quality.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'IMAE starts from a stochastic encoder that seeks to map each input data to a hybrid discrete and continuous representation . a decoder is included to approximate the posterior distribution of the data given their representations, where a high fidelity approximation can be achieved by leveraging the informative representations .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  22%|██▏       | 444/1992 [28:39<1:38:14,  3.81s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep neural networks have achieved impressive performance in handling complicated semantics in natural language, while mostly treated as black boxes. To explain how the model handles compositional semantics of words and phrases, we study the hierarchical explanation problem. We highlight the key challenge is to compute non-additive and context-independent importance for individual words and phrases. We show some prior efforts on hierarchical explanations, e.g. contextual decomposition,  do not satisfy the desired properties mathematically, leading to inconsistent explanation quality in different models. In this paper, we propose a formal way to quantify the importance of each word or phrase to generate hierarchical explanations. We modify contextual decomposition algorithms according to our formulation, and propose a model-agnostic explanation algorithm with competitive performance. Human evaluation and automatic metrics evaluation on both LSTM models and fine-tuned BERT Transformer models on multiple datasets show that our algorithms robustly outperform prior works on hierarchical explanations. We show our algorithms help explain compositionality of semantics, extract\n[{'summary_text': 'deep neural networks have achieved impressive performance in handling complicated semantics in natural language, while mostly treated as black boxes . to explain how the model handles compositional semantics, we study the hierarchical explanation problem . we highlight the key challenge is to compute non-additive and context-independent importance for individual words and phrases .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  22%|██▏       | 445/1992 [28:43<1:40:14,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"There are myriad kinds of segmentation, and ultimately the `\"right\" segmentation of a given scene is in the eye of the annotator. Standard approaches require large amounts of labeled data to learn just one particular kind of segmentation. As a first step towards relieving this annotation burden, we propose the problem of guided segmentation: given varying amounts of pixel-wise labels, segment unannotated pixels by propagating supervision locally (within an image) and non-locally (across images). We propose guided networks, which extract a latent task representation---guidance---from variable amounts and classes (categories, instances, etc.) of pixel supervision and optimize our architecture end-to-end for fast, accurate, and data-efficient segmentation by meta-learning. To span the few-shot and many-shot learning regimes, we examine guidance from as little as one pixel per concept to as much as 1000+ images, and compare to full gradient optimization at both extremes\n[{'summary_text': 'standard approaches require large amounts of labeled data to learn just one particular kind of segmentation . we propose guided networks, which extract a latent task representation---guidance---from variable amounts and classes (categories, instances, etc.) of pixel supervision and optimize our architecture end-to-end for fast, accurate, and data-efficient segmentation by meta-learning .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  22%|██▏       | 446/1992 [28:47<1:42:17,  3.97s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Autonomous driving is still considered as an “unsolved problem” given its inherent important variability and that many processes associated with its development like vehicle control and scenes recognition remain open issues. Despite reinforcement learning algorithms have achieved notable results in games and some robotic manipulations, this technique has not been widely scaled up to the more challenging real world applications like autonomous driving. In this work, we propose a deep reinforcement learning (RL) algorithm embedding an actor critic architecture with multi-step returns to achieve a better robustness of the agent learning strategies when acting in complex and unstable environments. The experiment is conducted with Carla simulator offering a customizable and realistic urban driving conditions. The developed deep actor RL guided by a policy-evaluator critic distinctly surpasses the performance of a standard deep RL agent.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'despite reinforcement learning algorithms have achieved notable results in games and some robotic manipulations, this technique has not been widely scaled up to the more challenging real world applications like autonomous driving . in this work, we propose a deep reinforcement learning (RL) algorithm embedding an actor critic architecture with multi-step returns .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  22%|██▏       | 447/1992 [28:51<1:39:30,  3.86s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Variational inference (VI) methods and especially variational autoencoders (VAEs) specify scalable generative models that enjoy an intuitive connection to manifold learning --- with many default priors the posterior/likelihood pair $q(z|x)$/$p(x|z)$ can be viewed as an approximate homeomorphism (and its inverse) between the data manifold and a latent Euclidean space. However, these approximations are well-documented to become degenerate in training. Unless the subjective prior is carefully chosen, the topologies of the prior and data distributions often will not match.\n Conversely, diffusion maps (DM) automatically \\textit{infer} the data topology and enjoy a rigorous connection to manifold learning, but do not scale easily or provide the inverse homeomorphism.\n In this paper, we propose \\textbf{a)} a principled measure for recognizing the mismatch between data and latent distributions and \\textbf{b)} a\n[{'summary_text': 'variationsal inference (VI) methods and especially variational autoencoders (VAEs) specify scalable generative models that enjoy an intuitive connection to manifold learning . these approximations are well-documented to become degenerate in training . unless subjective prior is carefully chosen, topologies of the prior and data distributions often will not match .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  22%|██▏       | 448/1992 [28:55<1:42:27,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recurrent neural networks (RNNs) are an effective representation of control policies for a wide range of reinforcement and imitation learning problems. RNN policies, however, are particularly difficult to explain, understand, and analyze due to their use of continuous-valued memory vectors and observation features. In this paper, we introduce a new technique, Quantized Bottleneck Insertion, to learn finite representations of these vectors and features. The result is a quantized representation of the RNN that can be analyzed to improve our understanding of memory use and general behavior. We present results of this approach on synthetic environments and six Atari games. The resulting finite representations are surprisingly small in some cases, using as few as 3 discrete memory states and 10 observations for a perfect Pong policy. We also show that these finite policy representations lead to improved interpretability.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the results are a quantized representation of the RNN that can be analyzed to improve our understanding of memory use and general behavior . the resulting finite representations are surprisingly small in some cases, using as few as 3 discrete memory states and 10 observations for a perfect Pong policy.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  23%|██▎       | 449/1992 [28:59<1:39:32,  3.87s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The problem of building a coherent and non-monotonous conversational agent with proper discourse and coverage is still an area of open research. Current architectures only take care of semantic and contextual information for a given query and fail to completely account for syntactic and external knowledge which are crucial for generating responses in a chit-chat system. To overcome this problem, we propose an end to end multi-stream deep learning architecture which learns unified embeddings for query-response pairs by leveraging contextual information from memory networks and syntactic information by incorporating Graph Convolution Networks (GCN) over their dependency parse. A stream of this network also utilizes transfer learning by pre-training a bidirectional transformer to extract semantic representation for each input sentence and incorporates external knowledge through the neighbourhood of the entities from a Knowledge Base (KB). We benchmark these embeddings on next sentence prediction task and significantly improve upon the existing techniques. Furthermore, we use AMUSED to represent query and responses along with its context to\n[{'summary_text': 'current architectures only take care of semantic and contextual information for a given query and fail to completely account for syntactic and external knowledge . to overcome this problem, we propose an end to end multi-stream deep learning architecture which learns unified embeddings for query-response pairs by leveraging contextual information from memory networks .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  23%|██▎       | 450/1992 [29:03<1:41:04,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We present a new latent model of natural images that can be learned on large-scale datasets. The learning process provides a latent embedding for every image in the training dataset, as well as a deep convolutional network that maps the latent space to the image space. After training, the new model provides a strong and universal image prior for a variety of image restoration tasks such as large-hole inpainting, superresolution, and colorization. To model high-resolution natural images, our approach uses latent spaces of very high dimensionality (one to two orders of magnitude higher than previous latent image models). To tackle this high dimensionality, we use latent spaces with a special manifold structure (convolutional manifolds) parameterized by a ConvNet of a certain architecture. In the experiments, we compare the learned latent models with latent models learned by autoencoders, advanced variants of generative adversarial networks, and a strong baseline system using simpler parameterization of the latent space. Our\n[{'summary_text': 'the learning process provides a latent embedding for every image in the training dataset, as well as a deep convolutional network that maps the latent space to the image space . to model high-resolution natural images, our approach uses latent spaces of very high dimensionality (one to two orders of magnitude higher than previous latent image models)'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  23%|██▎       | 451/1992 [29:07<1:42:37,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Variational Auto Encoders (VAE) are capable of generating realistic images, sounds and video sequences. From practitioners point of view, we are usually interested in solving problems where tasks are learned sequentially, in a way that avoids revisiting all previous data at each stage. We address this problem by introducing a conceptually simple and scalable end-to-end approach of incorporating past knowledge by learning prior directly from the data.   We consider scalable boosting-like approximation for intractable theoretical optimal prior. We provide empirical studies on two commonly used benchmarks, namely  MNIST and Fashion MNIST on disjoint sequential image generation tasks. For each dataset proposed method delivers the best results among comparable approaches,  avoiding catastrophic forgetting in a fully automatic way with a fixed model architecture.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Variational Auto Encoders (VAE) are capable of generating realistic images, sounds and video sequences . from practitioners point of view, we are usually interested in solving problems where tasks are learned sequentially, in a way that avoids revisiting all previous data at each stage .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  23%|██▎       | 452/1992 [29:10<1:39:24,  3.87s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Two important topics in deep learning both involve incorporating humans into the modeling process: Model priors transfer information from humans to a model by regularizing the model's parameters; Model attributions transfer information from a model to humans by explaining the model's behavior. Previous work has taken important steps to connect these topics through various forms of gradient regularization. We find, however, that existing methods that use attributions to align a model's behavior with human intuition are ineffective. We develop an efficient and theoretically grounded feature attribution method, expected gradients, and a novel framework, attribution priors, to enforce prior expectations about a model's behavior during training. We demonstrate that attribution priors are broadly applicable by instantiating them on three different types of data: image data, gene expression data, and health care data. Our experiments show that models trained with attribution priors are more intuitive and achieve better generalization performance than both equivalent baselines and existing methods to regularize model behavior.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': \"models trained with attribution priors are more intuitive and achieve better generalization performance than both equivalent baselines and existing methods to regularize model behavior . previous work has taken important steps to connect these topics through various forms of gradient regularization . we find that existing methods that use attributions to align a model's behavior with human intuition are ineffective .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  23%|██▎       | 453/1992 [29:14<1:38:43,  3.85s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recent neural network and language models have begun to rely on softmax distributions with an extremely large number of categories. In this context calculating the softmax normalizing constant is prohibitively expensive. This has spurred a growing literature of efficiently computable but biased estimates of the softmax. In this paper we present the first two unbiased algorithms for maximizing the softmax likelihood whose work per iteration is independent of the number of classes and datapoints (and does not require extra work at the end of each epoch). We compare our unbiased methods' empirical performance to the state-of-the-art on seven real world datasets, where they comprehensively outperform all competitors.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'recent neural network and language models have begun to rely on softmax distributions with an extremely large number of categories . in this context calculating the softmax normalizing constant is prohibitively expensive . this has spurred a growing literature of efficiently computable but biased estimates .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  23%|██▎       | 454/1992 [29:18<1:36:32,  3.77s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Backpropagation is driving today's artificial neural networks. However, despite extensive research, it remains unclear if the brain implements this algorithm. Among neuroscientists, reinforcement learning (RL) algorithms are often seen as a realistic alternative. However, the convergence rate of such learning scales poorly with the number of involved neurons. Here we propose a hybrid learning approach, in which each neuron uses an RL-type strategy to learn how to approximate the gradients that backpropagation would provide. We show that our approach learns to approximate the gradient, and can match the performance of gradient-based learning on fully connected and convolutional networks. Learning feedback weights provides a biologically plausible mechanism of achieving good performance, without the need for precise, pre-specified learning rules.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'reinforcement learning (RL) algorithms are often seen as a realistic alternative . but the convergence rate of such learning scales poorly with the number of involved neurons . here we propose a hybrid learning approach, in which each neuron uses an RL-type strategy to learn how to approximate the gradients that backpropagation would provide .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  23%|██▎       | 455/1992 [29:21<1:35:48,  3.74s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose a framework to understand the unprecedented performance and robustness of deep neural networks using field theory. Correlations between the weights within the same layer can be described by symmetries in that layer, and networks generalize better if such symmetries are broken to reduce the redundancies of the weights. Using a two parameter field theory, we find that the network can break such symmetries itself towards the end of training in a process commonly known in physics as spontaneous symmetry breaking. This corresponds to a network generalizing itself without any user input layers to break the symmetry, but by communication with adjacent layers. In the layer decoupling limit applicable to residual networks (He et al., 2015), we show that the remnant symmetries that survive the non-linear layers are spontaneously broken based on empirical results. The Lagrangian for the non-linear and weight layers together has striking similarities with the one in quantum field theory of a scalar. Using results from quantum field theory we show that\n[{'summary_text': 'the lagrangian for the non-linear and weight layers together has striking similarities with the one in quantum field theory of a scalar . we find that the network can break such symmetries itself towards the end of training . this corresponds to a network generalizing itself without any user input layers to break the symmetry, but by communication with adjacent layers .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  23%|██▎       | 456/1992 [29:26<1:39:06,  3.87s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Compressed forms of deep neural networks are essential in deploying large-scale\n computational models on resource-constrained devices. Contrary to analogous\n domains where large-scale systems are build as a hierarchical repetition of small-\n scale units, the current practice in Machine Learning largely relies on models with\n non-repetitive components. In the spirit of molecular composition with repeating\n atoms, we advance the state-of-the-art in model compression by proposing Atomic\n Compression Networks (ACNs), a novel architecture that is constructed by recursive\n repetition of a small set of neurons. In other words, the same neurons with the\n same weights are stochastically re-positioned in subsequent layers of the network.\n Empirical evidence suggests that ACNs achieve compression rates of up to three\n orders of magnitudes compared to fine-tuned fully-connected neural networks (88×\n to 1116× reduction) with only a fractional deterioration of classification accuracy\n (0\n[{'summary_text': 'the current practice in Machine Learning largely relies on models with non-repetitive components . in the spirit of molecular composition with repeating atoms, we advance the state-of-the-art in model compression by proposing Atomic Compression Networks (ACNs) the same neurons with the same weights are stochastically re-positioned in subsequent layers of the network .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  23%|██▎       | 457/1992 [29:30<1:42:06,  3.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Training neural networks with verifiable robustness guarantees is challenging. Several existing approaches utilize linear relaxation based neural network output bounds under perturbation, but they can slow down training by a factor of hundreds depending on the underlying network architectures. Meanwhile, interval bound propagation (IBP) based training is efficient and significantly outperforms linear relaxation based methods on many tasks, yet it may suffer from stability issues since the bounds are much looser especially at the beginning of training. In this paper, we propose a new certified adversarial training method, CROWN-IBP, by combining the fast IBP bounds in a forward bounding pass and a tight linear relaxation based bound, CROWN, in a backward bounding pass. CROWN-IBP is computationally efficient and consistently outperforms IBP baselines on training verifiably robust neural networks. We conduct large scale experiments on MNIST and CIFAR datasets, and outperform all previous linear relaxation and bound propagation based certified defenses in L_inf\n[{'summary_text': 'interval bound propagation based training is efficient and significantly outperforms linear relaxation based methods on many tasks . in this paper, we propose a new certified adversarial training method, CROWN-IBP, by combining the fast IBP bounds in a forward bounding pass . we conduct large scale experiments on MNIST and CIFAR datasets .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  23%|██▎       | 458/1992 [29:34<1:43:34,  4.05s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Neural networks could misclassify inputs that are slightly different from their training data, which indicates a small margin between their decision boundaries and the training dataset. In this work, we study the binary classification of linearly separable datasets and show that linear classifiers could also have decision boundaries that lie close to their training dataset if cross-entropy loss is used for training. In particular, we show that if the features of the training dataset lie in a low-dimensional affine subspace and the cross-entropy loss is minimized by using a gradient method, the margin between the training points and the decision boundary could be much smaller than the optimal value. This result is contrary to the conclusions of recent related works such as (Soudry et al., 2018), and we identify the reason for this contradiction. In order to improve the margin, we introduce differential training, which is a training paradigm that uses a loss function defined on pairs of points from each class. We show that the decision boundary\n[{'summary_text': 'the decision boundaryNeural networks could misclassify inputs that are slightly different from their training data, which indicates a small margin between their decision boundaries and the training dataset . in order to improve the margin, we introduce differential training, which is a training paradigm that uses a loss function defined on pairs of points from each class .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  23%|██▎       | 459/1992 [29:38<1:42:57,  4.03s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper we present a method for algorithmic melody generation using a generative adversarial network without recurrent components. Music generation has been successfully done using recurrent neural networks, where the model learns sequence information that can help create authentic sounding melodies.   Here, we use DCGAN architecture with dilated convolutions and towers to capture sequential information as spatial image information, and learn long-range dependencies in fixed-length melody forms such as Irish traditional reel.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'music generation has been successfully done using recurrent neural networks . the model learns sequence information that can help create authentic sounding melodies . we use DCGAN architecture with dilated convolutions and towers to capture sequential information as spatial image information .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  23%|██▎       | 460/1992 [29:41<1:38:24,  3.85s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Sequence generation models such as recurrent networks can be trained with a diverse set of learning algorithms. For example, maximum likelihood learning is simple and efficient, yet suffers from the exposure bias problem. Reinforcement learning like policy gradient addresses the problem but can have prohibitively poor exploration efficiency. A variety of other algorithms such as RAML, SPG, and data noising, have also been developed in different perspectives. This paper establishes a formal connection between these algorithms. We present a generalized entropy regularized policy optimization formulation, and show that the apparently divergent algorithms can all be reformulated as special instances of the framework, with the only difference being the configurations of reward function and a couple of hyperparameters. The unified interpretation offers a systematic view of the varying properties of exploration and learning efficiency. Besides, based on the framework, we present a new algorithm that dynamically interpolates among the existing algorithms for improved learning. Experiments on machine translation and text summarization demonstrate the superiority of the proposed algorithm.\n[{'summary_text': 'recurrent networks can be trained with a diverse set of learning algorithms . maximum likelihood learning is simple and efficient, yet suffers from the exposure bias problem . a variety of other algorithms such as RAML, SPG, and data noising, have also been developed in different perspectives .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  23%|██▎       | 461/1992 [29:45<1:38:48,  3.87s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recurrent models for sequences have been recently successful at many tasks, especially for language modeling\n and machine translation. Nevertheless, it remains challenging to extract good representations from\n these models. For instance, even though language has a clear hierarchical structure going from characters\n through words to sentences, it is not apparent in current language models.\n We propose to improve the representation in sequence models by\n augmenting current approaches with an autoencoder that is forced to compress\n the sequence through an intermediate discrete latent space. In order to propagate gradients\n though this discrete representation we introduce an improved semantic hashing technique.\n We show that this technique performs well on a newly proposed quantitative efficiency measure.\n We also analyze latent codes produced by the model showing how they correspond to\n words and phrases. Finally, we present an application of the autoencoder-augmented\n model to generating diverse translations.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we propose to improve the representation in sequence models by augmenting current approaches with an autoencoder that is forced to compress the sequence through an intermediate discrete latent space . in order to propagate gradients though this discrete representation we introduce an improved semantic hashing technique .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  23%|██▎       | 462/1992 [29:49<1:35:57,  3.76s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"High intra-class diversity and inter-class similarity is a characteristic of remote sensing scene image data sets currently posing significant difficulty for deep learning algorithms on classification tasks. To improve accuracy, post-classification\n methods have been proposed for smoothing results of model predictions. However, those approaches require an additional neural network to perform the smoothing operation, which adds overhead to the task. We propose an approach that involves learning deep features directly over neighboring scene images without requiring use of a cleanup model. Our approach utilizes a siamese network to improve the discriminative power of convolutional neural networks on a pair\n of neighboring scene images. It then exploits semantic coherence between this pair to enrich the feature vector of the image for which we want to predict a label.\n Empirical results show that this approach provides a viable alternative to existing methods. For example, our model improved prediction accuracy by 1 percentage point and dropped the mean squared error value by 0.02 over the baseline, on a\n[{'summary_text': 'high intra-class diversity and inter-class similarity is a characteristic of remote sensing scene image data sets currently posing significant difficulty for deep learning algorithms . post-classification methods have been proposed for smoothing results of model predictions . however, those approaches require an additional neural network to perform the smoothing operation, which adds overhead to the task .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  23%|██▎       | 463/1992 [29:53<1:38:05,  3.85s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"This paper addresses unsupervised domain adaptation, the setting where labeled training data is available on a source domain, but the goal is to have good performance on a target domain with only unlabeled data. Like much of previous work, we seek to align the learned representations of the source and target domains while preserving discriminability. The way we accomplish alignment is by learning to perform auxiliary self-supervised task(s) on both domains simultaneously.   Each self-supervised task brings the two domains closer together along the direction relevant to that task. Training this jointly with the main task classifier on the source domain is shown to successfully generalize to the unlabeled target domain.   The presented objective is straightforward to implement and easy to optimize. We achieve state-of-the-art results on four out of seven standard benchmarks, and competitive results on segmentation adaptation. We also demonstrate that our method composes well with another popular pixel-level adaptation method.<|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'paper addresses unsupervised domain adaptation, the setting where labeled training data is available on a source domain . but the goal is to have good performance on the target domain with only unlabeled data . the way we accomplish alignment is by learning to perform auxiliary self-supervised task(s) on both domains simultaneously .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  23%|██▎       | 464/1992 [29:57<1:36:48,  3.80s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Neural networks for structured data like graphs have been studied extensively in recent years.\n To date, the bulk of research activity has focused mainly on static graphs.\n However, most real-world networks are dynamic since their topology tends to change over time.\n Predicting the evolution of dynamic graphs is a task of high significance in the area of graph mining.\n Despite its practical importance, the task has not been explored in depth so far, mainly due to its challenging nature.\n In this paper, we propose a model that predicts the evolution of dynamic graphs.\n Specifically, we use a graph neural network along with a recurrent architecture to capture the temporal evolution patterns of dynamic graphs.\n Then, we employ a generative model which predicts the topology of the graph at the next time step and constructs a graph instance that corresponds to that topology.\n We evaluate the proposed model on several artificial datasets following common network evolving dynamics, as well as on real-world datasets.\n Results demonstrate\n[{'summary_text': 'to date, the bulk of research activity has focused mainly on static graphs . however, most real-world networks are dynamic since their topology tends to change over time . despite its practical importance, the task has not been explored in depth so far .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  23%|██▎       | 465/1992 [30:00<1:36:38,  3.80s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper, we introduce a novel method to interpret recurrent neural networks (RNNs), particularly long short-term memory networks (LSTMs) at the cellular level. We propose a systematic pipeline for interpreting individual hidden state dynamics within the network using response characterization methods. The ranked contribution of individual cells to the network's output is computed by analyzing a set of interpretable metrics of their decoupled step and sinusoidal responses. As a result, our method is able to uniquely identify neurons with insightful dynamics, quantify relationships between dynamical properties and test accuracy through ablation analysis, and interpret the impact of network capacity on a network's dynamical distribution. Finally, we demonstrate generalizability and scalability of our method by evaluating a series of different benchmark sequential datasets.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': \"the ranked contribution of individual cells to the network's output is computed by analyzing a set of interpretable metrics of their decoupled step and sinusoidal responses . as a result, our method is able to uniquely identify neurons with insightful dynamics, quantify relationships between dynamical properties and test accuracy through ablation analysis .\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  23%|██▎       | 466/1992 [30:04<1:36:01,  3.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We propose to extend existing deep reinforcement learning  (Deep RL) algorithms by allowing them to additionally choose sequences of actions as a part of their policy.   This modification forces the network to anticipate the reward of action sequences, which, as we show, improves the exploration leading to better convergence. Our proposal is simple, flexible, and can be easily incorporated into any Deep RL framework. We show the power of our scheme by consistently outperforming the state-of-the-art GA3C algorithm on several popular Atari Games.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we propose to extend existing deep reinforcement learning (Deep RL) algorithms by allowing them to additionally choose sequences of actions as a part of their policy . this modification forces the network to anticipate the reward of action sequences . our proposal is simple, flexible, and can be easily incorporated into any Deep RL framework .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  23%|██▎       | 467/1992 [30:08<1:34:45,  3.73s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The current trade-off between depth and computational cost makes it difficult to adopt deep neural networks for many industrial applications, especially when computing power is limited. Here, we are inspired by the idea that, while deeper embeddings are needed to discriminate difficult samples, a large number of samples can be well discriminated via much shallower embeddings. In this study, we introduce the concept of decision gates (d-gate), modules trained to decide whether a sample needs to be projected into a deeper embedding or if an early prediction can be made at the d-gate, thus enabling the computation of dynamic representations at different depths.   The proposed d-gate modules can be integrated with any deep neural network and reduces the average computational cost of the deep neural networks while maintaining modeling accuracy. Experimental results show that leveraging the proposed d-gate modules led to a ~38% speed-up and ~39% FLOPS reduction on ResNet-101 and ~46% speed-up and $\\sim\n[{'summary_text': 'the current trade-off between depth and computational cost makes it difficult to adopt deep neural networks for many industrial applications, especially when computing power is limited . in this study, we introduce the concept of decision gates (d-gate), modules trained to decide whether a sample needs to be projected into a deeper embedding or if an early prediction can be made . the proposed modules can be integrated with any deep neural network and reduces the average computational cost while maintaining modeling accuracy .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  23%|██▎       | 468/1992 [30:12<1:39:38,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Learning a policy using only observational data is challenging because the distribution of states it induces at execution time may differ from the distribution observed during training. In this work, we propose to train a policy while explicitly penalizing the mismatch between these two distributions over a fixed time horizon. We do this by using a learned model of the environment dynamics which is unrolled for multiple time steps, and training a policy network to minimize a differentiable cost over this rolled-out trajectory. This cost contains two terms: a policy cost which represents the objective the policy seeks to optimize, and an uncertainty cost which represents its divergence from the states it is trained on. We propose to measure this second cost by using the uncertainty of the dynamics model about its own predictions, using recent ideas from uncertainty estimation for deep networks. We evaluate our approach using a large-scale observational dataset of driving behavior recorded from traffic cameras, and show that we are able to learn effective driving policies from purely observational data, with no environment interaction.<|endoftext|><|endoftext|>\n[{'summary_text': 'in this work, we propose to train a policy while explicitly penalizing the mismatch between these two distributions over a fixed time horizon . we do this by using a learned model of the environment dynamics which is unrolled for multiple time steps . a cost which represents the objective the policy seeks to optimize, and an uncertainty cost that represents its divergence from the states it is trained on.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  24%|██▎       | 469/1992 [30:16<1:39:27,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Many irregular domains such as social networks, financial transactions, neuron connections, and natural language structures are represented as graphs. In recent years, a variety of  graph neural networks (GNNs) have been successfully applied for representation learning and prediction on such graphs. However, in many of the applications, the underlying graph changes over time and existing GNNs are inadequate for handling such dynamic graphs. In this paper we propose a novel technique for learning embeddings of dynamic graphs based on a tensor algebra framework. Our method extends the popular graph convolutional network (GCN) for learning representations of dynamic graphs using the recently proposed tensor M-product technique. Theoretical results that establish the connection between the proposed tensor approach and spectral convolution of tensors are developed. Numerical experiments on real datasets demonstrate the usefulness of the proposed method for an edge classification task on dynamic graphs.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'in recent years, a variety of graph neural networks (GNNs) have been successfully applied for representation learning and prediction on such graphs . however, in many of the applications, the underlying graph changes over time and existing GNNs are inadequate for handling such dynamic graphs.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  24%|██▎       | 470/1992 [30:20<1:36:28,  3.80s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"A fundamental, and still largely unanswered, question in the context of Generative Adversarial Networks (GANs) is whether GANs are actually able to capture the key characteristics of the datasets they are trained on. The current approaches to examining this issue require significant human supervision, such as visual inspection of sampled images, and often offer only fairly limited scalability. In this paper, we propose new techniques that employ classification-based perspective to evaluate synthetic GAN distributions and their capability to accurately reflect the essential properties of the training data. These techniques require only minimal human supervision and can easily be scaled and adapted to evaluate a variety of state-of-the-art GANs on large, popular datasets. They also indicate that GANs have significant problems in reproducing the more distributional properties of the training dataset. In particular, the diversity of such synthetic data is orders of magnitude smaller than that of the original data.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the current approaches to examining this issue require significant human supervision, such as visual inspection of sampled images, and often offer only fairly limited scalability . these techniques require only minimal human supervision and can easily be scaled and adapted to evaluate a variety of state-of-the-art GANs on large, popular datasets . the diversity of such synthetic data is orders of magnitude smaller than that of the original data .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  24%|██▎       | 471/1992 [30:24<1:37:54,  3.86s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd’s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined — applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x\n[{'summary_text': 'Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices . their energy is dominated by the number of multiplies needed to perform the convolutions . but these two methods cannot be straightforwardly combined .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  24%|██▎       | 472/1992 [30:27<1:37:37,  3.85s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Edge intelligence especially binary neural network (BNN) has attracted considerable attention of the artificial intelligence community recently. BNNs significantly reduce the computational cost, model size, and memory footprint.   However, there is still a performance gap between the successful full-precision neural network with ReLU activation and BNNs. We argue that the accuracy drop of BNNs is due to their geometry. \n We analyze the behaviour of the full-precision neural network with ReLU activation and compare it with its binarized counterpart. This comparison suggests random bias initialization as a remedy to activation saturation in full-precision networks and  leads us towards an improved BNN training. Our numerical experiments confirm our geometric intuition.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'Edge intelligence especially binary neural network (BNN) has attracted considerable attention of the artificial intelligence community recently . BNNs significantly reduce the computational cost, model size, and memory footprint . however, there is still a performance gap between the successful full-precision neural network with ReLU activation .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  24%|██▎       | 473/1992 [30:31<1:37:44,  3.86s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Model-free deep reinforcement learning approaches have shown superhuman performance in simulated environments (e.g., Atari games, Go, etc). During training, these approaches often implicitly construct a latent space that contains key information for decision making. In this paper, we learn a forward model on this latent space and apply it to model-based planning in miniature Real-time Strategy game with incomplete information (MiniRTS). We first show that the latent space constructed from existing actor-critic models contains relevant information of the game, and design training procedure to learn forward models. We also show that our learned forward model can predict meaningful future state and is usable for latent space Monte-Carlo Tree Search (MCTS), in terms of win rates against rule-based agents.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'model-free deep reinforcement learning approaches have shown superhuman performance in simulated environments (e.g., Atari games, Go, etc). in this paper, we learn a forward model on this latent space and apply it to model-based planning in miniature Real-time Strategy game with incomplete information (MiniRTS). we also show that our learned forward model can predict meaningful future state .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  24%|██▍       | 474/1992 [30:35<1:39:13,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Semantic parsing which maps a natural language sentence into a formal machine-readable representation of its meaning, is highly constrained by the limited annotated training data. Inspired by the idea of coarse-to-fine, we propose a general-to-detailed neural network(GDNN) by incorporating cross-domain sketch(CDS) among utterances and their logic forms. For utterances in different domains, the General Network will extract CDS using an encoder-decoder model in a multi-task learning setup. Then for some utterances in a specific domain, the Detailed Network will generate the detailed target parts using sequence-to-sequence architecture with advanced attention to both utterance and generated CDS. Our experiments show that compared to direct multi-task learning, CDS has improved the performance in semantic parsing task which converts users' requests into meaning representation language(MRL). We also use experiments to illustrate that CDS works by adding some constraints to the target decoding process, which\n[{'summary_text': 'semantic parsing which maps a natural language sentence into a formal machine-readable representation of its meaning is highly constrained by the limited annotated training data . the general network will extract CDS using an encoder-decoder model in a multi-task learning setup . for some utterances in different domains, the Detailed Network will generate the detailed target parts using sequence-to-sequence architecture . compared to direct multitask learning, CDS has improved the performance in semantic'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  24%|██▍       | 475/1992 [30:40<1:43:41,  4.10s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We introduce the Convolutional Conditional Neural Process (ConvCNP), a new member of the Neural Process family that models translation equivariance in the data. Translation equivariance is an important inductive bias for many learning problems including time series modelling, spatial data, and images. The model embeds data sets into an infinite-dimensional function space, as opposed to finite-dimensional vector spaces. To formalize this notion, we extend the theory of neural representations of sets to include functional representations, and demonstrate that any translation-equivariant embedding can be represented using a convolutional deep-set. We evaluate ConvCNPs in several settings, demonstrating that they achieve state-of-the-art performance compared to existing NPs. We demonstrate that building in translation equivariance enables zero-shot generalization to challenging, out-of-domain tasks.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the model embeds data sets into an infinite-dimensional function space, as opposed to finite-dimensional vector spaces . to formalize this notion, we extend the theory of neural representations of sets to include functional representations . we demonstrate that any translation-equivariant embedding can be represented using a convolutional deep-set .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  24%|██▍       | 476/1992 [30:44<1:41:03,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Sequence-to-sequence (Seq2Seq) models with attention have excelled at tasks which involve generating natural language sentences such as machine translation, image captioning and speech recognition. Performance has further been improved by leveraging unlabeled data, often in the form of a language model. In this work, we present the Cold Fusion method, which leverages a pre-trained language model during training, and show its effectiveness on the speech recognition task. We show that Seq2Seq models with Cold Fusion are able to better utilize language information enjoying i) faster convergence and better generalization, and ii) almost complete transfer to a new domain while using less than 10% of the labeled training data.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'models with attention have excelled at tasks which involve generating natural language sentences such as machine translation, image captioning and speech recognition . performance has further been improved by leveraging unlabeled data, often in the form of a language model . in this work, we present the Cold Fusion method, which leverages a pre-trained language model during training .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  24%|██▍       | 477/1992 [30:47<1:38:59,  3.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recent work suggests goal-driven training of neural networks can be used to model neural activity in the brain. While response properties of neurons in artificial neural networks bear similarities to those in the brain, the network architectures are often constrained to be different. Here we ask if a neural network can recover both neural representations and, if the architecture is unconstrained and optimized, also the anatomical properties of neural circuits. We demonstrate this in a system where the connectivity and the functional organization have been characterized, namely, the head direction circuit of the rodent and fruit fly. We trained recurrent neural networks (RNNs) to estimate head direction through integration of angular velocity. We found that the two distinct classes of neurons observed in the head direction system, the Ring neurons and the Shifter neurons, emerged naturally in artificial neural networks as a result of training. Furthermore, connectivity analysis and in-silico neurophysiology revealed structural and mechanistic similarities between artificial networks and the head direction system. Overall, our results show that\n[{'summary_text': 'recent work suggests goal-driven training of neural networks can be used to model neural activity in the brain . the network architectures are often constrained to be different . if the architecture is unconstrained and optimized, also the anatomical properties of neural circuits .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  24%|██▍       | 478/1992 [30:51<1:38:45,  3.91s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We introduce Lyceum, a high-performance computational ecosystem for robotlearning.    Lyceum is built on top of the Julia programming language and theMuJoCo physics simulator, combining the ease-of-use of a high-level program-ming  language  with  the  performance  of  native  C.  Lyceum  is  up  to  10-20Xfaster  compared  to  other  popular  abstractions  like  OpenAI’sGymand  Deep-Mind’sdm-control.   This substantially reduces training time for various re-inforcement learning algorithms;  and is also fast enough to support real-timemodel  predictive  control  with  physics  simulators.    Lyceum  has  a  straightfor-ward API and supports parallel computation across multiple cores or machines. The code base,  tutorials,  and demonstration videos can be\n[{'summary_text': 'Lyceum is built on top of the Julia programming language and theMuJoCo physics simulator, combining ease-of-use of a high-level program-ming language with the performance of native C . this substantially reduces training time for various re-inforcement learning algorithms; and is also fast enough to support real-timemodel predictive control .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  24%|██▍       | 479/1992 [30:55<1:40:24,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Automatic Essay Scoring (AES) has been an active research area as it can greatly reduce the workload of teachers and prevents subjectivity bias . Most recent AES solutions apply deep neural network (DNN)-based models with regression, where the neural neural-based encoder learns an essay representation that helps differentiate among the essays and the corresponding essay score is inferred by a regressor. Such DNN approach usually requires a lot of expert-rated essays as training data in order to learn a good essay representation for accurate scoring. However, such data is usually expensive and thus is sparse. Inspired by the observation that human usually scores an essay by comparing it with some references, we propose a Siamese framework called Referee Network (RefNet) which allows the model to compare the quality of two essays by capturing the relative features that can differentiate the essay pair. The proposed framework can be applied as an extension to regression models as it can capture additional relative features on top of internal information. Moreover,\n[{'summary_text': 'most recent AES solutions apply deep neural network (DNN)-based models with regression . the neural-based encoder learns an essay representation that helps differentiate among the essays and the corresponding essay score is inferred by a regressor . such data is usually expensive and thus is sparse .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  24%|██▍       | 480/1992 [30:59<1:40:21,  3.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Predictive coding, within theoretical neuroscience, and variational autoencoders, within machine learning, both involve latent Gaussian models and variational inference. While these areas share a common origin, they have evolved largely independently. We outline connections and contrasts between these areas, using their relationships to identify new parallels between machine learning and neuroscience. We then discuss specific frontiers at this intersection: backpropagation, normalizing flows, and attention, with mutual benefits for both fields.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'predictive coding, within theoretical neuroscience, and variational autoencoders, within machine learning, involve latent Gaussian models . while these areas share a common origin, they have evolved largely independently . we outline connections and contrasts between these areas, using their relationships to identify new parallels between machine learning and neuroscience .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  24%|██▍       | 481/1992 [31:03<1:38:05,  3.90s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Questions that require counting a variety of objects in images remain a major challenge in visual question answering (VQA). The most common approaches to VQA involve either classifying answers based on fixed length representations of both the image and question or summing fractional counts estimated from each section of the image. In contrast, we treat counting as a sequential decision process and force our model to make discrete choices of what to count. Specifically, the model sequentially selects from detected objects and learns interactions between objects that influence subsequent selections. A distinction of our approach is its intuitive and interpretable output, as discrete counts are automatically grounded in the image. Furthermore, our method outperforms the state of the art architecture for VQA on multiple metrics that evaluate counting.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'most common approaches to VQA involve either classifying answers based on fixed length representations of both the image and question or summing fractional counts estimated from each section of the image . Specifically, the model sequentially selects from detected objects and learns interactions between objects that influence subsequent selections . our method outperforms the state of the art architecture for VQA on multiple metrics that evaluate counting .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  24%|██▍       | 482/1992 [31:07<1:38:07,  3.90s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Experimental reproducibility and replicability are critical topics in machine learning. Authors have often raised concerns about their lack in scientific publications to improve the quality of the field. Recently, the graph representation learning field has attracted the attention of a wide research community, which resulted in a large stream of works.\n As such, several Graph Neural Network models have been developed to effectively tackle graph classification. However, experimental procedures often lack rigorousness and are hardly reproducible. Motivated by this, we provide an overview of common practices that should be avoided to fairly compare with the state of the art. To counter this troubling trend, we ran more than 47000 experiments in a controlled and uniform framework to re-evaluate five popular models across nine common benchmarks. Moreover, by comparing GNNs with structure-agnostic baselines we provide convincing evidence that, on some datasets, structural information has not been exploited yet. We believe that this work can contribute to the development of the graph learning field, by providing\n[{'summary_text': 'several Graph Neural Network models have been developed to effectively tackle graph classification . but experimental procedures often lack rigorousness and are hardly reproducible . to counter this troubling trend, we ran more than 47000 experiments in a controlled and uniform framework to re-evaluate five popular models across nine common benchmarks.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  24%|██▍       | 483/1992 [31:11<1:38:26,  3.91s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Rectified linear units, or ReLUs, have become a preferred activation function for artificial neural networks. In this paper we consider the problem of learning a generative model in the presence of nonlinearity (modeled by the ReLU functions). Given a set of signal vectors $\\mathbf{y}^i \\in \\mathbb{R}^d, i =1, 2, \\dots , n$, we  aim to learn the network parameters, i.e., the $d\\times k$ matrix $A$, under the model $\\mathbf{y}^i = \\mathrm{ReLU}(A\\mathbf{c}^i +\\mathbf{b})$, where $\\mathbf{b}\\in \\mathbb{R}^d$ is a random bias vector, and {$\\mathbf{c}^i \\in \\mathbb{R}^k$ are arbitrary unknown latent vectors}. We\n[{'summary_text': 'weRectified linear units, or ReLUs, have become a preferred activation function for artificial neural networks . in this paper we consider the problem of learning a generative model in the presence of nonlinearity (modeled by the ReLU functions)'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  24%|██▍       | 484/1992 [31:15<1:37:59,  3.90s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In cognitive systems, the role of a working memory is crucial for visual reasoning and decision making. Tremendous progress has been made in understanding the mechanisms of the human/animal working memory, as well as in formulating different frameworks of artificial neural networks.   In the case of humans, the visual working memory (VWM) task is a standard one in which the subjects are presented with a sequence of images, each of which needs to be identified as to whether it was already seen or not. \n\n Our work is a study of multiple ways to learn a working memory model using recurrent neural networks that learn to remember input images across timesteps. We train these neural networks to solve the working memory task by training them with a sequence of images in supervised and reinforcement learning settings. The supervised setting uses image sequences with their corresponding labels. The reinforcement learning setting is inspired by the popular view in neuroscience that the working memory in the prefrontal cortex is modulated by a dopaminergic mechanism. We\n[{'summary_text': 'the visual working memory task is a standard one in which the subjects are presented with a sequence of images, each of which needs to be identified as to whether it was already seen or not . the reinforcement learning setting is inspired by the popular view in neuroscience that the working memory in the prefrontal cortex is modulated by a dopaminergic mechanism .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  24%|██▍       | 485/1992 [31:19<1:40:41,  4.01s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper, we study the adversarial attack and defence problem in deep learning from the perspective of Fourier analysis. We first explicitly compute the Fourier transform of deep ReLU neural networks and show that there exist decaying but non-zero high frequency components in the Fourier spectrum of neural networks. We then demonstrate that the vulnerability of neural networks towards adversarial samples can be attributed to these insignificant but non-zero high frequency components. Based on this analysis, we propose to use a simple post-averaging technique to smooth out these high frequency components to improve the robustness of neural networks against adversarial attacks. Experimental results on the ImageNet and the CIFAR-10 datasets have shown that our proposed method is universally effective to defend many existing adversarial attacking methods proposed in the literature, including FGSM, PGD, DeepFool and C&W attacks. Our post-averaging method is simple since it does not require any re-training, and meanwhile it can successfully defend over\n[{'summary_text': 'in this paper, we study the adversarial attack and defence problem in deep learning from the perspective of Fourier analysis . we first explicitly compute the Fourier transform of deep ReLU neural networks and show that there exist decaying but non-zero high frequency components . based on this analysis, we propose to use a simple post-averaging technique to smooth out these high frequencies components to improve the robustness of neural networks .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  24%|██▍       | 486/1992 [31:23<1:43:15,  4.11s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Adversaries in neural networks have drawn much attention since their first debut. \n While most existing methods aim at deceiving image classification models into misclassification or crafting attacks for specific object instances in the object setection tasks, we focus on creating universal adversaries to fool object detectors and hide objects from the detectors. \n The adversaries we examine are universal in three ways: \n (1) They are not specific for specific object instances; \n (2) They are image-independent; \n (3) They can further transfer to different unknown models. \n To achieve this, we propose two novel techniques to improve the transferability of the adversaries: \\textit{piling-up} and \\textit{monochromatization}. \n Both techniques prove to simplify the patterns of generated adversaries, and ultimately result in higher transferability.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the adversaries we examine are universal in three ways: (1) they are not specific for specific object instances; (2) they are image-independent; (3) They can further transfer to different unknown models . both techniques prove to simplify the patterns of generated adversaries, and ultimately result in higher transferability .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  24%|██▍       | 487/1992 [31:27<1:38:36,  3.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Recent studies have highlighted adversarial examples as a ubiquitous threat to different neural network models and many downstream  applications. Nonetheless, as unique data properties have inspired distinct and powerful learning principles, this paper aims to explore their potentials towards mitigating adversarial inputs. In particular, our results reveal the importance of using the temporal dependency in audio data to gain discriminate power against adversarial examples. Tested on the automatic speech recognition (ASR) tasks and three recent audio adversarial attacks, we find that (i) input transformation developed from image adversarial defense provides limited robustness improvement and is subtle to advanced attacks; (ii) temporal dependency can be exploited to gain discriminative power against audio adversarial examples and is resistant to adaptive attacks considered in our experiments. Our results not only show promising means of improving the robustness of ASR systems, but also offer novel insights in exploiting domain-specific data properties to mitigate negative effects of adversarial examples.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'our results show the importance of using the temporal dependency in audio data to gain discriminate power against adversarial examples . our results not only show promising means of improving the robustness of ASR systems, but also offer novel insights in exploiting domain-specific data properties to mitigate negative effects .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  24%|██▍       | 488/1992 [31:31<1:36:08,  3.84s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Deep image prior (DIP), which utilizes a deep convolutional network (ConvNet) structure itself as an image prior, has attracted huge attentions in computer vision community.   It empirically shows the effectiveness of ConvNet structure for various image restoration applications.   However, why the DIP works so well is still unknown, and why convolution operation is essential for image reconstruction or enhancement is not very clear. In this study, we tackle these questions. The proposed approach is dividing the convolution into ``delay-embedding'' and ``transformation (\\ie encoder-decoder)'', and proposing a simple, but essential, image/tensor modeling method which is closely related to dynamical systems and self-similarity. The proposed method named as manifold modeling in embedded space (MMES) is implemented by using a novel denoising-auto-encoder in combination with multi-way delay-embedding transform. In spite of its simplicity, the\n[{'summary_text': \"the proposed approach is dividing the convolution into delay-embedding'' and transformation (ie encoder-decoder)'', and proposing a simple, but essential, image/tensor modeling method which is closely related to dynamical systems and self-similarity . the proposed method named as manifold modeling in embedded space (MMES) is implemented by using a novel denoising-auto-encoder in combination with multi-way delay-e\"}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  25%|██▍       | 489/1992 [31:35<1:41:46,  4.06s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Information need of humans is essentially multimodal in nature, enabling maximum exploitation of situated context. We introduce a dataset for sequential procedural (how-to) text generation from images in cooking domain. The dataset consists of 16,441 cooking recipes with 160,479 photos associated with different steps. We setup a baseline motivated by the best performing model in terms of human evaluation for the Visual Story Telling (ViST) task. In addition, we introduce two models to incorporate high level structure learnt by a Finite State Machine (FSM) in neural sequential generation process by: (1) Scaffolding Structure in Decoder (SSiD) (2) Scaffolding Structure in Loss (SSiL). These models show an improvement in empirical as well as human evaluation. Our best performing model (SSiL) achieves a METEOR score of 0.31, which is an improvement of 0.6 over the baseline model. We also conducted human evaluation of the generated grounded recipes,\n[{'summary_text': 'dataset consists of 16,441 cooking recipes with 160,479 photos associated with different steps . we setup a baseline motivated by the best performing model in terms of human evaluation for the Visual Story Telling task .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  25%|██▍       | 490/1992 [31:39<1:38:47,  3.95s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels.   By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples.   Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures.   We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples . mixup trains a neural network on convex combinations of pairs of examples and their labels . by doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples.'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  25%|██▍       | 491/1992 [31:43<1:37:25,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Neural networks can converge faster with help from a smarter batch selection strategy. In this regard, we propose Ada-Boundary, a novel adaptive-batch selection algorithm that constructs an effective mini-batch according to the learning progress of the model.Our key idea is to present confusing samples what the true label is. Thus, the samples near the current decision boundary are considered as the most effective to expedite convergence. Taking advantage of our design, Ada-Boundary maintains its dominance in various degrees of training difficulty. We demonstrate the advantage of Ada-Boundary by extensive experiments using two convolutional neural networks for three benchmark data sets. The experiment results show that Ada-Boundary improves the training time by up to 31.7% compared with the state-of-the-art strategy and by up to 33.5% compared with the baseline strategy.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'we propose Ada-Boundary, a novel adaptive-batch selection algorithm . our key idea is to present confusing samples what the true label is . the samples near the current decision boundary are considered as the most effective to expedite convergence .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  25%|██▍       | 492/1992 [31:46<1:34:24,  3.78s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In many settings, it is desirable to learn decision-making and control policies through learning or from expert demonstrations. The most common approaches under this framework are Behaviour Cloning (BC), and Inverse Reinforcement Learning (IRL). Recent methods for IRL have demonstrated the capacity to learn effective policies with access to a very limited set of demonstrations, a scenario in which BC methods often fail. Unfortunately, directly comparing the algorithms for these methods does not provide adequate intuition for understanding this difference in performance. This is the motivating factor for our work. We begin by presenting $f$-MAX, a generalization of AIRL (Fu et al., 2018), a state-of-the-art IRL method. $f$-MAX provides grounds for more directly comparing the objectives for LfD. We demonstrate that $f$-MAX, and by inheritance AIRL, is a subset of the cost-regularized IRL framework laid out by Ho & Ermon (2016). We\n[{'summary_text': 'the most common approaches under this framework are Behaviour Cloning (BC), and Inverse Reinforcement Learning (IRL) recent methods for IRL have demonstrated the capacity to learn effective policies with access to a very limited set of demonstrations, a scenario in which BC methods often fail . unfortunately, directly comparing the algorithms for these methods does not provide adequate intuition for understanding this difference in performance .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  25%|██▍       | 493/1992 [31:50<1:37:35,  3.91s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"A commonplace belief in the machine learning community is that using adaptive gradient methods hurts generalization. We re-examine this belief both theoretically and experimentally, in light of insights and trends from recent years.\n We revisit some previous oft-cited experiments and theoretical accounts in more depth, and provide a new set of experiments in larger-scale, state-of-the-art settings. We conclude that with proper tuning, the improved training performance of adaptive optimizers does not in general carry an overfitting penalty, especially in contemporary deep learning. Finally, we synthesize a ``user's guide'' to adaptive optimizers, including some proposed modifications to AdaGrad to mitigate some of its empirical shortcomings.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'a commonplace belief in the machine learning community is that using adaptive gradient methods hurts generalization . we re-examine this belief both theoretically and experimentally, in light of insights and trends from recent years .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  25%|██▍       | 494/1992 [31:54<1:33:37,  3.75s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In recent years, deep neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images. At inference time, it finds a close output to a given image which does not contain the adversarial changes. This output is then fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-GAN is consistently effective against different attack methods and improves on existing defense strategies.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'defense-GAN is trained to model the distribution of unperturbed images . at inference time, it finds a close output to a given image which does not contain the adversarial changes . this output is then fed to the classifier .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  25%|██▍       | 495/1992 [31:57<1:31:39,  3.67s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Wide adoption of complex RNN based models is hindered by their inference performance, cost and memory requirements. To address this issue, we develop AntMan, combining structured sparsity with low-rank decomposition synergistically, to reduce model computation, size and execution time of RNNs while attaining desired accuracy. AntMan extends knowledge distillation based training to learn the compressed models efficiently. Our evaluation shows that AntMan offers up to 100x computation reduction with less than 1pt accuracy drop for language and machine reading comprehension models. Our evaluation also shows that for a given accuracy target, AntMan produces 5x smaller models than the state-of-art. Lastly, we show that AntMan offers super-linear speed gains compared to theoretical speedup, demonstrating its practical value on commodity hardware.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'to address this issue, we develop AntMan, combining structured sparsity with low-rank decomposition synergistically, to reduce model computation, size and execution time of RNNs while attaining desired accuracy . AntMan offers up to 100x computation reduction with less than 1pt accuracy drop for language and machine reading comprehension models .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  25%|██▍       | 496/1992 [32:01<1:31:44,  3.68s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The physical design of a robot and the policy that controls its motion are inherently coupled. However, existing approaches largely ignore this coupling, instead choosing to alternate between separate design and control phases, which requires expert intuition throughout and risks convergence to suboptimal designs. In this work, we propose a method that jointly optimizes over the physical design of a robot and the corresponding control policy in a model-free fashion, without any need for expert supervision. Given an arbitrary robot morphology, our method maintains a distribution over the design parameters and uses reinforcement learning to train a neural network controller. Throughout training, we refine the robot distribution to maximize the expected reward. This results in an assignment to the robot parameters and neural network policy that are jointly optimal. We evaluate our approach in the context of legged locomotion, and demonstrate that it discovers novel robot designs and walking gaits for several different morphologies, achieving performance comparable to or better than that of hand-crafted designs.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'existing approaches largely ignore this coupling, instead choosing to alternate between separate design and control phases . given an arbitrary robot morphology, our method maintains a distribution over design parameters and uses reinforcement learning to train a neural network controller . this results in an assignment to the robot parameters and neural network policy that are jointly optimal .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  25%|██▍       | 497/1992 [32:05<1:32:03,  3.69s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The backpropagation (BP) algorithm is often thought to be biologically implausible in the brain. One of the main reasons is that BP requires symmetric weight matrices in the feedforward and feedback pathways. To address this “weight transport problem” (Grossberg, 1987), two biologically-plausible algorithms, proposed by Liao et al. (2016) and Lillicrap et al. (2016), relax BP’s weight symmetry requirements and demonstrate comparable learning capabilities to that of BP on small datasets. However, a recent study by Bartunov et al. (2018) finds that although feedback alignment (FA) and some variants of target-propagation (TP) perform well on MNIST and CIFAR, they perform significantly worse than BP on ImageNet. Here, we additionally evaluate the sign-symmetry (SS) algorithm (Liao et al., 2016), which differs from both BP and FA in that the feedback and feed\n[{'summary_text': 'the backpropagation (BP) algorithm is often thought to be biologically implausible in the brain . one of the main reasons is that BP requires symmetric weight matrices in the feedforward and feedback pathways . a recent study by bartunov et al. (2018) found that although feedback alignment (FA) and some variants of target-propaging (TP) perform well on MNIST and CIFAR, they perform significantly worse than BP on ImageNet .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  25%|██▌       | 498/1992 [32:09<1:38:29,  3.96s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We consider tackling a single-agent RL problem by distributing it to $n$ learners. These learners, called advisors, endeavour to solve the problem from a different focus. Their advice, taking the form of action values, is then communicated to an aggregator, which is in control of the system. We show that the local planning method for the advisors is critical and that none of the ones found in the literature is flawless: the \\textit{egocentric} planning overestimates values of states where the other advisors disagree, and the \\textit{agnostic} planning is inefficient around danger zones. We introduce a novel approach called \\textit{empathic} and discuss its theoretical aspects. We empirically examine and validate our theoretical findings on a fruit collection task.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'learners, called advisors, endeavour to solve the problem from a different focus . their advice, taking the form of action values, is then communicated to an aggregator, which is in control of the system . we introduce a novel approach called textitempathic and discuss its theoretical aspects .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  25%|██▌       | 499/1992 [32:13<1:36:30,  3.88s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The Wasserstein distance received a lot of attention recently in the community of machine learning, especially for its principled way of comparing distributions. It has found numerous applications in several hard problems, such as domain adaptation, dimensionality reduction or generative models. However, its use is still limited by a heavy computational cost. Our goal is to alleviate this problem by providing an approximation mechanism that allows to break its inherent complexity. It relies on the search of an embedding where the Euclidean distance mimics the Wasserstein distance. We show that such an embedding can be found with a siamese architecture associated with a decoder network that allows to move from the embedding space back to the original input space. Once this embedding has been found, computing optimization problems in the Wasserstein space (e.g. barycenters, principal directions or even archetypes) can be conducted extremely fast. Numerical experiments supporting this idea are conducted on image datasets, and show the\n[{'summary_text': 'the Wasserstein distance received a lot of attention recently in the community of machine learning . it has found numerous applications in several hard problems, such as domain adaptation, dimensionality reduction or generative models . however, its use is still limited by a heavy computational cost .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  25%|██▌       | 500/1992 [32:17<1:36:14,  3.87s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The folding structure of the DNA molecule combined with helper molecules, also referred to as the chromatin, is highly relevant for the functional properties of DNA. The chromatin structure is largely determined by the underlying primary DNA sequence, though the interaction is not yet fully understood. In this paper we develop a convolutional neural network that takes an image-representation of primary DNA sequence as its input, and predicts key determinants of chromatin structure. The method is developed such that it is capable of detecting interactions between distal elements in the DNA sequence, which are known to be highly relevant. Our experiments show that the method outperforms several existing methods both in terms of prediction accuracy and training time.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'the chromatin structure is highly relevant for the functional properties of DNA . it is largely determined by the underlying primary DNA sequence, though the interaction is not fully understood . the method is developed so that it is capable of detecting interactions between distal elements in the DNA sequence .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  25%|██▌       | 501/1992 [32:20<1:33:25,  3.76s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We investigate the extent to which individual attention heads in pretrained transformer language models, such as BERT and RoBERTa, implicitly capture syntactic dependency relations. We employ two methods—taking the maximum attention weight and computing the maximum spanning tree—to extract implicit dependency relations from the attention weights of each layer/head, and compare them to the ground-truth Universal Dependency (UD) trees. We show that, for some UD relation types, there exist heads that can recover the dependency type significantly better than baselines on parsed English text, suggesting that some self-attention heads act as a proxy for syntactic structure. We also analyze BERT fine-tuned on two datasets—the syntax-oriented CoLA and the semantics-oriented MNLI—to investigate whether fine-tuning affects the patterns of their self-attention, but we do not observe substantial differences in the overall dependency relations extracted using our methods. Our results suggest that these models have some specialist attention heads that track\n[{'summary_text': 'we employ two methods—taking the maximum attention weight and computing the maximum spanning tree—to extract implicit dependency relations from the attention weights of each layer/head . we also analyze BERT fine-tuned on two datasets—the syntax-oriented CoLA and the semantics-oriented MNLI .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  25%|██▌       | 502/1992 [32:24<1:36:33,  3.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al., 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction—membership classification and API\n[{'summary_text': 'we study the problem of model extraction in natural language processing . an adversary with only query access to a victim model attempts to reconstruct a local copy of that model . the attacker need not even use grammatical or semantically meaningful queries .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  25%|██▌       | 503/1992 [32:28<1:35:28,  3.85s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Skip connections are increasingly utilized by deep neural networks to improve accuracy and cost-efficiency. In particular, the recent DenseNet is efficient in computation and parameters, and achieves state-of-the-art predictions by directly connecting each feature layer to all previous ones. However, DenseNet's extreme connectivity pattern may hinder its scalability to high depths, and in applications like fully convolutional networks, full DenseNet connections are prohibitively expensive. \n This work first experimentally shows that one key advantage of skip connections is to have short distances among feature layers during backpropagation. Specifically, using a fixed number of skip connections, the connection patterns with shorter backpropagation distance among layers have more accurate predictions. Following this insight, we propose a connection template, Log-DenseNet, which, in comparison to DenseNet,  only slightly increases the backpropagation distances among layers from 1 to  ($1 + \\log_2 L$), but\n[{'summary_text': 'the recent DenseNet is efficient in computation and parameters . it achieves state-of-the-art predictions by directly connecting each feature layer to all previous ones . but the extreme connectivity pattern may hinder its scalability to high depths . this work first experimentally shows that one key advantage of skip connections is to have short distances among feature layers during backpropagation .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  25%|██▌       | 504/1992 [32:32<1:37:37,  3.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"In this paper, we show that a simple coloring scheme can improve, both theoretically and empirically, the expressive power of Message Passing Neural Networks (MPNNs). More specifically, we introduce a graph neural network called Colored Local Iterative Procedure (CLIP) that uses colors to disambiguate identical node attributes, and show that this representation is a universal approximator of continuous functions on graphs with node attributes. Our method relies on separability, a key topological characteristic that allows to extend well-chosen neural networks into universal representations. Finally, we show experimentally that CLIP is capable of capturing structural characteristics that traditional MPNNs fail to distinguish, while being state-of-the-art on benchmark graph classification datasets.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n[{'summary_text': 'a graph neural network called Colored Local Iterative Procedure (CLIP) uses colors to disambiguate identical node attributes . this representation is a universal approximator of continuous functions on graphs . we show experimentally that CLIP is capable of capturing structural characteristics that traditional MPNNs fail to distinguish .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  25%|██▌       | 505/1992 [32:36<1:35:35,  3.86s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Normalising Flows (NFs) are a class of likelihood-based generative models that have recently gained popularity. They are based on the idea of transforming a simple density into that of the data. We seek to better understand this class of models, and how they compare to previously proposed techniques for generative modeling and unsupervised representation learning. For this purpose we reinterpret NFs in the framework of Variational Autoencoders (VAEs), and present a new form of VAE that generalises normalising flows. The new generalised model also reveals a close connection to denoising autoencoders, and we therefore call our model the Variational Denoising Autoencoder (VDAE). Using our unified model, we systematically examine the model space between flows, variational autoencoders, and denoising autoencoders, in a set of preliminary experiments on the MNIST handwritten digits. The experiments shed light on the modeling assumptions implicit in\n[{'summary_text': 'normalising Flows (NFs) are a class of likelihood-based generative models . they are based on the idea of transforming a simple density into that of the data . for this purpose we reinterpret NFs in the framework of Variational Autoencoders (VAEs) and present a new form of VAE that generalises normalising flows .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  25%|██▌       | 506/1992 [32:40<1:39:04,  4.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"The tremendous success of deep neural networks has motivated the need to better understand the fundamental properties of these networks, but many of the theoretical results proposed have only been for shallow networks. In this paper, we study an important primitive for understanding the meaningful input space of a deep network: span recovery. For $k<n$, let $\\mathbf{A} \\in \\mathbb{R}^{k \\times n}$ be the innermost weight matrix of an arbitrary feed forward neural network $M: \\mathbb{R}^n \\to  \\mathbb{R}$, so $M(x)$ can be written as $M(x) = \\sigma(\\mathbf{A} x)$, for some network $\\sigma: \\mathbb{R}^k \\to  \\mathbb{R}$. The goal is then to recover the row span of $\\mathbf{A}$ given only oracle access to the value of\n[{'summary_text': 'in this paper, we study an important primitive for understanding the meaningful input space of a deep network: span recovery . the tremendous success of deep neural networks has motivated the need to better understand the fundamental properties of these networks .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  25%|██▌       | 507/1992 [32:44<1:39:52,  4.04s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Existing black-box attacks on deep neural networks (DNNs) so far have largely focused on transferability, where an adversarial instance generated for a locally trained model can “transfer” to attack other learning models. In this paper, we propose novel Gradient Estimation black-box attacks for adversaries with query access to the target model’s class probabilities, which do not rely on transferability. We also propose strategies to decouple the number of queries required to generate each adversarial sample from the dimensionality of the input. An iterative variant of our attack achieves close to 100% adversarial success rates for both targeted and untargeted attacks on DNNs. We carry out extensive experiments for a thorough comparative evaluation of black-box attacks and show that the proposed Gradient Estimation attacks outperform all transferability based black-box attacks we tested on both MNIST and CIFAR-10 datasets, achieving adversarial success rates similar to well known, state\n[{'summary_text': 'in this paper, we propose novel Gradient Estimation black-box attacks for adversaries with query access to the target model’s class probabilities, which do not rely on transferability . we also propose strategies to decouple the number of queries required to generate each adversarial sample from the dimensionality of the input .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  26%|██▌       | 508/1992 [32:48<1:39:33,  4.03s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Neural programs are highly accurate and structured policies that perform algorithmic tasks by controlling the behavior of a computation mechanism. Despite the potential to increase the interpretability and the compositionality of the behavior of artificial agents, it remains difficult to learn from demonstrations neural networks that represent computer programs. The main challenges that set algorithmic domains apart from other imitation learning domains are the need for high accuracy, the involvement of specific structures of data, and the extremely limited observability. To address these challenges, we propose to model programs as Parametrized Hierarchical Procedures (PHPs). A PHP is a sequence of conditional operations, using a program counter along with the observation to select between taking an elementary action, invoking another PHP as a sub-procedure, and returning to the caller. We develop an algorithm for training PHPs from a set of supervisor demonstrations, only some of which are annotated with the internal call structure, and apply it to efficient level-wise training of multi-level PHPs\n[{'summary_text': 'the main challenges that set algorithmic domains apart from other imitation learning domains are the need for high accuracy, the involvement of specific structures of data, and the extremely limited observability . a PHP is a sequence of conditional operations, using a program counter along with the observation to select between taking an elementary action, invoking another PHP as a sub-procedure, and returning to the caller .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  26%|██▌       | 509/1992 [32:53<1:41:08,  4.09s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We build a virtual agent for learning language in a 2D maze-like world. The agent sees images of the surrounding environment, listens to a virtual teacher, and takes actions to receive rewards. It interactively learns the teacher’s language from scratch based on two language use cases: sentence-directed navigation and question answering. It learns simultaneously the visual representations of the world, the language, and the action control. By disentangling language grounding from other computational routines and sharing a concept detection function between language grounding and prediction, the agent reliably interpolates and extrapolates to interpret sentences that contain new word combinations or new words missing from training sentences. The new words are transferred from the answers of language prediction. Such a language ability is trained and evaluated on a population of over 1.6 million distinct sentences consisting of 119 object words, 8 color words, 9 spatial-relation words, and 50 grammatical words. The proposed model significantly outperforms five comparison methods for interpreting zero-shot sentences.\n[{'summary_text': 'agent sees images of the surrounding environment, listens to a virtual teacher, and takes actions to receive rewards . it interactively learns the teacher’s language from scratch based on two language use cases: sentence-directed navigation and question answering .'}]\n","output_type":"stream"},{"name":"stderr","text":"\nbatch:  26%|██▌       | 510/1992 [32:56<1:38:36,  3.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"We show that information about whether a neural network's output will be correct or incorrect is present in the outputs of the network's intermediate layers. To demonstrate this effect, we train a new \"meta\" network to predict from either the final output of the underlying \"base\" network or the output of one of the base network's intermediate layers whether the base network will be correct or incorrect for a particular input. We find that, over a wide range of tasks and base networks, the meta network can achieve accuracies ranging from 65% - 85% in making this determination.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}